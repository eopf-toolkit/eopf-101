{
 "cells": [
  {
   "cell_type": "raw",
   "id": "0",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "---\n",
    "title: \"Flood Mapping - Time Series Analysis in Valencia\" \n",
    "execute:\n",
    "  enabled: true\n",
    "format: html\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "<a href=\"https://jupyterhub.user.eopf.eodc.eu/hub/login?next=%2Fhub%2Fspawn%3Fnext%3D%252Fhub%252Fuser-redirect%252Fgit-pull%253Frepo%253Dhttps%253A%252F%252Fgithub.com%252Feopf-toolkit%252Feopf-101%2526branch%253Dmain%2526urlpath%253Dlab%252Ftree%252Feopf-101%252F06_eopf_zarr_in_action%252F64_flood_mapping_valencia.ipynb%23fancy-forms-config=%7B%22profile%22%3A%22choose-your-environment%22%2C%22image%22%3A%22unlisted_choice%22%2C%22image%3Aunlisted_choice%22%3A%224zm3809f.c1.de1.container-registry.ovh.net%2Feopf-toolkit-python%2Feopf-toolkit-python%3Alatest%22%2C%22autoStart%22%3A%22true%22%7D\" target=\"_blank\">\n",
    "  <button style=\"background-color:#0072ce; color:white; padding:0.6em 1.2em; font-size:1rem; border:none; border-radius:6px; margin-top:1em;\">\n",
    "    üöÄ Launch this notebook in JupyterLab\n",
    "  </button>\n",
    "</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2",
   "metadata": {},
   "source": [
    "### Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "**Sentinel-1 GRD** data is particularly valuable to detect water and underwater areas. Synthetic Aperture Radar (SAR) can capture images day and night, in any weather, a feature especially important for flooding events, where cloudy and rainy weather can persist for weeks. This makes it far more reliable than optical sensors during storms.\n",
    "\n",
    "With its frequent revisits, wide coverage, and free high-resolution data, **Sentinel-1** enables the rapid mapping of flood extents, as will be demonstrated in this workflow. **VV** polarization is preferred for flood mapping due to its sensitivity to water surfaces, which typically appear darker in the images compared to land surfaces."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4",
   "metadata": {},
   "source": [
    "#### The Flooding Event"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5",
   "metadata": {},
   "source": [
    "On October 29, 2024, the city of Valencia (Spain) was hit by catastrophic flooding caused by intense storms, leaving over 230 deaths and billions in damages. This disaster was part of Europe‚Äôs worst flood year in over a decade, with hundreds of thousands affected continent-wide. Such events highlight the urgent need for reliable flood monitoring to support **emergency response**, damage assessment and long-term resilience planning.\n",
    "\n",
    "With respect to this event, we will demonstrate how to use **Sentinel-1 GRD** data to map flood extents. We will use 14 **Sentinel-1 GRD** images from the **IW** swath, covering the city and metropolitan area of Valencia from October 7, 2024 to March 24, 2025. This includes 2 images captured before, 1 immediately after the heavy rains, and 11 images taken after the flooding event, until the water levels got back to normal:\n",
    "- October 7, 2424 (before)\n",
    "- October 19, 2024 (before)\n",
    "- October 31, 2024 (right after the event)\n",
    "- November 12, 2024 (after)\n",
    "- November 24, 2024 (after)\n",
    "- December 6, 2024 (after)\n",
    "- December 18, 2024 (after)\n",
    "- December 30, 2024 (after)\n",
    "- January 11, 2025 (after)\n",
    "- January 23, 2025 (after)\n",
    "- February 4, 2025 (after)\n",
    "- February 16, 2025 (after)\n",
    "- March 12, 2025 (after)\n",
    "- March 24, 2025 (after)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6",
   "metadata": {},
   "source": [
    "#### What we will learn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7",
   "metadata": {},
   "source": [
    "- üåä How to create a workflow to map flood events.\n",
    "- ‚öíÔ∏è Basic SAR processing tools.\n",
    "- üìä How to create a data cube to perform time series analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8",
   "metadata": {},
   "source": [
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9",
   "metadata": {},
   "source": [
    "#### Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xarray as xr \n",
    "import xarray_sentinel \n",
    "import pandas as pd\n",
    "import matplotlib as plt\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "import dask                             # these last two libraries are imported to open the datasets faster\n",
    "from dask.distributed import Client     # and in the end take advantage of the optimized .zarr format\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11",
   "metadata": {},
   "source": [
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## Data pre-processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13",
   "metadata": {},
   "source": [
    "To search and load the data needed for the analysis, we will follow the processes we presented in [Sentinel-1 GRD structure tutorial](/02_about_eopf_zarr/22_zarr_structure_S1GRD.ipynb) and [S1 basic operations tutorial](/02_about_eopf_zarr/23_S1_basic_operations.ipynb).\n",
    "\n",
    "Once we defined our interest Sentinel-1 GRD items, we can see that they contain both **VH** and **VV** polarizations.<br>\n",
    "For this flood mapping context, **VV** polarization is the choice of interest, as water backscatter is much more visible with it, rather than with VH."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14",
   "metadata": {},
   "source": [
    "### Loading the datatree"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15",
   "metadata": {},
   "source": [
    "The list below shows the names of the products we will use for the flood mapping and time series analysis.<br>\n",
    "As we have seen in previous chapters, these names already contain valuable information that can be used to search for specific products within the [EOPF STAC catalogue]()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "scenes = [\"S1A_IW_GRDH_1SDV_20241007T180256_20241007T180321_056000_06D943_D46B\", \n",
    "          \"S1A_IW_GRDH_1SDV_20241019T180256_20241019T180321_056175_06E02E_2D52\", \n",
    "          \"S1A_IW_GRDH_1SDV_20241031T180256_20241031T180321_056350_06E71E_479F\", \n",
    "          \"S1A_IW_GRDH_1SDV_20241112T180255_20241112T180320_056525_06EE16_DC29\", \n",
    "          \"S1A_IW_GRDH_1SDV_20241124T180254_20241124T180319_056700_06F516_BA27\", \n",
    "          \"S1A_IW_GRDH_1SDV_20241206T180253_20241206T180318_056875_06FBFD_25AD\", \n",
    "          \"S1A_IW_GRDH_1SDV_20241218T180252_20241218T180317_057050_0702F2_0BC2\", \n",
    "          \"S1A_IW_GRDH_1SDV_20241230T180251_20241230T180316_057225_0709DD_15AC\", \n",
    "          \"S1A_IW_GRDH_1SDV_20250111T180250_20250111T180315_057400_0710C7_ADBB\", \n",
    "          \"S1A_IW_GRDH_1SDV_20250123T180249_20250123T180314_057575_0717B9_A784\", \n",
    "          \"S1A_IW_GRDH_1SDV_20250204T180249_20250204T180314_057750_071EA2_4373\", \n",
    "          \"S1A_IW_GRDH_1SDV_20250216T180248_20250216T180313_057925_0725AE_8AC7\", \n",
    "          \"S1A_IW_GRDH_1SDV_20250312T180248_20250312T180313_058275_0733E6_4F5B\", \n",
    "          \"S1A_IW_GRDH_1SDV_20250324T180248_20250324T180313_058450_073AD0_04B7\", \n",
    "          ]\n",
    "\n",
    "zarr_paths = []\n",
    "for scene in scenes:\n",
    "    zarr_paths.append(f\"https://objects.eodc.eu/e05ab01a9d56408d82ac32d69a5aae2a:notebook-data/tutorial_data/cpm_v260/{scene}.zarr\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17",
   "metadata": {},
   "source": [
    "Next, we will load all `zarr` datasets as xarray.Datatrees. Here **we are not reading** the entire dataset from the store; but, creating a set of references to the data, which enables us to access it efficiently later in the analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = Client()  # Set up local cluster on your laptop\n",
    "client\n",
    "\n",
    "@dask.delayed\n",
    "def load_datatree_delayed(path):\n",
    "    return xr.open_datatree(path, consolidated=True, chunks=\"auto\")\n",
    "\n",
    "# Create delayed objects\n",
    "delayed_datatrees = [load_datatree_delayed(path) for path in zarr_paths]\n",
    "# Compute in parallel\n",
    "datatrees = dask.compute(*delayed_datatrees)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19",
   "metadata": {},
   "source": [
    "Each element inside the `datatree` list is a datatree and corresponds to a Sentinel-1 GRD scene datatree present on the list above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Each element inside the datatree list is a datatree and corresponds to a Sentinel-1 GRD scene datatree present on the list above\n",
    "type(datatrees[0]) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21",
   "metadata": {},
   "source": [
    "### Defining variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of scenes we are working with for the time series analysis\n",
    "DATASET_NUMBER = len(datatrees) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23",
   "metadata": {},
   "source": [
    "If we run the following commented out code line we will be able to see how each datatree is organized within its groups and subgroups (as explained in this [section](./22_zarr_structure_S1GRD.ipynb)). From this datatree, we took the groups and subgroups constant `ID` numbers used to open specific grouos and variables such as:\n",
    "- Measurements group = 7 so, in order to open this group, on the first element of our list of scenes, over the first polarization `VV`, we do `datatrees[0][datatrees[0].groups[7]]`\n",
    "- Calibration group = 33 so, in order to open this group, on the first element of our list of scenes, over the first polarization `VV`, we do `datatrees[0][datatrees[0].groups[33]]`\n",
    "\n",
    "Over the course of this notebook these `IDs` will be used to call variables and compute some other functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Opening the measurements group from the datatree\n",
    "datatrees[0][datatrees[0].groups[7]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some other important constant ID numbers \n",
    "MEASUREMENTS_GROUP_ID = 7\n",
    "GCP_GROUP_ID = 28\n",
    "CALIBRATION_GROUP_ID = 33"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26",
   "metadata": {},
   "source": [
    "We now define the thresholds that will be used for the flood mapping analysis. These values are not fixed and they can be calibrated and adjusted to achieve a better fit for different regions or flood events.<br>\n",
    "\n",
    "In SAR imagery, open water surfaces typically appear very dark because they reflect the radar signal away from the sensor. This results in low backscatter values. In our case, pixels with a backscatter lower than approximately ‚Äì15 dB are likely to correspond to water."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27",
   "metadata": {},
   "outputs": [],
   "source": [
    "WATER_THRESHOLD_DB = -15"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28",
   "metadata": {},
   "source": [
    "It is interesting to study the flood event over a specific point within the area of interest.<br>\n",
    "Therefore, we are storing the coordinates of an anchor point inside the area which is not usually covered by water. After the heavy rain, it became flooded for a few weeks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29",
   "metadata": {},
   "outputs": [],
   "source": [
    "TARGET_LAT = 39.28\n",
    "TARGET_LONG = -0.30"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30",
   "metadata": {},
   "source": [
    "## Extracting information from the `.zarr`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31",
   "metadata": {},
   "source": [
    "As explained in the [S1 basic operations tutorial](23_S1_basic_operations.ipynb), we will perform over all the selected data the following operations:\n",
    "\n",
    "- Slicing the data to meet our area of interest and decimate it\n",
    "- Assigning latitude and longitude coordinates to the dataset\n",
    "- Computing the backscatter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32",
   "metadata": {},
   "source": [
    "### Slicing and decimating GRD variable"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33",
   "metadata": {},
   "source": [
    "To begin with, we access all our `.zarr` items `measurements` groups by creating a list storing all of them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34",
   "metadata": {},
   "outputs": [],
   "source": [
    "measurements = []\n",
    "# Looping to populate the measurements list with only the measurements groups of each dataset on the datatree list\n",
    "for i in range(DATASET_NUMBER):\n",
    "    measurements.append(datatrees[i][datatrees[i].groups[MEASUREMENTS_GROUP_ID]].to_dataset())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35",
   "metadata": {},
   "source": [
    "We continue by slicing and decimating `grd`'s data for our area of interest around Valencia.\n",
    "\n",
    "We'll use Ground Control Points (GCPs) to perform geographically-based slicing. This ensures that pixels from different products represent the same geographical areas, making them suitable for time series analysis.\n",
    "\n",
    "First, let's define a bounding box around our target area in Valencia:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define bounding box around Valencia (expanded from target coordinates)\n",
    "bbox = [\n",
    "    TARGET_LONG - 0.15,  # min longitude\n",
    "    TARGET_LAT - 0.10,   # min latitude  \n",
    "    TARGET_LONG + 0.15,  # max longitude\n",
    "    TARGET_LAT + 0.10    # max latitude\n",
    "]\n",
    "\n",
    "print(f\"Bounding box: {bbox}\")\n",
    "print(f\"Longitude range: {bbox[0]:.2f} to {bbox[2]:.2f}\")\n",
    "print(f\"Latitude range: {bbox[1]:.2f} to {bbox[3]:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting the first decimated GRD product from our list, corresponding to the whole scene\n",
    "measurements[0].grd.isel(\n",
    "        azimuth_time=slice(None, None, 20),\n",
    "        ground_range=slice(None, None, 20)).plot(vmax=300)\n",
    "plt.show()\n",
    "\n",
    "print(\"Azimuth time has\", measurements[0].grd.shape[0], \"values.\")\n",
    "print(\"Ground range has\", measurements[0].grd.shape[1], \"values.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38",
   "metadata": {},
   "source": [
    "Now we'll implement helper functions for GCP-based spatial slicing and geocoding:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_slice(shape, idx, offset=2):\n",
    "    \"\"\"\n",
    "    Builds two slice objects around a given index, clamped within the shape bounds.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    shape : tuple of int\n",
    "        The dimensions of the array (e.g., (height, width)).\n",
    "    idx : sequence of tensors or scalars\n",
    "        The index with two elements.\n",
    "    offset : int, optional\n",
    "        The number of elements to include on each side of the index (default is 2).\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    list of slice\n",
    "        A list containing two slice objects for each dimension.\n",
    "    \"\"\"\n",
    "    i0 = int(min(idx[0]))\n",
    "    i1 = int(max(idx[1]))\n",
    "\n",
    "    def clamp_slice(i, dim_size):\n",
    "        start = max(0, i - offset)\n",
    "        end = min(dim_size - 1, i + offset)\n",
    "        return slice(start, end + 1)\n",
    "\n",
    "    return [clamp_slice(i0, shape[0]), clamp_slice(i1, shape[1])]\n",
    "\n",
    "\n",
    "def create_regular_grid(min_x, max_x, min_y, max_y, spatialres):\n",
    "    \"\"\"\n",
    "    Create a regular coordinate grid given bounding box limits.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    min_x, max_x : float\n",
    "        Minimum and maximum X coordinates (e.g., longitude or projected X).\n",
    "    min_y, max_y : float\n",
    "        Minimum and maximum Y coordinates (e.g., latitude or projected Y).\n",
    "    spatialres : float\n",
    "        Desired spatial resolution (in same units as x/y).\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    grid_x_regular : ndarray\n",
    "        2D array of regularly spaced X coordinates.\n",
    "    grid_y_regular : ndarray\n",
    "        2D array of regularly spaced Y coordinates.\n",
    "    \"\"\"\n",
    "    # Ensure positive dimensions and consistent spacing\n",
    "    width = int(np.ceil((max_x - min_x) / spatialres))\n",
    "    height = int(np.ceil((max_y - min_y) / spatialres))\n",
    "\n",
    "    # Compute grid centers (half-pixel offset)\n",
    "    half_pixel = spatialres / 2.0\n",
    "    x_regular = np.linspace(\n",
    "        min_x + half_pixel, max_x - half_pixel, width, dtype=np.float32\n",
    "    )\n",
    "    y_regular = np.linspace(\n",
    "        min_y + half_pixel, max_y - half_pixel, height, dtype=np.float32\n",
    "    )\n",
    "\n",
    "    grid_x_regular, grid_y_regular = np.meshgrid(x_regular, y_regular)\n",
    "\n",
    "    return grid_x_regular, grid_y_regular\n",
    "\n",
    "\n",
    "def geocode_grd(sigma_0, grid_x_regular, grid_y_regular):\n",
    "    \"\"\"\n",
    "    Geocode GRD data to a regular grid using nearest neighbor interpolation.\n",
    "    \"\"\"\n",
    "    from scipy.interpolate import griddata\n",
    "    \n",
    "    grid_lat = sigma_0.latitude.values\n",
    "    grid_lon = sigma_0.longitude.values\n",
    "\n",
    "    # Set the border values to zero to avoid border artifacts with nearest interpolator\n",
    "    sigma_0_copy = sigma_0.copy()\n",
    "    sigma_0_copy.data[[0, -1], :] = 0\n",
    "    sigma_0_copy.data[:, [0, -1]] = 0\n",
    "\n",
    "    interpolated_values_grid = griddata(\n",
    "        (grid_lon.flatten(), grid_lat.flatten()),\n",
    "        sigma_0_copy.values.flatten(),\n",
    "        (grid_x_regular, grid_y_regular),\n",
    "        method=\"nearest\",\n",
    "    )\n",
    "\n",
    "    ds = xr.Dataset(\n",
    "        coords=dict(\n",
    "            time=([\"time\"], [sigma_0.time.values]),\n",
    "            y=([\"y\"], grid_y_regular[:, 0]),\n",
    "            x=([\"x\"], grid_x_regular[0, :]),\n",
    "        )\n",
    "    )\n",
    "    ds[\"grd\"] = ((\"time\", \"y\", \"x\"), np.expand_dims(interpolated_values_grid, 0))\n",
    "    ds = ds.where(ds != 0)\n",
    "\n",
    "    return ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now perform GCP-based spatial slicing for each product\n",
    "grd = []\n",
    "gcps_list = []\n",
    "\n",
    "for i in range(DATASET_NUMBER):\n",
    "    print(f\"Processing dataset {i+1}/{DATASET_NUMBER}...\")\n",
    "    \n",
    "    # Access GRD and GCP data\n",
    "    grd_group = measurements[i].grd\n",
    "    \n",
    "    # Get GCPs from the corresponding datatree\n",
    "    # Find the measurements group name dynamically\n",
    "    group_name = datatrees[i].groups[MEASUREMENTS_GROUP_ID]\n",
    "    gcps = datatrees[i][group_name].conditions.gcp.to_dataset()[[\"latitude\", \"longitude\"]]\n",
    "    \n",
    "    # Create mask based on bounding box\n",
    "    mask = (\n",
    "        (gcps.latitude < bbox[3])    # lat < max_lat\n",
    "        & (gcps.latitude > bbox[1])  # lat > min_lat\n",
    "        & (gcps.longitude < bbox[2]) # lon < max_lon\n",
    "        & (gcps.longitude > bbox[0]) # lon > min_lon\n",
    "    )\n",
    "    \n",
    "    # Find indices where mask is True\n",
    "    idx = np.where(mask == 1)\n",
    "    \n",
    "    if len(idx[0]) == 0:\n",
    "        print(f\"Warning: No GCPs found within bounding box for dataset {i}\")\n",
    "        continue\n",
    "    \n",
    "    # Build slices around the found indices\n",
    "    azimuth_time_slice, ground_range_slice = build_slice(mask.shape, idx)\n",
    "    \n",
    "    # Crop GCPs to the area of interest\n",
    "    gcps_crop = gcps.isel(\n",
    "        dict(azimuth_time=azimuth_time_slice, ground_range=ground_range_slice)\n",
    "    )\n",
    "    \n",
    "    # Get min/max coordinates for final slicing\n",
    "    azimuth_time_min = gcps_crop.azimuth_time.min().values\n",
    "    azimuth_time_max = gcps_crop.azimuth_time.max().values\n",
    "    ground_range_min = gcps_crop.ground_range.min().values\n",
    "    ground_range_max = gcps_crop.ground_range.max().values\n",
    "    \n",
    "    # Crop and decimate GRD data\n",
    "    grd_crop = grd_group.sel(\n",
    "        azimuth_time=slice(azimuth_time_min, azimuth_time_max),\n",
    "        ground_range=slice(ground_range_min, ground_range_max)\n",
    "    ).isel(\n",
    "        azimuth_time=slice(None, None, 10),  # Decimate by factor of 10\n",
    "        ground_range=slice(None, None, 10)\n",
    "    )\n",
    "    \n",
    "    # Interpolate GCPs to match decimated GRD data\n",
    "    gcps_crop_interp = gcps_crop.interp_like(grd_crop)\n",
    "    \n",
    "    # Assign coordinates to GRD data\n",
    "    grd_crop = grd_crop.assign_coords(\n",
    "        {\"latitude\": gcps_crop_interp.latitude, \"longitude\": gcps_crop_interp.longitude}\n",
    "    )\n",
    "    \n",
    "    # Apply final mask to ensure we only keep data within our AOI\n",
    "    final_mask = (\n",
    "        (gcps_crop_interp.latitude < bbox[3])\n",
    "        & (gcps_crop_interp.latitude > bbox[1])\n",
    "        & (gcps_crop_interp.longitude < bbox[2])\n",
    "        & (gcps_crop_interp.longitude > bbox[0])\n",
    "    )\n",
    "    grd_crop = grd_crop.where(final_mask.compute(), drop=True)\n",
    "    \n",
    "    grd.append(grd_crop)\n",
    "    gcps_list.append(gcps_crop_interp)\n",
    "    \n",
    "    print(f\"Dataset {i+1} cropped to shape: {grd_crop.shape}\")\n",
    "\n",
    "print(f\"\\nProcessed {len(grd)} datasets successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check shape of processed data\n",
    "if len(grd) > 1:\n",
    "    print(f\"GRD data shape: {grd[1].shape}\")\n",
    "    \n",
    "    # Plotting the second sliced and decimated GRD product from our list with coordinates\n",
    "    grd[1].plot(x=\"longitude\", y=\"latitude\", vmax=300)\n",
    "    plt.title(\"GCP-based sliced GRD product for Valencia AOI\")\n",
    "    plt.xlabel(\"Longitude\")\n",
    "    plt.ylabel(\"Latitude\")\n",
    "    \n",
    "    # Add target point\n",
    "    plt.scatter(TARGET_LONG, TARGET_LAT, color=\"red\", marker=\"o\", s=50, label=\"Target Point\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"No data to display\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42",
   "metadata": {},
   "source": [
    "### Creating Regular Grid for Time Series Analysis\n",
    "\n",
    "Now that we have geographically aligned data, let's create a regular grid to enable proper time series analysis. This step geocodes all products to the same regular grid, ensuring that each pixel represents the same geographical area across all time steps."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43",
   "metadata": {},
   "source": [
    "First, let's determine the common geographic bounds across all datasets and create a regular grid:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract min/max values for longitude and latitude to build a common regular grid\n",
    "if len(grd) > 0:\n",
    "    # Collect all latitude and longitude values from all datasets\n",
    "    all_lats = []\n",
    "    all_lons = []\n",
    "    \n",
    "    for i in range(len(grd)):\n",
    "        if 'latitude' in grd[i].coords and 'longitude' in grd[i].coords:\n",
    "            lats = grd[i].latitude.values\n",
    "            lons = grd[i].longitude.values\n",
    "            \n",
    "            # Remove NaN values\n",
    "            valid_mask = ~(np.isnan(lats) | np.isnan(lons))\n",
    "            all_lats.extend(lats[valid_mask].flatten())\n",
    "            all_lons.extend(lons[valid_mask].flatten())\n",
    "    \n",
    "    # Calculate bounds\n",
    "    min_lon = np.min(all_lons)\n",
    "    max_lon = np.max(all_lons)\n",
    "    min_lat = np.min(all_lats)\n",
    "    max_lat = np.max(all_lats)\n",
    "    \n",
    "    print(f\"Geographic bounds:\")\n",
    "    print(f\"Longitude: {min_lon:.4f} to {max_lon:.4f}\")\n",
    "    print(f\"Latitude: {min_lat:.4f} to {max_lat:.4f}\")\n",
    "    \n",
    "    # Create regular grid with 0.001 degree resolution (~110m)\n",
    "    spatial_resolution = 0.001\n",
    "    grid_x_regular, grid_y_regular = create_regular_grid(\n",
    "        min_lon, max_lon, min_lat, max_lat, spatial_resolution\n",
    "    )\n",
    "    \n",
    "    print(f\"Regular grid shape: {grid_x_regular.shape}\")\n",
    "else:\n",
    "    print(\"No data available for grid creation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Geocode all GRD datasets to the common regular grid\n",
    "grd_geocoded = []\n",
    "\n",
    "if len(grd) > 0 and 'grid_x_regular' in locals():\n",
    "    for i in range(len(grd)):\n",
    "        print(f\"Geocoding dataset {i+1}/{len(grd)}...\")\n",
    "        \n",
    "        # Add time coordinate if not present\n",
    "        if 'time' not in grd[i].coords:\n",
    "            # Extract time from the original datatree\n",
    "            group_name = datatrees[i].groups[MEASUREMENTS_GROUP_ID]\n",
    "            time_val = datatrees[i][group_name].attrs.get('start_time', \n",
    "                      datatrees[i].attrs.get('start_time', f'2024-{i+1:02d}-01'))\n",
    "            grd[i] = grd[i].assign_coords(time=pd.to_datetime(time_val))\n",
    "        \n",
    "        # Geocode to regular grid\n",
    "        geocoded_ds = geocode_grd(grd[i], grid_x_regular, grid_y_regular)\n",
    "        grd_geocoded.append(geocoded_ds)\n",
    "        \n",
    "        print(f\"Dataset {i+1} geocoded to regular grid shape: {geocoded_ds.grd.shape}\")\n",
    "\n",
    "    print(f\"\\nGeocoded {len(grd_geocoded)} datasets to regular grid.\")\n",
    "else:\n",
    "    print(\"Skipping geocoding - no data or grid available\")\n",
    "    grd_geocoded = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot comparison between original and geocoded data\n",
    "if len(grd) > 1 and len(grd_geocoded) > 1:\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
    "    \n",
    "    # Original data (irregular grid)\n",
    "    grd[1].plot(x=\"longitude\", y=\"latitude\", vmax=300, ax=ax1)\n",
    "    ax1.set_title(\"Original GRD (irregular grid)\")\n",
    "    ax1.scatter(TARGET_LONG, TARGET_LAT, color=\"red\", marker=\"o\", s=50, label=\"Target\")\n",
    "    ax1.legend()\n",
    "    \n",
    "    # Geocoded data (regular grid)\n",
    "    grd_geocoded[1].grd.squeeze().plot(x=\"x\", y=\"y\", vmax=300, ax=ax2)\n",
    "    ax2.set_title(\"Geocoded GRD (regular grid)\")\n",
    "    ax2.set_xlabel(\"Longitude\")\n",
    "    ax2.set_ylabel(\"Latitude\")\n",
    "    ax2.scatter(TARGET_LONG, TARGET_LAT, color=\"red\", marker=\"o\", s=50, label=\"Target\")\n",
    "    ax2.legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Print information about the geocoded grid\n",
    "    print(f\"Geocoded data info:\")\n",
    "    print(f\"Shape: {grd_geocoded[1].grd.shape}\")\n",
    "    print(f\"X (longitude) range: {grd_geocoded[1].x.min().values:.4f} to {grd_geocoded[1].x.max().values:.4f}\")\n",
    "    print(f\"Y (latitude) range: {grd_geocoded[1].y.min().values:.4f} to {grd_geocoded[1].y.max().values:.4f}\")\n",
    "elif len(grd) > 0:\n",
    "    # Fallback to just showing original data\n",
    "    grd[0].plot(x=\"longitude\", y=\"latitude\", vmax=300)\n",
    "    plt.title(\"GRD product with latitude and longitude coordinates\")\n",
    "    plt.scatter(TARGET_LONG, TARGET_LAT, color=\"red\", marker=\"o\", s=50, label=\"Target\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"No data to display\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47",
   "metadata": {},
   "source": [
    "### Computing backscatter\n",
    "\n",
    "Now we'll compute the backscatter for our geocoded data. The geocoded data is now properly aligned on a regular grid, making it suitable for time series analysis and comparison across different acquisition dates."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48",
   "metadata": {},
   "source": [
    "We'll now compute backscatter intensity for both our original sliced data and our geocoded data. The geocoded data provides properly aligned pixels for time series analysis.\n",
        "\n",
        "For the original data, we need to access and decimate the calibration values to match our sliced GRD data. For the geocoded data, we'll compute backscatter first on the original data and then geocode the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute backscatter for original sliced data (if available)\n",
    "intensity = []\n",
    "calibration = []\n",
    "\n",
    "if len(grd) > 0:\n",
    "    # Get calibration data for each dataset and interpolate to match decimated GRD data\n",
    "    for i in range(len(grd)):\n",
    "        print(f\"Processing calibration for dataset {i+1}/{len(grd)}...\")\n",
    "        \n",
    "        # Get calibration data\n",
    "        cal_data = datatrees[i][datatrees[i].groups[CALIBRATION_GROUP_ID]].to_dataset()\n",
    "        \n",
    "        # Interpolate calibration to match the decimated GRD data\n",
    "        cal_interp = cal_data.interp_like(grd[i])\n",
    "        calibration.append(cal_interp)\n",
    "        \n",
    "        # Compute backscatter intensity\n",
    "        intensity_data = xarray_sentinel.calibrate_intensity(\n",
    "            grd[i], \n",
    "            cal_interp.beta_nought, \n",
    "            as_db=True\n",
    "        )\n",
    "        intensity.append(intensity_data)\n",
    "        \n",
    "        print(f\"Computed intensity for dataset {i+1}, shape: {intensity_data.shape}\")\n",
    "else:\n",
    "    print(\"No GRD data available for backscatter computation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute backscatter for geocoded data\n",
    "intensity_geocoded = []\n",
    "\n",
    "if len(grd_geocoded) > 0:\n",
    "    print(\"\\nComputing backscatter for geocoded data...\")\n",
    "    \n",
    "    for i in range(len(grd_geocoded)):\n",
    "        # The geocoded data already contains GRD values\n",
    "        # We'll convert from linear to dB scale for the geocoded data\n",
    "        grd_db = 10 * np.log10(grd_geocoded[i].grd.where(grd_geocoded[i].grd > 0))\n",
    "        \n",
    "        # Create a new dataset with the dB values\n",
    "        intensity_ds = grd_geocoded[i].copy()\n",
    "        intensity_ds['intensity'] = grd_db\n",
    "        intensity_ds = intensity_ds.drop_vars('grd')\n",
    "        \n",
    "        intensity_geocoded.append(intensity_ds)\n",
    "        \n",
    "        print(f\"Computed geocoded intensity for dataset {i+1}\")\n",
    "    \n",
    "    print(f\"Processed {len(intensity_geocoded)} geocoded intensity datasets.\")\n",
    "else:\n",
    "    print(\"No geocoded data available for intensity computation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot comparison of backscatter intensity results\n",
    "if len(intensity) > 1 and len(intensity_geocoded) > 1:\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
    "    \n",
    "    # Original backscatter (irregular grid)\n",
    "    intensity[1].plot(x=\"longitude\", y=\"latitude\", vmin=-25, vmax=5, ax=ax1)\n",
    "    ax1.set_title(\"Backscatter Intensity (irregular grid)\")\n",
    "    ax1.scatter(TARGET_LONG, TARGET_LAT, color=\"red\", marker=\"o\", s=50, label=\"Target\")\n",
    "    ax1.legend()\n",
    "    \n",
    "    # Geocoded backscatter (regular grid)\n",
    "    intensity_geocoded[1].intensity.squeeze().plot(x=\"x\", y=\"y\", vmin=-25, vmax=5, ax=ax2)\n",
    "    ax2.set_title(\"Backscatter Intensity (regular grid)\")\n",
    "    ax2.set_xlabel(\"Longitude\")\n",
    "    ax2.set_ylabel(\"Latitude\")\n",
    "    ax2.scatter(TARGET_LONG, TARGET_LAT, color=\"red\", marker=\"o\", s=50, label=\"Target\")\n",
    "    ax2.legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "elif len(intensity) > 1:\n",
    "    # Fallback to original data only\n",
    "    intensity[1].plot(x=\"longitude\", y=\"latitude\", vmin=-25, vmax=5)\n",
    "    plt.title(\"Computed backscatter intensity\")\n",
    "    plt.scatter(TARGET_LONG, TARGET_LAT, color=\"red\", marker=\"o\", s=50, label=\"Target\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"Insufficient data for plotting backscatter intensity\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52",
   "metadata": {},
   "source": [
    "## Creating the data cube\n",
    "\n",
    "Now we have both irregular and regular grid data. The regular grid (geocoded) data is particularly valuable for time series analysis because all pixels are aligned and represent the same geographical areas across different acquisition dates."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53",
   "metadata": {},
   "source": [
    "Now we can create proper data cubes from both our irregular grid data and our geocoded regular grid data. The geocoded data cube will provide properly aligned pixels for accurate time series analysis.\n",
    "\n",
    "First, let's extract acquisition dates for the time dimension:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract acquisition dates from the datatrees\n",
    "acquisition_dates = []\n",
    "\n",
    "for i in range(min(len(datatrees), len(intensity))):\n",
    "    # Try to get the start time from datatree attributes\n",
    "    group_name = datatrees[i].groups[MEASUREMENTS_GROUP_ID] if i < len(datatrees) else None\n",
    "    \n",
    "    if group_name and hasattr(datatrees[i][group_name], 'attrs'):\n",
    "        start_time = datatrees[i][group_name].attrs.get('start_time')\n",
    "        if start_time:\n",
    "            acquisition_dates.append(pd.to_datetime(start_time).date())\n",
    "        else:\n",
    "            # Fallback to azimuth_time if available\n",
    "            if len(intensity) > i and 'azimuth_time' in intensity[i].coords:\n",
    "                date_val = intensity[i].azimuth_time.values[1].astype('datetime64[D]')\n",
    "                acquisition_dates.append(pd.to_datetime(date_val).date())\n",
    "            else:\n",
    "                # Create a dummy date\n",
    "                acquisition_dates.append(pd.to_datetime(f'2024-{i+1:02d}-01').date())\n",
    "    else:\n",
    "        # Create a dummy date\n",
    "        acquisition_dates.append(pd.to_datetime(f'2024-{i+1:02d}-01').date())\n",
    "\n",
    "print(f\"Acquisition dates: {acquisition_dates}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55",
   "metadata": {},
   "source": [
    "### Creating Data Cubes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56",
   "metadata": {},
   "source": [
    "We'll create two different data cubes:\n",
    "\n",
    "1. **Regular Grid Data Cube (Geocoded)**: This uses our geocoded data where all pixels are properly aligned on the same regular geographic grid. Each pixel represents the same area on the ground across all time steps - this is the **recommended approach** for time series analysis.\n",
    "\n",
    "2. **Irregular Grid Data Cube (Coordinate Aligned)**: This uses the original coordinate alignment method for comparison, but note that pixels may not represent exactly the same ground locations across dates.\n",
    "\n",
    "#### Regular Grid Data Cube (Recommended)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create regular grid data cube from geocoded intensity data\n",
    "if len(intensity_geocoded) > 0:\n",
    "    print(\"Creating regular grid data cube...\")\n",
    "    \n",
    "    # Prepare geocoded intensity datasets for stacking\n",
    "    geocoded_datasets = []\n",
    "    valid_dates = []\n",
    "    \n",
    "    for i, ds in enumerate(intensity_geocoded):\n",
    "        if i < len(acquisition_dates):\n",
    "            # Add time coordinate\n",
    "            ds_with_time = ds.assign_coords(time=pd.to_datetime(acquisition_dates[i]))\n",
    "            geocoded_datasets.append(ds_with_time)\n",
    "            valid_dates.append(acquisition_dates[i])\n",
    "    \n",
    "    # Stack into a data cube along time dimension\n",
    "    if len(geocoded_datasets) > 0:\n",
    "        intensity_datacube_regular = xr.concat(geocoded_datasets, dim='time')\n",
    "        print(f\"Regular grid data cube shape: {intensity_datacube_regular.intensity.shape}\")\n",
    "        print(f\"Coordinates: {list(intensity_datacube_regular.coords.keys())}\")\n",
    "    else:\n",
    "        intensity_datacube_regular = None\n",
    "        print(\"No geocoded datasets available for regular grid data cube\")\n",
    "else:\n",
    "    intensity_datacube_regular = None\n",
    "    print(\"No geocoded data available for regular grid data cube\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58",
   "metadata": {},
   "source": [
    "#### Irregular Grid Data Cube (For Comparison)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create irregular grid data cube using coordinate alignment (original method)\n",
    "if len(intensity) > 0:\n",
    "    print(\"\\nCreating irregular grid data cube for comparison...\")\n",
    "    \n",
    "    # Use coordinate alignment approach\n",
    "    reference_coords = intensity[0].coords\n",
    "    datasets_aligned = []\n",
    "    \n",
    "    for ds in intensity:\n",
    "        ds_no_coords = ds.reset_coords(drop=True)\n",
    "        datasets_aligned.append(ds_no_coords.assign_coords(reference_coords))\n",
    "    \n",
    "    # Create time data array\n",
    "    time_data = [pd.to_datetime(date) for date in acquisition_dates[:len(datasets_aligned)]]\n",
    "    \n",
    "    # Stack into data cube\n",
    "    intensity_data_cube = xr.concat(datasets_aligned, dim=xr.DataArray(time_data, dims=\"time\"))\n",
    "    \n",
    "    print(f\"Irregular grid data cube shape: {intensity_data_cube.shape}\")\n",
    "    print(f\"‚ö†Ô∏è  Note: Pixels may not represent the same ground locations across dates\")\n",
    "else:\n",
    "    intensity_data_cube = None\n",
    "    print(\"No intensity data available for irregular grid data cube\")\n",
    "\n",
    "# Display the data cubes\n",
    "if intensity_datacube_regular is not None:\n",
    "    print(\"\\n=== Regular Grid Data Cube (Recommended) ===\")\n",
    "    display(intensity_datacube_regular)\n",
    "\n",
    "if intensity_data_cube is not None:\n",
    "    print(\"\\n=== Irregular Grid Data Cube (For Comparison) ===\")\n",
    "    display(intensity_data_cube)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## Flood mapping and time series analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61",
   "metadata": {},
   "source": [
    "The last step is to perform the time series and flood mapping analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62",
   "metadata": {},
   "source": [
    "### Visualization of Time Series Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63",
   "metadata": {},
   "source": [
    "Now we can visualize our time series data. We'll compare both the regular grid (geocoded) and irregular grid data cubes to demonstrate the difference.\n",
    "\n",
    "The regular grid data ensures that each pixel represents the same geographical area across all time steps, making it much more suitable for accurate time series analysis and flood detection.\n",
    "\n",
    "#### Regular Grid Time Series (Recommended)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Flood Detection Using Threshold Values\n",
    "\n",
    "Now we'll demonstrate flood detection using both data cubes. Water appears as darker pixels in SAR imagery, typically with backscatter values lower than **-15 dB**.\n",
    "\n",
    "The regular grid approach provides more accurate flood mapping because:\n",
    "- Each pixel represents the same geographical area across all dates\n",
    "- Changes in backscatter values accurately reflect changes in surface conditions\n",
    "- Time series analysis is geographically meaningful\n",
    "\n",
    "#### Regular Grid Flood Maps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot regular grid (geocoded) time series\n",
    "if intensity_datacube_regular is not None:\n",
    "    n_times = len(intensity_datacube_regular.time)\n",
    "    cols = 4\n",
    "    rows = int(np.ceil(n_times / cols))\n",
    "    \n",
    "    fig, axes = plt.subplots(rows, cols, figsize=(4*cols, 4*rows))\n",
    "    if rows == 1 and cols == 1:\n",
    "        axes = [axes]\n",
    "    else:\n",
    "        axes = axes.flatten()\n",
    "    \n",
    "    print(f\"Plotting {n_times} time steps from regular grid data cube\")\n",
    "    \n",
    "    for i in range(n_times):\n",
    "        ax = axes[i]\n",
    "        \n",
    "        # Plot the geocoded intensity data\n",
    "        intensity_datacube_regular.intensity.isel(time=i).plot(\n",
    "            x=\"x\", y=\"y\",\n",
    "            vmin=-25, vmax=5,\n",
    "            ax=ax,\n",
    "            add_colorbar=False\n",
    "        )\n",
    "        \n",
    "        ax.scatter(TARGET_LONG, TARGET_LAT, color=\"red\", marker=\"o\", s=20, label=\"Target Point\")\n",
    "        ax.set_xlabel(\"Longitude\")\n",
    "        ax.set_ylabel(\"Latitude\")\n",
    "        \n",
    "        # Add date to title\n",
    "        time_val = intensity_datacube_regular.time.values[i]\n",
    "        ax.set_title(f\"Regular Grid - {pd.to_datetime(time_val).strftime('%Y-%m-%d')}\")\n",
    "        ax.legend()\n",
    "    \n",
    "    # Hide unused subplots\n",
    "    for j in range(n_times, len(axes)):\n",
    "        axes[j].axis('off')\n",
    "    \n",
    "    plt.suptitle(\"Geocoded Time Series (Regular Grid) - Each pixel represents the same ground area\", fontsize=14)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"No regular grid data available for visualization\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65",
   "metadata": {},
   "source": [
    "#### Irregular Grid Time Series (For Comparison)\n",
    "\n",
    "Below is the original approach for comparison. Note that pixels may not represent exactly the same ground locations across different dates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot irregular grid (coordinate aligned) time series for comparison\n",
    "if intensity_data_cube is not None:\n",
    "    n_times = len(intensity_data_cube.time)\n",
    "    cols = 4\n",
    "    rows = int(np.ceil(n_times / cols))\n",
    "    \n",
    "    fig, axes = plt.subplots(rows, cols, figsize=(4*cols, 4*rows))\n",
    "    if rows == 1 and cols == 1:\n",
    "        axes = [axes]\n",
    "    else:\n",
    "        axes = axes.flatten()\n",
    "    \n",
    "    for i in range(n_times):\n",
    "        ax = axes[i]\n",
    "        intensity_data_cube[i].plot(\n",
    "            x=\"longitude\", y=\"latitude\",\n",
    "            vmin=-25, vmax=5,\n",
    "            ax=ax,\n",
    "            add_colorbar=False\n",
    "        )\n",
    "        ax.scatter(TARGET_LONG, TARGET_LAT, color=\"red\", marker=\"o\", s=20, label=\"Target Point\")\n",
    "        \n",
    "        # Add date to title\n",
    "        time_val = intensity_data_cube.time.values[i]\n",
    "        ax.set_title(f\"Irregular Grid - {pd.to_datetime(time_val).strftime('%Y-%m-%d')}\")\n",
    "        ax.legend()\n",
    "    \n",
    "    # Hide unused subplots\n",
    "    for j in range(n_times, len(axes)):\n",
    "        axes[j].axis('off')\n",
    "    \n",
    "    plt.suptitle(\"‚ö†Ô∏è Coordinate Aligned Time Series (Irregular Grid) - Pixels may not align geographically\", fontsize=14)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"No irregular grid data available for visualization\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67",
   "metadata": {},
   "source": [
    "### Flood Detection Using Water Threshold\n",
    "\n",
    "Water detection in SAR imagery is based on the principle that water surfaces appear very dark due to low backscatter. According to [literature](https://www.researchgate.net/figure/VV-and-VH-threshold-statistics-1-obtained-via-graphical-interpretation-and-2_tbl4_360412209) and [other sources](https://mbonnema.github.io/GoogleEarthEngine/07-SAR-Water-Classification/?utm_source=chatgpt.com), water typically has backscatter values lower than **-15 dB**.\n",
    "\n",
    "We'll demonstrate flood detection using this threshold on both data cubes to show the advantages of the geocoded approach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define water threshold\n",
    "WATER_THRESHOLD_DB = -15\n",
    "\n",
    "# Create flood maps using regular grid (geocoded) data\n",
    "if intensity_datacube_regular is not None:\n",
    "    print(\"Creating flood maps from regular grid (geocoded) data...\")\n",
    "    \n",
    "    n_times = len(intensity_datacube_regular.time)\n",
    "    cols = 4\n",
    "    rows = int(np.ceil(n_times / cols))\n",
    "    \n",
    "    fig, axes = plt.subplots(rows, cols, figsize=(4*cols, 4*rows))\n",
    "    if rows == 1 and cols == 1:\n",
    "        axes = [axes]\n",
    "    else:\n",
    "        axes = axes.flatten()\n",
    "    \n",
    "    for i in range(n_times):\n",
    "        ax = axes[i]\n",
    "        \n",
    "        # Create water mask (True where intensity <= threshold)\n",
    "        water_mask = (intensity_datacube_regular.intensity.isel(time=i) <= WATER_THRESHOLD_DB)\n",
    "        \n",
    "        # Plot the water mask\n",
    "        water_mask.plot(\n",
    "            x=\"x\", y=\"y\",\n",
    "            ax=ax,\n",
    "            add_colorbar=False,\n",
    "            cmap='RdYlBu_r'  # Water in blue, land in red/yellow\n",
    "        )\n",
    "        \n",
    "        ax.scatter(TARGET_LONG, TARGET_LAT, color=\"red\", marker=\"o\", s=50, label=\"Target Point\")\n",
    "        ax.set_xlabel(\"Longitude\")\n",
    "        ax.set_ylabel(\"Latitude\")\n",
    "        \n",
    "        # Add date to title\n",
    "        time_val = intensity_datacube_regular.time.values[i]\n",
    "        ax.set_title(f\"Regular Grid Flood Map - {pd.to_datetime(time_val).strftime('%Y-%m-%d')}\")\n",
    "        ax.legend()\n",
    "    \n",
    "    # Hide unused subplots\n",
    "    for j in range(n_times, len(axes)):\n",
    "        axes[j].axis('off')\n",
    "    \n",
    "    plt.suptitle(f\"Geocoded Flood Maps (Threshold: {WATER_THRESHOLD_DB} dB) - Blue = Water, Yellow/Red = Land\", fontsize=14)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Calculate flood statistics\n",
    "    print(\"\\nFlood extent statistics (Regular Grid):\")\n",
    "    for i in range(n_times):\n",
    "        water_pixels = (intensity_datacube_regular.intensity.isel(time=i) <= WATER_THRESHOLD_DB).sum().values\n",
    "        total_pixels = intensity_datacube_regular.intensity.isel(time=i).count().values\n",
    "        flood_percentage = (water_pixels / total_pixels) * 100\n",
    "        time_val = pd.to_datetime(intensity_datacube_regular.time.values[i])\n",
    "        print(f\"  {time_val.strftime('%Y-%m-%d')}: {flood_percentage:.1f}% water coverage ({water_pixels} of {total_pixels} pixels)\")\n",
    "        \n",
    "else:\n",
    "    print(\"No regular grid data available for flood mapping\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69",
   "metadata": {},
   "source": [
    "#### Irregular Grid Flood Maps (For Comparison)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform change detection using regular grid (geocoded) data\n",
    "if intensity_datacube_regular is not None and len(intensity_datacube_regular.time) >= 2:\n",
    "    print(\"Performing change detection with geocoded data...\")\n",
    "    \n",
    "    # Select before and after images (adjust indices as needed based on your data)\n",
    "    before_idx = 0  # First image (before flood)\n",
    "    after_idx = 1   # Second image (after flood)\n",
    "    \n",
    "    if len(intensity_datacube_regular.time) > 2:\n",
    "        after_idx = len(intensity_datacube_regular.time) // 2  # Middle image\n",
    "    \n",
    "    before_img = intensity_datacube_regular.intensity.isel(time=before_idx)\n",
    "    after_img = intensity_datacube_regular.intensity.isel(time=after_idx)\n",
    "    \n",
    "    # Calculate difference (after - before)\n",
    "    change_map = after_img - before_img\n",
    "    \n",
    "    # Create comparison plot\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(12, 10))\n",
    "    \n",
    "    # Before image\n",
    "    before_img.plot(x=\"x\", y=\"y\", ax=axes[0,0], vmin=-25, vmax=5, cmap='viridis')\n",
    "    axes[0,0].set_title(f\"Before Flood - {pd.to_datetime(intensity_datacube_regular.time.values[before_idx]).strftime('%Y-%m-%d')}\")\n",
    "    axes[0,0].scatter(TARGET_LONG, TARGET_LAT, color=\"red\", marker=\"o\", s=50, label=\"Target\")\n",
    "    axes[0,0].legend()\n",
    "    \n",
    "    # After image\n",
    "    after_img.plot(x=\"x\", y=\"y\", ax=axes[0,1], vmin=-25, vmax=5, cmap='viridis')\n",
    "    axes[0,1].set_title(f\"After Flood - {pd.to_datetime(intensity_datacube_regular.time.values[after_idx]).strftime('%Y-%m-%d')}\")\n",
    "    axes[0,1].scatter(TARGET_LONG, TARGET_LAT, color=\"red\", marker=\"o\", s=50, label=\"Target\")\n",
    "    axes[0,1].legend()\n",
    "    \n",
    "    # Change detection map\n",
    "    change_map.plot(x=\"x\", y=\"y\", ax=axes[1,0], cmap='RdBu_r', vmin=-15, vmax=15)\n",
    "    axes[1,0].set_title(\"Change Detection (After - Before)\")\n",
    "    axes[1,0].scatter(TARGET_LONG, TARGET_LAT, color=\"yellow\", marker=\"o\", s=50, label=\"Target\")\n",
    "    axes[1,0].legend()\n",
    "    \n",
    "    # Flood areas (areas with significant decrease in backscatter)\n",
    "    flood_areas = change_map < -10  # Areas that became much darker (flooded)\n",
    "    flood_areas.plot(x=\"x\", y=\"y\", ax=axes[1,1], cmap='Blues')\n",
    "    axes[1,1].set_title(\"Detected Flood Areas (Change < -10 dB)\")\n",
    "    axes[1,1].scatter(TARGET_LONG, TARGET_LAT, color=\"red\", marker=\"o\", s=50, label=\"Target\")\n",
    "    axes[1,1].legend()\n",
    "    \n",
    "    plt.suptitle(\"Flood Change Detection - Regular Grid (Geocoded) Data\", fontsize=16)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Statistics\n",
    "    total_pixels = change_map.count().values\n",
    "    flooded_pixels = (change_map < -10).sum().values\n",
    "    flood_percentage = (flooded_pixels / total_pixels) * 100\n",
    "    \n",
    "    print(f\"\\nFlood Change Detection Results:\")\n",
    "    print(f\"Total pixels analyzed: {total_pixels}\")\n",
    "    print(f\"Pixels showing flood signature (>10dB decrease): {flooded_pixels}\")\n",
    "    print(f\"Estimated flood coverage: {flood_percentage:.1f}%\")\n",
    "    \n",
    "else:\n",
    "    print(\"Insufficient regular grid data for change detection\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74",
   "metadata": {},
   "source": [
    "#### Change Detection with Irregular Grid Data (For Comparison)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create flood maps using irregular grid (coordinate aligned) data for comparison\n",
    "if intensity_data_cube is not None:\n",
    "    print(\"\\nCreating flood maps from irregular grid (coordinate aligned) data...\")\n",
    "    \n",
    "    n_times = len(intensity_data_cube.time)\n",
    "    cols = 4\n",
    "    rows = int(np.ceil(n_times / cols))\n",
    "    \n",
    "    fig, axes = plt.subplots(rows, cols, figsize=(4*cols, 4*rows))\n",
    "    if rows == 1 and cols == 1:\n",
    "        axes = [axes]\n",
    "    else:\n",
    "        axes = axes.flatten()\n",
    "    \n",
    "    for i in range(n_times):\n",
    "        ax = axes[i]\n",
    "        \n",
    "        # Create water mask\n",
    "        water_mask = (intensity_data_cube[i] <= WATER_THRESHOLD_DB)\n",
    "        \n",
    "        water_mask.plot(\n",
    "            x=\"longitude\", y=\"latitude\",\n",
    "            ax=ax,\n",
    "            add_colorbar=False,\n",
    "            cmap='RdYlBu_r'\n",
    "        )\n",
    "        \n",
    "        ax.scatter(TARGET_LONG, TARGET_LAT, color=\"red\", marker=\"o\", s=20, label=\"Target Point\")\n",
    "        \n",
    "        # Add date to title\n",
    "        time_val = intensity_data_cube.time.values[i]\n",
    "        ax.set_title(f\"Irregular Grid Flood Map - {pd.to_datetime(time_val).strftime('%Y-%m-%d')}\")\n",
    "        ax.legend()\n",
    "    \n",
    "    # Hide unused subplots\n",
    "    for j in range(n_times, len(axes)):\n",
    "        axes[j].axis('off')\n",
    "    \n",
    "    plt.suptitle(f\"‚ö†Ô∏è Coordinate Aligned Flood Maps (Threshold: {WATER_THRESHOLD_DB} dB) - Pixels may not align geographically\", fontsize=14)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"No irregular grid data available for flood mapping\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71",
   "metadata": {},
   "source": [
    "### Flood Change Detection\n",
    "\n",
    "Now we'll demonstrate change detection by comparing images from before and after the flood event. This analysis is much more accurate with geocoded data because we can be confident that we're comparing the exact same geographical locations across time."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72",
   "metadata": {},
   "source": [
    "#### Change Detection with Regular Grid (Geocoded) Data\n",
    "\n",
    "With properly geocoded data, we can perform accurate change detection by comparing the same geographical areas before and after the flood event. This provides reliable flood mapping because each pixel represents the exact same ground location across different acquisition dates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform change detection using irregular grid data for comparison\n",
    "if intensity_data_cube is not None and len(intensity_data_cube.time) >= 2:\n",
    "    print(\"\\nPerforming change detection with irregular grid data...\")\n",
    "    \n",
    "    # Calculate difference between second and third datasets\n",
    "    dif = (intensity_data_cube[1] - intensity_data_cube[2])\n",
    "    \n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
    "    \n",
    "    # Change detection map\n",
    "    dif.plot(x=\"longitude\", y=\"latitude\", vmin=-10, vmax=20, ax=ax1, cmap='RdBu_r')\n",
    "    ax1.set_title(\"‚ö†Ô∏è Irregular Grid Change Detection\")\n",
    "    ax1.scatter(TARGET_LONG, TARGET_LAT, color=\"yellow\", marker=\"o\", s=50, label=\"Target\")\n",
    "    ax1.legend()\n",
    "    \n",
    "    # Flood areas (significant decreases)\n",
    "    flood_areas_irregular = dif < -10\n",
    "    flood_areas_irregular.plot(x=\"longitude\", y=\"latitude\", ax=ax2, cmap='Blues')\n",
    "    ax2.set_title(\"Detected Flood Areas (Change < -10 dB)\")\n",
    "    ax2.scatter(TARGET_LONG, TARGET_LAT, color=\"red\", marker=\"o\", s=50, label=\"Target\")\n",
    "    ax2.legend()\n",
    "    \n",
    "    plt.suptitle(\"‚ö†Ô∏è Change Detection - Irregular Grid (Coordinate Aligned)\", fontsize=14)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"Insufficient irregular grid data for change detection\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76",
   "metadata": {},
   "source": [
    "### Time Series Analysis at Target Location\n",
    "\n",
    "Now we'll demonstrate time series analysis by extracting backscatter values at our target location. This comparison will highlight the advantages of using geocoded data for temporal analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77",
   "metadata": {},
   "source": [
    "#### Time Series from Regular Grid (Geocoded) Data - Recommended Approach\n",
    "\n",
    "With geocoded data, we can extract time series values at precise geographic coordinates. This provides accurate temporal analysis because each measurement corresponds to exactly the same ground location."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81",
   "metadata": {},
   "source": [
    "#### Comparison of Time Series Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot comparison of time series from both approaches\n",
    "fig, axes = plt.subplots(2, 1, figsize=(12, 10))\n",
    "\n",
    "# Regular grid (geocoded) time series\n",
    "if target_point_regular is not None:\n",
    "    ax1 = axes[0]\n",
    "    target_point_regular.plot(ax=ax1, marker='o', label='Geocoded time series', color='blue')\n",
    "    \n",
    "    # Add flood threshold line\n",
    "    x_vals = target_point_regular.time.values\n",
    "    ax1.axhline(y=WATER_THRESHOLD_DB, color='red', linestyle='--', label=f'Flood threshold ({WATER_THRESHOLD_DB} dB)')\n",
    "    \n",
    "    # Add trend line\n",
    "    if len(target_point_regular) > 3:\n",
    "        x_num = np.arange(len(target_point_regular))\n",
    "        y_vals = target_point_regular.values\n",
    "        z = np.polyfit(x_num, y_vals, min(3, len(y_vals)-1))\n",
    "        p = np.poly1d(z)\n",
    "        ax1.plot(x_vals, p(x_num), 'g--', alpha=0.7, label='Trend line')\n",
    "    \n",
    "    ax1.set_title('‚úÖ Regular Grid (Geocoded) Time Series - Reliable Geographic Alignment')\n",
    "    ax1.set_ylabel('Backscatter Intensity (dB)')\n",
    "    ax1.legend()\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "else:\n",
    "    axes[0].text(0.5, 0.5, 'No regular grid data available', \n",
    "                ha='center', va='center', transform=axes[0].transAxes)\n",
    "    axes[0].set_title('Regular Grid Time Series (Not Available)')\n",
    "\n",
    "# Irregular grid time series\n",
    "if target_point_irregular is not None:\n",
    "    ax2 = axes[1]\n",
    "    target_point_irregular.plot(ax=ax2, marker='s', label='Coordinate aligned time series', color='orange')\n",
    "    \n",
    "    # Add flood threshold line\n",
    "    x_vals = target_point_irregular.time.values\n",
    "    ax2.axhline(y=WATER_THRESHOLD_DB, color='red', linestyle='--', label=f'Flood threshold ({WATER_THRESHOLD_DB} dB)')\n",
    "    \n",
    "    # Add trend line\n",
    "    if len(target_point_irregular) > 3:\n",
    "        x_num = np.arange(len(target_point_irregular))\n",
    "        y_vals = target_point_irregular.values\n",
    "        z = np.polyfit(x_num, y_vals, min(3, len(y_vals)-1))\n",
    "        p = np.poly1d(z)\n",
    "        ax2.plot(x_vals, p(x_num), 'g--', alpha=0.7, label='Trend line')\n",
    "    \n",
    "    ax2.set_title('‚ö†Ô∏è Irregular Grid (Coordinate Aligned) Time Series - Potential Geographic Misalignment')\n",
    "    ax2.set_ylabel('Backscatter Intensity (dB)')\n",
    "    ax2.set_xlabel('Time')\n",
    "    ax2.legend()\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "else:\n",
    "    axes[1].text(0.5, 0.5, 'No irregular grid data available', \n",
    "                ha='center', va='center', transform=axes[1].transAxes)\n",
    "    axes[1].set_title('Irregular Grid Time Series (Not Available)')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print summary statistics\n",
    "print(\"\\n=== Time Series Analysis Summary ===\")\n",
    "\n",
    "if target_point_regular is not None:\n",
    "    regular_below_threshold = (target_point_regular < WATER_THRESHOLD_DB).sum().values\n",
    "    regular_total = len(target_point_regular)\n",
    "    print(f\"Regular Grid (Geocoded):\")\n",
    "    print(f\"  - Total time points: {regular_total}\")\n",
    "    print(f\"  - Points below flood threshold: {regular_below_threshold}\")\n",
    "    print(f\"  - Flood detection rate: {(regular_below_threshold/regular_total)*100:.1f}%\")\n",
    "    print(f\"  - Mean backscatter: {target_point_regular.mean().values:.2f} dB\")\n",
    "    print(f\"  - Min backscatter: {target_point_regular.min().values:.2f} dB\")\n",
    "    print(f\"  - Max backscatter: {target_point_regular.max().values:.2f} dB\")\n",
    "\n",
    "if target_point_irregular is not None:\n",
    "    irregular_below_threshold = (target_point_irregular < WATER_THRESHOLD_DB).sum().values\n",
    "    irregular_total = len(target_point_irregular)\n",
    "    print(f\"\\nIrregular Grid (Coordinate Aligned):\")\n",
    "    print(f\"  - Total time points: {irregular_total}\")\n",
    "    print(f\"  - Points below flood threshold: {irregular_below_threshold}\")\n",
    "    print(f\"  - Flood detection rate: {(irregular_below_threshold/irregular_total)*100:.1f}%\")\n",
    "    print(f\"  - Mean backscatter: {target_point_irregular.mean().values:.2f} dB\")\n",
    "    print(f\"  - Min backscatter: {target_point_irregular.min().values:.2f} dB\")\n",
    "    print(f\"  - Max backscatter: {target_point_irregular.max().values:.2f} dB\")\n",
    "\n",
    "print(\"\\n=== Key Advantages of Geocoded Approach ===\")\n",
    "print(\"‚úÖ Each pixel represents exactly the same ground location across all dates\")\n",
    "print(\"‚úÖ Time series analysis is geographically meaningful and accurate\")\n",
    "print(\"‚úÖ Suitable for operational flood monitoring and change detection\")\n",
    "print(\"‚úÖ Can be easily combined with other geocoded datasets\")\n",
    "print(\"‚úÖ Enables quantitative analysis of flood extent and duration\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "This tutorial has demonstrated a significant improvement to Sentinel-1 flood mapping by implementing proper GCP-based slicing and geocoding. The key improvements include:\n",
    "\n",
    "### üéØ **Main Improvements Made:**\n",
    "\n",
    "1. **GCP-Based Spatial Slicing**: Instead of using hardcoded index slicing, we now use Ground Control Points (GCPs) to find the appropriate geographical area, ensuring that all products cover the same ground locations.\n",
    "\n",
    "2. **Proper Geocoding**: Data is geocoded to a regular grid where each pixel represents exactly the same geographical area across all time steps.\n",
    "\n",
    "3. **Aligned Time Series**: The geocoded approach enables accurate time series analysis where temporal changes reflect real changes in surface conditions.\n",
    "\n",
    "### üöÄ **Benefits for Multi-Product Analysis:**\n",
    "\n",
    "- **Geographic Consistency**: Each pixel represents the same area on the ground across all dates\n",
    "- **Accurate Change Detection**: Differences between images reflect real changes, not geometric misalignment\n",
    "- **Reliable Time Series**: Temporal analysis is geographically meaningful\n",
    "- **Operational Ready**: Suitable for automated flood monitoring systems\n",
    "- **Interoperability**: Can be easily combined with other geocoded datasets\n",
    "\n",
    "### üìä **Technical Implementation:**\n",
    "\n",
    "The improved workflow follows these steps:\n",
    "1. Define bounding box around area of interest\n",
    "2. Use GCPs to find appropriate slicing indices\n",
    "3. Crop and decimate data based on geographic bounds\n",
    "4. Interpolate GCPs to match processed data\n",
    "5. Geocode all products to a common regular grid\n",
    "6. Create aligned time series for accurate analysis\n",
    "\n",
    "This approach transforms the notebook from a single-product demonstration to a robust multi-product flood monitoring system suitable for operational use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract time series from regular grid (geocoded) data\n",
    "if intensity_datacube_regular is not None:\n",
    "    print(\"Extracting time series from geocoded data...\")\n",
    "    \n",
    "    # Find nearest pixel to target coordinates using geocoded coordinates\n",
    "    # Use the x,y coordinates (longitude, latitude) from the regular grid\n",
    "    x_coords = intensity_datacube_regular.x.values\n",
    "    y_coords = intensity_datacube_regular.y.values\n",
    "    \n",
    "    # Find closest x and y indices\n",
    "    x_idx = np.argmin(np.abs(x_coords - TARGET_LONG))\n",
    "    y_idx = np.argmin(np.abs(y_coords - TARGET_LAT))\n",
    "    \n",
    "    # Extract time series at the target location\n",
    "    target_point_regular = intensity_datacube_regular.intensity.isel(x=x_idx, y=y_idx)\n",
    "    \n",
    "    print(f\"Extracting data at coordinates: Lon={x_coords[x_idx]:.4f}, Lat={y_coords[y_idx]:.4f}\")\n",
    "    print(f\"Target coordinates: Lon={TARGET_LONG:.4f}, Lat={TARGET_LAT:.4f}\")\n",
    "    print(f\"Distance from target: {np.abs(x_coords[x_idx] - TARGET_LONG):.4f}¬∞ lon, {np.abs(y_coords[y_idx] - TARGET_LAT):.4f}¬∞ lat\")\n",
    "    \n",
    "else:\n",
    "    target_point_regular = None\n",
    "    print(\"No regular grid data available for time series extraction\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79",
   "metadata": {},
   "source": [
    "#### Time Series from Irregular Grid Data (For Comparison)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract time series from irregular grid data for comparison\n",
    "if intensity_data_cube is not None:\n",
    "    print(\"\\nExtracting time series from irregular grid data...\")\n",
    "    \n",
    "    # Find how far each pixel's latitude and longitude is from the target point\n",
    "    abs_error = np.abs(intensity_data_cube.latitude - TARGET_LAT) + np.abs(intensity_data_cube.longitude - TARGET_LONG)  \n",
    "    \n",
    "    # Get the indexes of the closest point\n",
    "    i, j = np.unravel_index(np.argmin(abs_error.values), abs_error.shape)\n",
    "    \n",
    "    # Slice the data cube to get only the pixel that corresponds to the target point\n",
    "    target_point_irregular = intensity_data_cube.isel(ground_range=j, azimuth_time=i)\n",
    "    \n",
    "    print(f\"Closest pixel found at azimuth_time index {i}, ground_range index {j}\")\n",
    "else:\n",
    "    target_point_irregular = None\n",
    "    print(\"No irregular grid data available for time series extraction\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76",
   "metadata": {},
   "source": [
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77",
   "metadata": {},
   "source": [
    "## Challenges"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78",
   "metadata": {},
   "source": [
    "While using the optimised `.zarr` format saves a lot of time and makes creating workflows relatively simple and achievable, there are still a few challenges to handle and to keep in mind:\n",
    "\n",
    "- Sentinel-1 GRD Data Availability: For **Sentinel-1 GRD**, most of the datasets are not yet available on the STAC catalogue. This makes searching and data handling harder because, in the end, only a few products are correctly converted.\n",
    "\n",
    "- Backscatter Computation Libraries: There are only a few working Python libraries that handle backscatter computation. When considering the `.zarr` format, the list becomes even smaller. `xarray_sentinel` is a very good library that handles intensity backscatter computation with `.zarr`.\n",
    "\n",
    "- Terrain Correction: With the available libraries, it is very difficult to perform geometric and radiometric terrain correction. The existing tools that support the .`zarr` format are not yet fully operational and do not accept the format as it is.\n",
    "\n",
    "- Image Coregistration: As discussed previously, the .`zarr` format is perfect for handling multiple datasets simultaneously and, thus, for time series analysis. The problem is that there is no library or package that performs proper **coregistration** of Sentinel images, especially with the `.zarr` format. In this tutorial, we used a simplified coordinate assignment approach rather than true coregistration, which works for our specific use case but has limitations. Proper coregistration remains a significant challenge because it is an important step for most production SAR workflows."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79",
   "metadata": {},
   "source": [
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80",
   "metadata": {},
   "source": [
    "## Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81",
   "metadata": {},
   "source": [
    "The `.zarr` format is particularly well suited for hazard analysis because it enables multiple datasets to be combined into a single structure, either as a data cube or as a list of datatrees. This makes it ideal for rapid, multi-temporal, and multi-spatial monitoring. Unlike the `.SAFE` format, which required downloading entire products, `.zarr` only loads the specific groups needed, while the rest is accessed on the fly. As a result, both data handling and subsequent operations are much faster and more efficient.\n",
    "\n",
    "Although the ecosystem for `.zarr` is still evolving, there are already promising developments. In the past, `.SAFE` products could be fully processed on applications like SNAP, but similar completeness has not yet been reached for `.zarr`. Nevertheless, libraries such as `xarray_sentinel` and are beginning to cover essential SAR operations. This potential is illustrated in the Valencia flood case study, where Sentinel-1 backscatter sensitivity to water enabled clear mapping of flood extent and duration. The same workflow can be adapted to other flood events by adjusting the relevant thresholds and parameters to match local conditions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82",
   "metadata": {},
   "source": [
    "## What's next?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83",
   "metadata": {},
   "source": [
    "This online resource is under active development. So stay tuned for regular updates üõ∞Ô∏è."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "eopf-101",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
