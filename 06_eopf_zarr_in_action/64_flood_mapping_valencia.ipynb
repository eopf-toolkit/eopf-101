{
 "cells": [
  {
   "cell_type": "raw",
   "id": "0",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "---\n",
    "title: \"Flood Mapping - Time Series Analysis in Valencia\" \n",
    "execute:\n",
    "  enabled: true\n",
    "format: html\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "<a href=\"https://jupyterhub.user.eopf.eodc.eu/hub/user-redirect/git-pull?repo=https://github.com/eopf-toolkit/eopf-101&branch=main&urlpath=lab/tree/eopf-101/64_flood_mapping_valencia.ipynb\" target=\"_blank\">\n",
    "  <button style=\"background-color:#0072ce; color:white; padding:0.6em 1.2em; font-size:1rem; border:none; border-radius:6px; margin-top:1em;\">\n",
    "    üöÄ Launch this notebook in JupyterLab\n",
    "  </button>\n",
    "</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2",
   "metadata": {},
   "source": [
    "### Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "**Sentinel-1 GRD** data is particularly valuable to detect water and underwater areas. Synthetic Aperture Radar (SAR) can capture images day and night, in any weather, a feature especially important for flooding events, where cloudy and rainy weather can persist for weeks. This makes it far more reliable than optical sensors during storms.\n",
    "\n",
    "With its frequent revisits, wide coverage, and free high-resolution data, **Sentinel-1** enables the rapid mapping of flood extents, as will be demonstrated in this workflow. **VV** polarization is preferred for flood mapping due to its sensitivity to water surfaces, which typically appear darker in the images compared to land surfaces."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4",
   "metadata": {},
   "source": [
    "#### The Flooding Event"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5",
   "metadata": {},
   "source": [
    "On October 29, 2024, the city of Valencia (Spain) was hit by catastrophic flooding caused by intense storms, leaving over 230 deaths and billions in damages. This disaster was part of Europe‚Äôs worst flood year in over a decade, with hundreds of thousands affected continent-wide. Such events highlight the urgent need for reliable flood monitoring to support **emergency response**, damage assessment and long-term resilience planning.\n",
    "\n",
    "With respect to this event, we will demonstrate how to use **Sentinel-1 GRD** data to map flood extents. We will use 14 **Sentinel-1 GRD** images from the **IW** swath, covering the city and metropolitan area of Valencia from October 7, 2024 to March 24, 2025. This includes 2 images captured before, 1 immediately after the heavy rains, and 11 images taken after the flooding event, until the water levels got back to normal:\n",
    "- October 7, 2424 (before)\n",
    "- October 19, 2024 (before)\n",
    "- October 31, 2024 (right after the event)\n",
    "- November 12, 2024 (after)\n",
    "- November 24, 2024 (after)\n",
    "- December 6, 2024 (after)\n",
    "- December 18, 2024 (after)\n",
    "- December 30, 2024 (after)\n",
    "- January 11, 2025 (after)\n",
    "- January 23, 2025 (after)\n",
    "- February 4, 2025 (after)\n",
    "- February 16, 2025 (after)\n",
    "- March 12, 2025 (after)\n",
    "- March 24, 2025 (after)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6",
   "metadata": {},
   "source": [
    "#### What we will learn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7",
   "metadata": {},
   "source": [
    "- üåä How to create a workflow to map flood events.\n",
    "- ‚öíÔ∏è Basic SAR processing tools.\n",
    "- üìä How to create a data cube to perform time series analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8",
   "metadata": {},
   "source": [
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9",
   "metadata": {},
   "source": [
    "#### Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xarray as xr \n",
    "import xarray_sentinel \n",
    "import pandas as pd\n",
    "import matplotlib as plt\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "import dask                             # these last two libraries are imported to open the datasets faster\n",
    "from dask.distributed import Client     # and in the end take advantage of the optimized .zarr format\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11",
   "metadata": {},
   "source": [
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## Data pre-processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13",
   "metadata": {},
   "source": [
    "To search and load the data needed for the analysis, we will follow the processes we presented in [Sentinel-1 GRD structure tutorial](/02_about_eopf_zarr/22_zarr_structure_S1GRD.ipynb) and [S1 basic operations tutorial](/02_about_eopf_zarr/23_S1_basic_operations.ipynb).\n",
    "\n",
    "Once we defined our interest Sentinel-1 GRD items, we can see that they contain both **VH** and **VV** polarizations.<br>\n",
    "For this flood mapping context, **VV** polarization is the choice of interest, as water backscatter is much more visible with it, rather than with VH."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14",
   "metadata": {},
   "source": [
    "### Loading the datatree"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15",
   "metadata": {},
   "source": [
    "The list below shows the names of the products we will use for the flood mapping and time series analysis.<br>\n",
    "As we have seen in previous chapters, these names already contain valuable information that can be used to search for specific products within the [EOPF STAC catalogue]()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "scenes = [\"S1A_IW_GRDH_1SDV_20241007T180256_20241007T180321_056000_06D943_D46B\", \n",
    "          \"S1A_IW_GRDH_1SDV_20241019T180256_20241019T180321_056175_06E02E_2D52\", \n",
    "          \"S1A_IW_GRDH_1SDV_20241031T180256_20241031T180321_056350_06E71E_479F\", \n",
    "          \"S1A_IW_GRDH_1SDV_20241112T180255_20241112T180320_056525_06EE16_DC29\", \n",
    "          \"S1A_IW_GRDH_1SDV_20241124T180254_20241124T180319_056700_06F516_BA27\", \n",
    "          \"S1A_IW_GRDH_1SDV_20241206T180253_20241206T180318_056875_06FBFD_25AD\", \n",
    "          \"S1A_IW_GRDH_1SDV_20241218T180252_20241218T180317_057050_0702F2_0BC2\", \n",
    "          \"S1A_IW_GRDH_1SDV_20241230T180251_20241230T180316_057225_0709DD_15AC\", \n",
    "          \"S1A_IW_GRDH_1SDV_20250111T180250_20250111T180315_057400_0710C7_ADBB\", \n",
    "          \"S1A_IW_GRDH_1SDV_20250123T180249_20250123T180314_057575_0717B9_A784\", \n",
    "          \"S1A_IW_GRDH_1SDV_20250204T180249_20250204T180314_057750_071EA2_4373\", \n",
    "          \"S1A_IW_GRDH_1SDV_20250216T180248_20250216T180313_057925_0725AE_8AC7\", \n",
    "          \"S1A_IW_GRDH_1SDV_20250312T180248_20250312T180313_058275_0733E6_4F5B\", \n",
    "          \"S1A_IW_GRDH_1SDV_20250324T180248_20250324T180313_058450_073AD0_04B7\", \n",
    "          ]\n",
    "\n",
    "zarr_paths = []\n",
    "for scene in scenes:\n",
    "    zarr_paths.append(f\"https://objects.eodc.eu/e05ab01a9d56408d82ac32d69a5aae2a:notebook-data/tutorial_data/cpm_v260/{scene}.zarr\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17",
   "metadata": {},
   "source": [
    "Next, we will load all `zarr` datasets as xarray.Datatrees. Here **we are not reading** the entire dataset from the store; but, creating a set of references to the data, which enables us to access it efficiently later in the analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = Client()  # Set up local cluster on your laptop\n",
    "client\n",
    "\n",
    "@dask.delayed\n",
    "def load_datatree_delayed(path):\n",
    "    return xr.open_datatree(path, consolidated=True, chunks=\"auto\")\n",
    "\n",
    "# Create delayed objects\n",
    "delayed_datatrees = [load_datatree_delayed(path) for path in zarr_paths]\n",
    "# Compute in parallel\n",
    "datatrees = dask.compute(*delayed_datatrees)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19",
   "metadata": {},
   "source": [
    "Each element inside the `datatree` list is a datatree and corresponds to a Sentinel-1 GRD scene datatree present on the list above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Each element inside the datatree list is a datatree and corresponds to a Sentinel-1 GRD scene datatree present on the list above\n",
    "type(datatrees[0]) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21",
   "metadata": {},
   "source": [
    "### Defining variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of scenes we are working with for the time series analysis\n",
    "DATASET_NUMBER = len(datatrees) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23",
   "metadata": {},
   "source": [
    "If we run the following commented out code line we will be able to see how each datatree is organized within its groups and subgroups (as explained in this [section](./22_zarr_structure_S1GRD.ipynb)). From this datatree, we took the groups and subgroups constant `ID` numbers used to open specific grouos and variables such as:\n",
    "- Measurements group = 7 so, in order to open this group, on the first element of our list of scenes, over the first polarization `VV`, we do `datatrees[0][datatrees[0].groups[7]]`\n",
    "- Calibration group = 33 so, in order to open this group, on the first element of our list of scenes, over the first polarization `VV`, we do `datatrees[0][datatrees[0].groups[33]]`\n",
    "\n",
    "Over the course of this notebook these `IDs` will be used to call variables and compute some other functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Opening the measurements group from the datatree\n",
    "datatrees[0][datatrees[0].groups[7]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some other important constant ID numbers \n",
    "MEASUREMENTS_GROUP_ID = 7\n",
    "GCP_GROUP_ID = 28\n",
    "CALIBRATION_GROUP_ID = 33"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26",
   "metadata": {},
   "source": [
    "We now define the thresholds that will be used for the flood mapping analysis. These values are not fixed and they can be calibrated and adjusted to achieve a better fit for different regions or flood events.<br>\n",
    "\n",
    "In SAR imagery, open water surfaces typically appear very dark because they reflect the radar signal away from the sensor. This results in low backscatter values. In our case, pixels with a backscatter lower than approximately ‚Äì15 dB are likely to correspond to water."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27",
   "metadata": {},
   "outputs": [],
   "source": [
    "WATER_THRESHOLD_DB = -15"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28",
   "metadata": {},
   "source": [
    "It is interesting to study the flood event over a specific point within the area of interest.<br>\n",
    "Therefore, we are storing the coordinates of an anchor point inside the area which is not usually covered by water. After the heavy rain, it became flooded for a few weeks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29",
   "metadata": {},
   "outputs": [],
   "source": [
    "TARGET_LAT = 39.28\n",
    "TARGET_LONG = -0.30"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30",
   "metadata": {},
   "source": [
    "## Extracting information from the `.zarr`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31",
   "metadata": {},
   "source": [
    "As explained in the [S1 basic operations tutorial](23_S1_basic_operations.ipynb), we will perform over all the selected data the following operations:\n",
    "\n",
    "- Slicing the data to meet our area of interest and decimate it\n",
    "- Assigning latitude and longitude coordinates to the dataset\n",
    "- Computing the backscatter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32",
   "metadata": {},
   "source": [
    "### Slicing and decimating GRD variable"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33",
   "metadata": {},
   "source": [
    "To begin with, we access all our `.zarr` items `measurements` groups by creating a list storing all of them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34",
   "metadata": {},
   "outputs": [],
   "source": [
    "measurements = []\n",
    "# Looping to populate the measurements list with only the measurements groups of each dataset on the datatree list\n",
    "for i in range(DATASET_NUMBER):\n",
    "    measurements.append(datatrees[i][datatrees[i].groups[MEASUREMENTS_GROUP_ID]].to_dataset())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35",
   "metadata": {},
   "source": [
    "We continue by decimating `grd`'s data. As we are only interested into a specific area (Valencia).\n",
    "\n",
    "Because we haven't assigned latitude and longitude coordinates yet, we can not crop the data acording to coordinates. On top of this, even though different products have the same shape and dimensions (`azimuth_time` and `ground_range`), the values for the same indexes number don't match so either ways, it wouldn't be doable to slice the data acording to coordinate values (which would always be different for different products)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting the first decimated GRD product from our list, corresponding to the whole scene\n",
    "measurements[0].grd.isel(\n",
    "        azimuth_time=slice(None, None, 20),\n",
    "        ground_range=slice(None, None, 20)).plot(vmax=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Azimuth time has\", measurements[0].grd.shape[0], \"values.\")\n",
    "print(\"Ground range has\", measurements[0].grd.shape[1], \"values.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38",
   "metadata": {},
   "source": [
    "\n",
    "The solution found is to slice the data by the coordinates index positon using `isel()` function. Just before, a rought plotting was done in order to visualize the whole image. it was also plotted the `azimuth_time` and `ground_range` shape. Considering the are we are interested, and after a few tests and direct proportion calculations, we see that we need:\n",
    "- for `azimuth_time`, more or less from the last 10000 positions to the last 4500;\n",
    "- for `ground_range`, more or less from the last 7000 positions to the last 2000;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39",
   "metadata": {},
   "outputs": [],
   "source": [
    "grd = []\n",
    "# Looping to populate the grd list with only the grd subgroups of each dataset on the \n",
    "# datatree list while simultaneaously slicing the data to match our AOI and decimating it\n",
    "for i in range(DATASET_NUMBER):\n",
    "    grd_group = measurements[i].grd\n",
    "    azimuth_time_len = grd_group.sizes['azimuth_time']\n",
    "    ground_range_len = grd_group.sizes['ground_range']\n",
    "    \n",
    "    grd.append(grd_group.isel(\n",
    "        azimuth_time=slice((azimuth_time_len - 10000),    # beginning of the slice\n",
    "                           (azimuth_time_len - 4500),     # end of the slice\n",
    "                           10),                           # interval of the slice/decimation\n",
    "        ground_range=slice((ground_range_len - 7000), \n",
    "                           (ground_range_len - 2000), \n",
    "                           10)\n",
    "    ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40",
   "metadata": {},
   "outputs": [],
   "source": [
    "grd[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting the second sliced and decimated GRD product from our list \n",
    "grd[1].plot(vmax=300)\n",
    "plt.title(\"Sliced GRD product for the area of interest\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42",
   "metadata": {},
   "source": [
    "### Assigning latitude and longitude coordinates"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43",
   "metadata": {},
   "source": [
    "We will execute the following step to assign latitude and longitude coordinates to our datasets:\n",
    "1. Creating a `gcp` dataset interpolated with the `grd` dataset;\n",
    "2. Assigning the latitude and longitude coordinates to the `grd` dataset;\n",
    "\n",
    "These steps are very important because we are computing a georeferenced image, which allows direct comparison with other spatial datasets. Until now, the image coordinates were expressed in `azimuth_time` and `ground_range`, which makes sense in a SAR context but not for geographical analyses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44",
   "metadata": {},
   "outputs": [],
   "source": [
    "gcp = []\n",
    "\n",
    "# Looping to populate the gcp list with only the gcp subgroups of each dataset on the datatree list\n",
    "for i in range(DATASET_NUMBER):\n",
    "    gcp.append(datatrees[i][datatrees[i].groups[GCP_GROUP_ID]].to_dataset())\n",
    "    gcp[i] = gcp[i].interp_like(grd[i]) # interpolate gcp to match the decimation done before"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Looping to assign the latitude and longitude coordinates to grd \n",
    "for i in range(DATASET_NUMBER):\n",
    "    grd[i] = grd[i].assign_coords({\"latitude\": gcp[i].latitude, \n",
    "                                   \"longitude\": gcp[i].longitude})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting the third sliced and decimated GRD product from our list with latitude and longitude coordinates\n",
    "grd[2].plot(x=\"longitude\", y=\"latitude\", vmax=300)\n",
    "plt.title(\"GRD product with latitude and longitude coordinates\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47",
   "metadata": {},
   "source": [
    "### Computing backscatter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48",
   "metadata": {},
   "source": [
    "Again, the following steps are just recreating what was done before, but this time over more datasets. For further detailed information, take a look at this [chapter](./23_S1_basic_operations.ipynb).\n",
    "\n",
    "Firstly we access the variables concerning the `calibration` values. These are the values that are going to be used for the backscatter computation. Because we¬¥ve decimated the `grd` dataset, we also need to decimate the `calibration` variables in the same way.\n",
    "\n",
    "After it, using the `xarray_sentinel` library, we compute the backscatter for each dataset. As input varialbes, we use the `grd` dataset and the `calibration` variables we¬¥ve just accessed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49",
   "metadata": {},
   "outputs": [],
   "source": [
    "calibration = []\n",
    "# Looping to populate the calibration list with only the calibration groups of each dataset on the datatree list\n",
    "for i in range(DATASET_NUMBER):\n",
    "    calibration.append(datatrees[i][datatrees[i].groups[CALIBRATION_GROUP_ID]].to_dataset())\n",
    "    calibration[i] = calibration[i].interp_like(grd[i]) # interpolate calibration to match the decimation done before"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50",
   "metadata": {},
   "outputs": [],
   "source": [
    "intensity = []\n",
    "# Looping to populate the intensity list with the calibrated intensity array originated from xarray_sentinel.calibrate_intensity function\n",
    "for i in range(DATASET_NUMBER):\n",
    "    intensity.append(xarray_sentinel.calibrate_intensity(\n",
    "        grd[i], \n",
    "        calibration[i].beta_nought, \n",
    "        as_db=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting the backscatter intensity for the second dataset on the list\n",
    "intensity[1].plot(x=\"longitude\", y=\"latitude\", vmin=-25, vmax=5)\n",
    "plt.title(\"Computed backscatter intensity\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52",
   "metadata": {},
   "source": [
    "### Create a datacube to prepare for time series analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53",
   "metadata": {},
   "source": [
    "Since we are performing a time series with `.zarr`, instead of analysing individual items stored in a list, we can create a combined dataset, containing all the data, stacked together by a new dimension `time`. Through the stacking, we are building a three-dimensional datacube.\n",
    "\n",
    "To get values for the new dimension `time`, we need to extract the acquisiton dates for each product."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []\n",
    "# Looping to populate the data list with all the acquisition dates from the datatree\n",
    "for i in range(DATASET_NUMBER):\n",
    "    data.append(intensity[i].azimuth_time.values[1].astype('datetime64[D]'))\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55",
   "metadata": {},
   "source": [
    "### Coregistration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56",
   "metadata": {},
   "source": [
    "The next step is sensitive. In order to stack data into an array, the dimension values need to match perfectly, which is not the case (Sentinel-1 GRD data dimension values differ from one product to the other).<br>\n",
    "\n",
    "This problem resembles the **coregistrations** problem. This refers to the process of aligning two or more images, in a way that each pixel in one image corresponds to the exact same ground location in the others. To have a deeper overview of this process you can take a look [here](https://elisecolin.medium.com/what-is-coregistration-in-remote-sensing-7d76d48d337d).\n",
    "\n",
    "There are only a few software programmes and packages that can perform coregistration, and most of the time, these processes are very time-consuming and resource-intensive.\n",
    "\n",
    "Since the GRD images we are working with already have the same dimensions (cropped during the initial steps of the tutorial), we can perform our own coregistration. Such process involves:\n",
    "- Keeping the coordinates of one `intensity` dataset as the reference\n",
    "- Resetting the coordinates of the other ones\n",
    "- Reassigning the coordinates to match the reference dataset and that they can all overlay perfectly.\n",
    "\n",
    "To do so we will use the function `reset_coords()` to reset the original coordinates and then use the `assign_coords` function after."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57",
   "metadata": {},
   "outputs": [],
   "source": [
    "reference_coords = intensity[0].coords # setting the first coordinate values as reference\n",
    "\n",
    "datasets_aligned = []\n",
    "\n",
    "# Looping to populate the datasets_aligned list with the newly assigned coordinate values\n",
    "for ds in intensity:\n",
    "    ds_no_coords = ds.reset_coords(drop=True)\n",
    "    datasets_aligned.append(ds_no_coords.assign_coords(reference_coords))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58",
   "metadata": {},
   "source": [
    "Now, the data is fully prepared to be stacked into a new array that contains all the datasets. They share the same coordinate values and everything is aligned along a third dimension, `time`. To stack all the datasets into only one we will use the `concat()` function from the `xarray` library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the data cube, stacking all the datasets over a new time dimension\n",
    "intensity_data_cube = xr.concat(datasets_aligned, dim=xr.DataArray(data, dims=\"time\"))\n",
    "\n",
    "# There is a new dimension coordinate (time) \n",
    "intensity_data_cube"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## Flood mapping and time series analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61",
   "metadata": {},
   "source": [
    "The last step is to perform the time series and flood mapping analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62",
   "metadata": {},
   "source": [
    "### Simple visualisation of all datasets selected"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63",
   "metadata": {},
   "source": [
    "First, we can plot all the datasets simply to create a visualisation of the flood. In addition to these plots, we are also plotting a chosen latitude and longitude point (as defined at beginning of this tutorial). The coordinate serves as a measure of comparison between all the datasets and from within different analysis methods.\n",
    "\n",
    "When we look over all the items plotted, we can clearly see that the significant flood event happened between the 19th and the 31st of October (it occurred on the 29th of October 2024).\n",
    "\n",
    "Additionally, we can see that the backscatter displaying the water presence was only going back to normal ranges around mid-February 2025."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64",
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = 4    # setting up column number\n",
    "rows = int(np.ceil(DATASET_NUMBER / cols))  # setting up row number according to clumn number\n",
    "fig, axes = plt.subplots(rows, cols, figsize=(4*cols, 4*rows))  \n",
    "axes = axes.flatten()  \n",
    "\n",
    "for i in range(DATASET_NUMBER):\n",
    "    ax = axes[i]\n",
    "    intensity_data_cube[i].plot(    # plotting all the datasets stored in the data cube\n",
    "        x=\"longitude\", y=\"latitude\",\n",
    "        vmin=-25, vmax=5,\n",
    "        ax=ax,  \n",
    "        add_colorbar=False  \n",
    "    )\n",
    "    ax.scatter(TARGET_LONG, TARGET_LAT, color=\"red\", marker=\"o\", s=10, label=\"Selected Point\")  # also plotting the known point defined before\n",
    "    ax.legend()\n",
    "\n",
    "for j in range(i+1, len(axes)):\n",
    "    axes[j].axis('off')     # to avoid having empty cells\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65",
   "metadata": {},
   "source": [
    "### Create a flood map based on threshold values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66",
   "metadata": {},
   "source": [
    "It is known through [literature](https://www.researchgate.net/figure/VV-and-VH-threshold-statistics-1-obtained-via-graphical-interpretation-and-2_tbl4_360412209) and other [sources](https://mbonnema.github.io/GoogleEarthEngine/07-SAR-Water-Classification/?utm_source=chatgpt.com) that water appears as darker pixels, typically with values lower than **-15 dB**. This is a very good method for identifying water because separating the pixels within this threshold value will give us almost a `True` and `False` map for pixels which are greater or smaller than the defined threshold.\n",
    "\n",
    "In the plots below, we classify the pixels with backscatter values equal to or lower than -15 in yellow. Conversely, in purple, we see the pixels that have backscatter values greater than -15.\n",
    "\n",
    "This type of visualisation allows us to easily identify flooded and non-flooded areas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(rows, cols, figsize=(4*cols, 4*rows))  \n",
    "axes = axes.flatten()  \n",
    "\n",
    "for i in range(DATASET_NUMBER):\n",
    "    ax = axes[i]\n",
    "    water_mask = (intensity_data_cube[i] <= WATER_THRESHOLD_DB)     # defining the water mask from the threshold\n",
    "    water_mask.plot(        # plotting all the water masks \n",
    "        x=\"longitude\", y=\"latitude\",\n",
    "        ax=ax,  \n",
    "        add_colorbar=False  \n",
    "    )\n",
    "    ax.scatter(TARGET_LONG, TARGET_LAT, color=\"red\", marker=\"o\", s=10, label=\"Selected Point\") # again plotting the known point defined before\n",
    "    ax.legend()\n",
    "\n",
    "for j in range(i+1, len(axes)):\n",
    "    axes[j].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68",
   "metadata": {},
   "source": [
    "### Create a map showing differences between two images"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69",
   "metadata": {},
   "source": [
    "Knowing the exact flood date, which we have, and from the images plotted previously, we can easily see that the second image is the one right before the flood event and that the third image is the one directly after it. These two images show significant differences in the flooded areas and backscatter values, ranging from **-5 dB** (in the image before the event) to **-20 dB** (in the image directly after the event).\n",
    "\n",
    "For this reason, when we compute the difference between the two images, we will mostly get:\n",
    "- Values around 0 dB for areas that did not change\n",
    "- Values ranging from -15 dB to -20 dB in the precise flooded areas.\n",
    "\n",
    "This is an excellent way to determine precisely which areas were flooded. As we are comparing an image from before the event with another one taken at the highest possible flooding point, the differences between them will be extreme."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70",
   "metadata": {},
   "outputs": [],
   "source": [
    "dif = (intensity_data_cube[1]-intensity_data_cube[2])   # computing the difference between third and second dataset\n",
    "dif.plot(x=\"longitude\", y=\"latitude\", vmin=-10, vmax=20)\n",
    "plt.title(\"Flooded area right after the heavy rains\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71",
   "metadata": {},
   "source": [
    "### Create a time-series plot of one location within the flood"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72",
   "metadata": {},
   "source": [
    "Taking advantage of the data cube we have created over a new `time` dimension, it is much easier to plot the data over this new dimension, as in a time series plot.<br>\n",
    "As our data now shares same dimensions and shape, we can choose to plot a backscatter analysis over the specific latitude and longitude point we defined earlier.<br>\n",
    "\n",
    "As these coordinates might not be exactly the ones shown on the dimension values, we need to perform some operations to find the closest values to the desired coordinates.<br> \n",
    "We will now change the latitude and longitude coordinate values and see how the corresponding `azimuth_time` and `ground_range` values and indexes change."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find how far each pixel's latitude and longitude is from the target point\n",
    "abs_error = np.abs(intensity_data_cube.latitude - TARGET_LAT) + np.abs(intensity_data_cube.longitude - TARGET_LONG)  \n",
    "\n",
    "# Get the indexes of the closest point\n",
    "i, j = np.unravel_index(np.argmin(abs_error.values), abs_error.shape)\n",
    "azimuth_time_index = i\n",
    "ground_range_index = j\n",
    "\n",
    "# Get the coordinate values of the closest point\n",
    "azimuth_time_value = intensity_data_cube.azimuth_time[i].values\n",
    "ground_range_value = intensity_data_cube.ground_range[j].values\n",
    "\n",
    "print(\"Nearest azimuth_time:\", azimuth_time_value, \", with index:\", azimuth_time_index)\n",
    "print(\"Nearest ground_range:\", ground_range_value, \", with index:\", ground_range_index)\n",
    "\n",
    "# Slice the data cube in order to get only the pixel that corresponds to the target point\n",
    "target_point = intensity_data_cube.isel(ground_range=ground_range_index, \n",
    "                                        azimuth_time=azimuth_time_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74",
   "metadata": {},
   "source": [
    "Now we can plot the data cube, showing the backscatter intensity over the target point we defined earlier. Since the datasets are stacked along the time dimension, it becomes much easier to plot the evolution of water backscatter at a specific location. This provides an effective way to monitor the flooding status at that point. \n",
    "\n",
    "We can also add a line representing the water threshold we defined. Any point with a backscatter value below this threshold will be classified as water, thus flooded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the sliced data cube\n",
    "target_point.plot(label='Time series backscatter') \n",
    "\n",
    "x = target_point[target_point.dims[0]].values   # getting the x axis values (time)\n",
    "y = target_point.values                         # getting the y axis values (backscatter intensity)\n",
    "\n",
    "# Creating the trend line\n",
    "x_num = np.arange(len(x))   \n",
    "z = np.polyfit(x_num, y, 6)\n",
    "p = np.poly1d(z)\n",
    "\n",
    "plt.plot(x, p(x_num), 'r--', label='Trend line')\n",
    "plt.plot(x, [-15] * len(x), 'g--', label='Flood threshold')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76",
   "metadata": {},
   "source": [
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77",
   "metadata": {},
   "source": [
    "## Challenges"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78",
   "metadata": {},
   "source": [
    "While using the optimised `.zarr` format saves a lot of time and makes creating workflows relatively simple and achievable, there are still a few challenges to handle and to keep in mind:\n",
    "\n",
    "- Sentinel-1 GRD Data Availability: For **Sentinel-1 GRD**, most of the datasets are not yet available on the STAC catalogue. This makes searching and data handling harder because, in the end, only a few products are correctly converted.\n",
    "\n",
    "- Backscatter Computation Libraries: There are only a few working Python libraries that handle backscatter computation. When considering the `.zarr` format, the list becomes even smaller. `xarray_sentinel` is a very good library that handles intensity backscatter computation with `.zarr`.\n",
    "\n",
    "- Terrain Correction: With the available libraries, it is very difficult to perform geometric and radiometric terrain correction. The existing tools that support the .`zarr` format are not yet fully operational and do not accept the format as it is.\n",
    "\n",
    "- Image Coregistration: As discussed previously, the .`zarr` format is perfect for handling multiple datasets simultaneously and, thus, for time series analysis. The problem is that there is no library or package that performs **coregistration** of Sentinel images, especially with the `.zarr` format. And it remains a significant problem because **coregistering** multiple Sentinel images is an important step for most SAR workflows."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79",
   "metadata": {},
   "source": [
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80",
   "metadata": {},
   "source": [
    "## Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81",
   "metadata": {},
   "source": [
    "The `.zarr` format is particularly well suited for hazard analysis because it enables multiple datasets to be combined into a single structure, either as a data cube or as a list of datatrees. This makes it ideal for rapid, multi-temporal, and multi-spatial monitoring. Unlike the `.SAFE` format, which required downloading entire products, `.zarr` only loads the specific groups needed, while the rest is accessed on the fly. As a result, both data handling and subsequent operations are much faster and more efficient.\n",
    "\n",
    "Although the ecosystem for `.zarr` is still evolving, there are already promising developments. In the past, `.SAFE` products could be fully processed on applications like SNAP, but similar completeness has not yet been reached for `.zarr`. Nevertheless, libraries such as `xarray_sentinel` and are beginning to cover essential SAR operations. This potential is illustrated in the Valencia flood case study, where Sentinel-1 backscatter sensitivity to water enabled clear mapping of flood extent and duration. The same workflow can be adapted to other flood events by adjusting the relevant thresholds and parameters to match local conditions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82",
   "metadata": {},
   "source": [
    "## What's next?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83",
   "metadata": {},
   "source": [
    "This online resource is under active development. So stay tuned for regular updates üõ∞Ô∏è."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "eopf-101",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
