{
 "cells": [
  {
   "cell_type": "raw",
   "id": "0",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "---\n",
    "title: \"Analyzing Forest Vegetation Anomalies Using Sentinel-2 Zarr Data Cubes\" ### Add here the title of the notebook as displayed on the left-side menu\n",
    "execute:\n",
    "  enabled: false\n",
    "format: html\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "<a href=\"https://jupyterhub.user.eopf.eodc.eu/hub/user-redirect/git-pull?repo=https://github.com/eopf-toolkit/eopf-101&branch=main&urlpath=lab/tree/eopf-101/06_eopf_zarr_in_action/65_vegetation_anomalies\" target=\"_blank\">\n",
    "  <button style=\"background-color:#0072ce; color:white; padding:0.6em 1.2em; font-size:1rem; border:none; border-radius:6px; margin-top:1em;\">\n",
    "    ðŸš€ Launch this notebook in JupyterLab\n",
    "  </button>\n",
    "</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "### Introduction\n",
    "\n",
    "Forests are the largest terrestrial carbon sinks, playing a critical role in regulating the global carbon cycle through sustained CO$_2$ uptake. However, extreme climate events (such as heatwaves and droughts) can disrupt forest functioning, reducing photosynthetic activity and, in severe cases, causing tree mortality.\n",
    "\n",
    "While in situ measurements provide valuable information on forest health, collecting such data over large areas is costly, time-consuming, and logistically challenging. Scalable and continuous monitoring therefore requires more efficient approaches.\n",
    "\n",
    "In this case study, we will explore how to use **Sentinel-2 L2A data stored as zarr data cubes** to analyze vegetation anomalies in forest ecosystems in Germany. Specifically, we will focus on two ICOS sites affected by the drought of 2018: [DE-Hai](https://meta.icos-cp.eu/resources/stations/ES_DE-Hai) and [DE-Tha](https://meta.icos-cp.eu/resources/stations/ES_DE-Tha), with DE-Tha left for learners to explore on their own.\n",
    "\n",
    "By leveraging **spatiotemporal data cubes**, we will compute spectral indices and derive anomaly time series to evaluate forest responses to extreme events in CO$_2$ uptake (Gross Primary Production, GPP, from ICOS). This notebook will guide you through a **modular workflow** for:\n",
    "\n",
    "1. Accessing Sentinel-2 zarr data from STAC.  \n",
    "2. Calculating vegetation indices.  \n",
    "3. Detecting anomalies in time series.  \n",
    "4. Visualizing forest responses to environmental extremes.\n",
    "\n",
    "Through this study case, you will see the potential of **zarr-based data cubes** for scalable forest monitoring."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6",
   "metadata": {},
   "source": [
    "### What we will learn\n",
    "\n",
    "In this notebook, you will gain hands-on experience with the following:\n",
    "\n",
    "- ðŸš€ **Creating Sentinel-2 L2A data cubes** from the [EOPF zarr STAC](https://stac.core.eopf.eodc.eu/).  \n",
    "- ðŸ”Ž **Operating on data cubes**, including resampling and interpolation.  \n",
    "- ðŸŒ¿ **Computing spectral indices** using [Awesome Spectral Indices](https://github.com/awesome-spectral-indices/awesome-spectral-indices).  \n",
    "- ðŸ“ˆ **Deriving vegetation anomalies** from time series data.  \n",
    "- ðŸ“Š **Visualizing time series** and comparing results against GPP measurements from [ICOS](https://www.icos-cp.eu/)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8",
   "metadata": {},
   "source": [
    "### Prerequisites\n",
    "\n",
    "To analyze vegetation anomalies, we will work with **vegetation indices**. To leverage the full power of `xarray`, we will use the **`spyndex` Python package**, which provides a Python API for the **Awesome Spectral Indices** catalogue. This gives access to **over 200 spectral indices**, which can be computed directly on various Python data types, including `xarray` datasets and data arrays.  \n",
    "\n",
    "For more details, refer to the [Awesome Spectral Indices paper in *Scientific Data*](https://doi.org/10.1038/s41597-023-02096-0)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9",
   "metadata": {},
   "source": [
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10",
   "metadata": {},
   "source": [
    "#### Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.stats as sps\n",
    "\n",
    "import pystac_client\n",
    "import dask\n",
    "import spyndex\n",
    "import os\n",
    "\n",
    "from pyproj import Transformer\n",
    "from zarr_wf_utils import validate_scl\n",
    "from dask.distributed import Client"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12",
   "metadata": {},
   "source": [
    "#### Helper functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13",
   "metadata": {},
   "source": [
    "##### `get_items`\n",
    "\n",
    "This function **queries the EOPF STAC API** for **Sentinel-2 L2A items** that intersect a specified latitude/longitude point within a given **date range**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f2efcd1-62b2-4bfe-b0ae-25b199e8f6cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_items(lat, lon, start_date=\"2017-01-01\", end_date=\"2024-12-31\", return_as_dicts=False):\n",
    "    \"\"\"\n",
    "    Query the EOPF STAC API for Sentinel-2 L2A items intersecting a given latitude/longitude point\n",
    "    within a specified date range.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    lat : float\n",
    "        Latitude of the point of interest (WGS84).\n",
    "    lon : float\n",
    "        Longitude of the point of interest (WGS84).\n",
    "    start_date : str, default=\"2017-01-01\"\n",
    "        Start of the date range.\n",
    "    end_date : str, default=\"2024-12-31\"\n",
    "        End of the date range.\n",
    "    return_as_dicts : bool, default=False\n",
    "        If True, return items as dictionaries; otherwise return STAC Item objects.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    list\n",
    "        List of STAC Items (or dictionaries) matching the query.\n",
    "    \"\"\"\n",
    "\n",
    "    # Connect to the STAC API\n",
    "    client = pystac_client.Client.open(\"https://stac.core.eopf.eodc.eu/\")\n",
    "    \n",
    "    # Define a GeoJSON Point for spatial filtering\n",
    "    point = {\"type\": \"Point\", \"coordinates\": [lon, lat]}\n",
    "    \n",
    "    # Search for Sentinel-2 L2A items intersecting the point and within the date range\n",
    "    search = client.search(\n",
    "        collections=[\"sentinel-2-l2a\"],\n",
    "        intersects=point,\n",
    "        datetime=f\"{start_date}/{end_date}\"\n",
    "    )\n",
    "\n",
    "    # Return items either as STAC objects or plain dictionaries\n",
    "    if return_as_dicts:\n",
    "        items = list(search.items_as_dicts())\n",
    "    else:\n",
    "        items = list(search.items())\n",
    "\n",
    "    return items"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82924152-de77-44ba-a55d-29469279a145",
   "metadata": {},
   "source": [
    "##### `latlon_to_buffer_bbox`\n",
    "\n",
    "This function **converts a latitude/longitude point** to a specified **projected coordinate system (EPSG)** and then generates a **square bounding box** centered on that point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7745eedf-6c75-477e-95f5-282ed49680a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def latlon_to_buffer_bbox(lat, lon, epsg, buffer=500):\n",
    "    \"\"\"\n",
    "    Convert a latitude/longitude point to a projected coordinate system (EPSG),\n",
    "    then generate a square bounding box centered on that point.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    lat : float\n",
    "        Latitude of the point of interest (WGS84).\n",
    "    lon : float\n",
    "        Longitude of the point of interest (WGS84).\n",
    "    epsg : str or int\n",
    "        Target projected coordinate system (e.g., EPSG:32632). Used for distance-based buffering.\n",
    "    buffer : float, default=500\n",
    "        Half-size of the square buffer (in meters). The output box will span\n",
    "        2*buffer in width and height.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    tuple\n",
    "        (minx, miny, maxx, maxy) bounding box coordinates in the target EPSG.\n",
    "    \"\"\"\n",
    "\n",
    "    # Transformer: convert geographic coordinates (lon, lat) to projected (x, y)\n",
    "    transformer = Transformer.from_crs(\"EPSG:4326\", epsg, always_xy=True)\n",
    "\n",
    "    # Perform the coordinate transformation\n",
    "    x, y = transformer.transform(lon, lat)\n",
    "\n",
    "    # Construct the bounding box by applying the buffer in projected units (meters)\n",
    "    minx = x - buffer\n",
    "    maxx = x + buffer\n",
    "    miny = y - buffer\n",
    "    maxy = y + buffer\n",
    "\n",
    "    return (minx, miny, maxx, maxy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b6cdb9c-db20-4495-bfbe-ca350f4299cf",
   "metadata": {},
   "source": [
    "##### `open_and_curate_data`\n",
    "\n",
    "This function **opens a Sentinel-2 STAC item (Zarr)**, performs a **spatial subset** around a given point, applies **SCL-based masking**, and returns an **`xarray.Dataset`** containing only the selected reflectance bands (e.g. `B04`, `B8A`) with a **time dimension**. It uses the `validate_scl` function from `zarr_wf_utils.py`.\n",
    "\n",
    "> **Note:** This function is a **delayed Dask object**, meaning it will only be executed when explicitly triggered through Dask."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ebafd3b-2f62-46b6-869b-6f3239a1fa14",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dask.delayed\n",
    "def open_and_curate_data(item, lat, lon, bands=[\"b04\", \"b8a\"], resolution=20, buffer=500, items_as_dicts=False):\n",
    "    \"\"\"\n",
    "    Open a Sentinel-2 STAC item (Zarr), spatially subset it around a point,\n",
    "    apply SCL-based masking, and return an xarray Dataset containing only\n",
    "    selected reflectance bands with a time dimension.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    item : pystac.Item or dict\n",
    "        STAC item describing the Sentinel-2 observation. Can be provided\n",
    "        either as a dict (e.g., after JSON serialization) or as a pystac.Item.\n",
    "    lat : float\n",
    "        Latitude of the point of interest (WGS84).\n",
    "    lon : float\n",
    "        Longitude of the point of interest (WGS84).\n",
    "    bands : list\n",
    "        Bands to retrieve.\n",
    "    resolution : int, optional\n",
    "        Spatial resolution to load for reflectance bands and SCL. Default is 20 m.\n",
    "    buffer : int, optional\n",
    "        Half-size of the square buffer (in meters) around the point of interest.\n",
    "        The function extracts a bounding box of size (2*buffer) centered on (lat, lon).\n",
    "    items_as_dicts : bool, optional\n",
    "        If True, treat `item` as a Python dictionary with STAC-like structure.\n",
    "        If False, treat it as a pystac.Item object.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    ds : xr.Dataset (wrapped in dask.delayed)\n",
    "        Dataset with dimensions (time, y, x) containing:\n",
    "        - reflectances\n",
    "        - only valid pixels according to SCL filtering\n",
    "        - only pixels inside the buffered bounding box\n",
    "        A time dimension is added so datasets can be concatenated later.\n",
    "    \"\"\"\n",
    "\n",
    "    # --------------------------------------------------------------\n",
    "    # 1. Extract STAC asset HREF and timestamp\n",
    "    # --------------------------------------------------------------\n",
    "\n",
    "    # Standard STAC read depending on input format\n",
    "    if items_as_dicts:\n",
    "        href = item[\"assets\"][\"product\"][\"href\"]\n",
    "        datetime_value = item[\"properties\"][\"datetime\"]\n",
    "    else:\n",
    "        href = item.assets[\"product\"].href\n",
    "        datetime_value = item.properties[\"datetime\"]\n",
    "\n",
    "    # Convert the STAC datetime to daily precision numpy datetime\n",
    "    # (removing the trailing \"Z\" timezone indicator)\n",
    "    time = np.datetime64(datetime_value.replace(\"Z\", \"\")).astype(\"datetime64[D]\")\n",
    "\n",
    "    # Resolution code used by S2 Zarr hierarchy (e.g., \"r20m\")\n",
    "    resolution = f\"r{resolution}m\"\n",
    "\n",
    "    # --------------------------------------------------------------\n",
    "    # 2. Open Zarr datatree\n",
    "    # --------------------------------------------------------------\n",
    "    ds = xr.open_datatree(\n",
    "        href,\n",
    "        engine=\"zarr\",\n",
    "        consolidated=True,\n",
    "        chunks=\"auto\"\n",
    "    )\n",
    "\n",
    "    # --------------------------------------------------------------\n",
    "    # 3. Determine projection and build projected bounding box\n",
    "    # --------------------------------------------------------------\n",
    "\n",
    "    # EPSG code for the scene (e.g., 32632 for Sentinel-2 tile)\n",
    "    epsg = ds.attrs[\"other_metadata\"][\"horizontal_CRS_code\"]\n",
    "\n",
    "    # Convert (lat, lon) to a projected bounding box centered on the point\n",
    "    # Buffer is in meters, so this is done in projected CRS\n",
    "    minx, miny, maxx, maxy = latlon_to_buffer_bbox(lat, lon, epsg, buffer)\n",
    "\n",
    "    # --------------------------------------------------------------\n",
    "    # 4. Extract SCL (Scene Classification Layer) and build valid mask\n",
    "    # --------------------------------------------------------------\n",
    "\n",
    "    # Access the classification layer at the correct resolution\n",
    "    scl = ds.conditions.mask.l2a_classification[resolution].scl\n",
    "\n",
    "    # Convert SCL to a boolean mask indicating valid surface reflectance pixels\n",
    "    valid_mask = validate_scl(scl)\n",
    "\n",
    "    # --------------------------------------------------------------\n",
    "    # 5. Extract reflectance bands and apply mask\n",
    "    # --------------------------------------------------------------\n",
    "\n",
    "    # reflectance[...] is a datatree, convert to dataset then select bands\n",
    "    ds = (\n",
    "        ds.measurements.reflectance[resolution]\n",
    "        .to_dataset()[bands]\n",
    "        .where(valid_mask)          # Apply SCL mask (invalid â†’ NaN)\n",
    "    )\n",
    "\n",
    "    # --------------------------------------------------------------\n",
    "    # 6. Spatial subsetting to bounding box\n",
    "    # --------------------------------------------------------------\n",
    "\n",
    "    ds = ds.where(\n",
    "        (ds.x >= minx) & (ds.x <= maxx) &\n",
    "        (ds.y >= miny) & (ds.y <= maxy),\n",
    "        drop=True\n",
    "    )\n",
    "\n",
    "    # --------------------------------------------------------------\n",
    "    # 7. Add time dimension for temporal stacking\n",
    "    # --------------------------------------------------------------\n",
    "\n",
    "    # Enforce the shape (time, y, x) so multiple calls can concatenate cleanly\n",
    "    ds = ds.expand_dims(time=[time])\n",
    "\n",
    "    return ds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "016d2cb4-7b49-444d-a10b-222d6c53bd3b",
   "metadata": {},
   "source": [
    "##### `curate_gpp`\n",
    "\n",
    "This function **loads a GPP time series**, computes **weekly anomalies**, identifies **extreme low-GPP events**, and filters these extremes to retain only events lasting at least a specified number of **consecutive weeks**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca35d7b0-fc84-4664-8872-e8e5017f04ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def curate_gpp(dataset=\"DE-Hai\", percentile=0.1, consecutive_weeks=2):\n",
    "    \"\"\"\n",
    "    Load a GPP time series, compute weekly anomalies, identify extreme low-GPP\n",
    "    events, and filter extremes to retain only events with at least a specified\n",
    "    number of consecutive weeks.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    dataset : str\n",
    "        Name of the CSV file (without extension) found in ./data/.\n",
    "    percentile : float\n",
    "        Lower-tail percentile used to define extreme negative anomalies\n",
    "        (e.g., 0.1 => 10th percentile of the anomaly distribution).\n",
    "    consecutive_weeks : int\n",
    "        Minimum run length of consecutive extreme weeks required for an\n",
    "        extreme event to be retained.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    df : pandas.DataFrame\n",
    "        A dataframe indexed by weekly timestamps, containing GPP, anomalies,\n",
    "        week-of-year, and a final binary 'extreme' flag.\n",
    "    \"\"\"\n",
    "\n",
    "    # --------------------------------------------------------------\n",
    "    # 1. Load & preprocess time series\n",
    "    # --------------------------------------------------------------\n",
    "    df = pd.read_csv(os.path.join(\"data\", f\"{dataset}.csv\"))\n",
    "\n",
    "    # Ensure 'time' is parsed as a datetime\n",
    "    df[\"time\"] = pd.to_datetime(df[\"time\"])\n",
    "\n",
    "    # Optionally restrict to a start date for consistency\n",
    "    df = df[df.time >= \"2017-01-01\"]\n",
    "\n",
    "    # Set time as index for easier resampling and time-based operations\n",
    "    df = df.set_index(\"time\")\n",
    "\n",
    "    # --------------------------------------------------------------\n",
    "    # 2. Temporal aggregation: convert to weekly median GPP\n",
    "    # --------------------------------------------------------------\n",
    "    df = df.resample(\"1W\").median()\n",
    "\n",
    "    # Extract week-of-year for building a weekly climatology\n",
    "    df[\"weekofyear\"] = df.index.isocalendar().week\n",
    "\n",
    "    # --------------------------------------------------------------\n",
    "    # 3. Compute weekly climatology (multi-year mean per week)\n",
    "    # --------------------------------------------------------------\n",
    "    df_msc = df.groupby(\"weekofyear\")[\"GPP_NT_VUT_REF\"].mean()\n",
    "\n",
    "    # Weekly anomaly = observed GPP - weekly climatological mean\n",
    "    df[\"anomaly\"] = df[\"GPP_NT_VUT_REF\"] - df[\"weekofyear\"].map(df_msc)\n",
    "\n",
    "    # --------------------------------------------------------------\n",
    "    # 4. Identify extreme anomalies using a Gaussian percentile threshold\n",
    "    # --------------------------------------------------------------\n",
    "    # Fit a normal distribution to the anomaly series\n",
    "    dist = sps.norm(\n",
    "        loc=df[\"anomaly\"].mean(),\n",
    "        scale=df[\"anomaly\"].std()\n",
    "    )\n",
    "\n",
    "    # Lower-tail anomaly threshold corresponding to the chosen percentile\n",
    "    q = np.abs(dist.ppf(percentile))\n",
    "\n",
    "    # Initial binary extreme flag: 1 = extreme low anomaly\n",
    "    df[\"extreme\"] = 0\n",
    "    df.loc[df[\"anomaly\"] <= -q, \"extreme\"] = 1\n",
    "\n",
    "    # --------------------------------------------------------------\n",
    "    # 5. Filter extremes: keep only runs with >= consecutive_weeks\n",
    "    # --------------------------------------------------------------\n",
    "    s = df[\"extreme\"]\n",
    "\n",
    "    # Identify contiguous groups of identical values (0-runs and 1-runs)\n",
    "    groups = (s != s.shift()).cumsum()\n",
    "\n",
    "    # Compute the run length for each time step\n",
    "    run_lengths = s.groupby(groups).transform(\"size\")\n",
    "\n",
    "    # Final extreme flag: extreme only if in a run of sufficient length\n",
    "    df[\"extreme\"] = ((s == 1) & (run_lengths >= consecutive_weeks)).astype(int)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea3de320-626f-49e6-b37a-07688c88423d",
   "metadata": {},
   "source": [
    "Global colors to be used for visualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b23daee9-ed2b-45f4-9f4e-15612557bd7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Colors for indices\n",
    "NDVI_COLOR = 'limegreen'\n",
    "kNDVI_COLOR = 'darkviolet'\n",
    "\n",
    "# Color for zero line in anomalies\n",
    "ZERO_COLOR = 'red'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16",
   "metadata": {},
   "source": [
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## Vegetation Anomalies in the Hainich National Park"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e37c50c8-d7e2-4a5b-80e0-2bd94a40b036",
   "metadata": {},
   "source": [
    "We will start the notebook with a forest ecosystem that was severely affacted by the drought of 2018: DE-Hai."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1524abec-dbc6-4b65-9468-9b27f021d3a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DE-Hai Coordinates\n",
    "HAI_LAT = 51.079212\n",
    "HAI_LON = 10.452168"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18",
   "metadata": {},
   "source": [
    "Initialize a **Dask distributed client** to enable parallel and delayed computation. This will manage the execution of tasks, such as loading and processing large Sentinel-2 zarr datasets, efficiently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95b4f0e4-432b-4a6a-bdc9-7c0cff862c9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = Client()\n",
    "client"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4098836f-a015-4b19-8457-38261c536e11",
   "metadata": {},
   "source": [
    "### Load the GPP data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2303c3d-f3df-468a-ba19-420ee50f6dfb",
   "metadata": {},
   "source": [
    "Load the **GPP time series** for the DE-Hai site and compute **weekly anomalies**, identifying extreme low-GPP events."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83335537-a470-43c4-9916-6f5156a9a9da",
   "metadata": {},
   "outputs": [],
   "source": [
    "HAI_df = curate_gpp(\"DE-Hai\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a22aab5-c6a4-4fe4-ad57-dcccb370a4c4",
   "metadata": {},
   "source": [
    "### Create the Sentinel-2 L2A Data Cube\n",
    "\n",
    "Query the **EOPF STAC API** to retrieve Sentinel-2 L2A items for the DE-Hai site. For each item, **open and curate** the data by subsetting around the site coordinates and selecting the relevant bands.  \n",
    "\n",
    "The datasets are then **computed in parallel with Dask**, concatenated along the **time dimension**, sorted by time, and finally **loaded into memory** as a single `xarray.Dataset`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68c32e62-b6a4-4a56-99af-a37c25276398",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all items as a list of dicts\n",
    "HAI_items = get_items(HAI_LAT,HAI_LON,return_as_dicts=True)\n",
    "\n",
    "# Create the delayed Dask objects\n",
    "HAI_ds = [open_and_curate_data(item,lat=HAI_LAT,lon=HAI_LON,items_as_dicts=True) for item in HAI_items]\n",
    "\n",
    "# Compute the delayed objects in parallel. This outputs a list of xarray.Dataset objects\n",
    "HAI_data = dask.compute(*HAI_ds)\n",
    "\n",
    "# Concatenate the previous list using the time dimension and sort it\n",
    "HAI_ds = xr.concat(HAI_data,dim=\"time\").sortby(\"time\")\n",
    "\n",
    "# Load it into memory\n",
    "HAI_ds = HAI_ds.compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4e05f2d-988d-400c-8464-174b23d2bcac",
   "metadata": {},
   "source": [
    "### Compute Vegetation Indices\n",
    "\n",
    "Compute **spectral indices** for the DE-Hai dataset using the [`spyndex` package](https://github.com/awesome-spectral-indices/spyndex). In this example, we calculate **NDVI** and **kernel NDVI (kNDVI)**.\n",
    "\n",
    "- [**NDVI (Normalized Difference Vegetation Index)**](https://ntrs.nasa.gov/citations/19740022614) is a widely used index to monitor vegetation health and greenness. It is calculated as:\n",
    "\n",
    "$$\\text{NDVI} = \\frac{N - R}{N + R}$$\n",
    "\n",
    "where `N` is the near-infrared band (`B8A`) and `R` is the red band (`B04`).\n",
    "\n",
    "- [**kNDVI (Kernel NDVI)**](https://doi.org/10.1126/sciadv.abc7447) is a kernelized version of NDVI. Here, an **RBF (Radial Basis Function) kernel** is applied:\n",
    "\n",
    "$$\\text{kNDVI} = \\frac{k(N,N) - k(N,R)}{k(N,N) + k(N,R)}$$\n",
    "\n",
    "where $k(a,b)$ is the RBF kernel:\n",
    "\n",
    "$$k(N,N) = 1$$\n",
    "\n",
    "$$k(N,R) = \\exp\\left(-\\frac{(N-R)^2}{2 \\sigma^2}\\right)$$\n",
    "\n",
    "Here, $\\sigma$ is claculated as the median in the time dimension of $0.5(N+R)$.\n",
    "\n",
    "The `spyndex.computeIndex` function computes both indices across the time series and stores them in an **`xarray.Dataset`** named `idx` for subsequent anomaly analysis.\n",
    "\n",
    "The `spyndex.computeKernel` function computes the kernel for the kNDVI.\n",
    "\n",
    "Note that in both cases `spyndex` just requires the data for computing the indices without the need of hard-coding the formulas.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68c55242-3900-4b74-b9c0-fc9c97860b82",
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = spyndex.computeIndex(\n",
    "    [\"NDVI\",\"kNDVI\"], # Indices to compute\n",
    "    N = HAI_ds[\"b8a\"], # NIR band\n",
    "    R = HAI_ds[\"b04\"], # Red band\n",
    "    kNN = 1.0,\n",
    "    kNR = spyndex.computeKernel(\n",
    "        \"RBF\", # RBF kernel\n",
    "        a = HAI_ds[\"b8a\"],\n",
    "        b = HAI_ds[\"b04\"],\n",
    "        sigma = ((HAI_ds[\"b8a\"] + HAI_ds[\"b04\"])/2.0).median(\"time\")\n",
    "    )\n",
    ").to_dataset(\"index\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "667a0b66-e3aa-460e-9e6b-cdb85a9472fc",
   "metadata": {},
   "source": [
    "Add the name and units of each index to the attributes according to the CF conventions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f915889-4abf-4568-8354-842e7eb2a155",
   "metadata": {},
   "outputs": [],
   "source": [
    "idx.NDVI.attrs[\"long_name\"] = spyndex.indices.NDVI.long_name\n",
    "idx.NDVI.attrs[\"units\"] = \"1\"\n",
    "\n",
    "idx.kNDVI.attrs[\"long_name\"] = spyndex.indices.kNDVI.long_name\n",
    "idx.kNDVI.attrs[\"units\"] = \"1\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cf1500e-b63d-4df2-9229-02ac980b691d",
   "metadata": {},
   "source": [
    "Resample the NDVI and kNDVI time series to **weekly frequency**, taking the **median** within each week. After resampling, fill temporal gaps by applying **cubic interpolation** along the time dimension. This produces smooth, continuous weekly index time series suitable for anomaly computation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dc7e8fe-ea30-4600-bf46-6d7f68f1f362",
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = idx.resample(time=\"1W\").median().interpolate_na(dim=\"time\",method=\"cubic\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "169c1acb-f29c-4237-8072-e888189a8a18",
   "metadata": {},
   "source": [
    "### Calculate Vegetation Anomalies\n",
    "\n",
    "Compute the **median seasonal cycle** (MSC) for NDVI and kNDVI.\n",
    "\n",
    "By grouping the time series by `weekofyear` and taking the **median across years**, this step produces a climatological baseline that represents the typical vegetation state for each week of the year.  \n",
    "\n",
    "The MSC will be used later to derive vegetation anomalies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1efb72c-8f2f-4198-9f88-a9401d3882b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "msc = idx.groupby(\"time.weekofyear\").median(\"time\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e878be5-3db6-4027-a2a9-75321ff7aab9",
   "metadata": {},
   "source": [
    "Plot the MSC of the NDVI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ae9b0d9-a01e-4215-8d30-1a632b69b90f",
   "metadata": {},
   "outputs": [],
   "source": [
    "msc.NDVI.plot.imshow(col = \"weekofyear\",cmap = \"viridis\",col_wrap = 8,vmin=0,vmax=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f175174c-5f7f-4cac-b002-1a9da2b43c34",
   "metadata": {},
   "source": [
    "Plot the MSC of the kNDVI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6566000-0a42-4834-a3fd-38135d9c6373",
   "metadata": {},
   "outputs": [],
   "source": [
    "msc.kNDVI.plot.imshow(col = \"weekofyear\",cmap = \"viridis\",col_wrap = 8,vmin=0,vmax=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dbfa647-1b75-4324-8dfa-c3428067aeaa",
   "metadata": {},
   "source": [
    "Compute **vegetation anomalies** by subtracting the **median seasonal cycle (MSC)** from the weekly NDVI and kNDVI values. This step isolates deviations from the expected seasonal pattern, allowing us to identify abnormal vegetation conditions potentially linked to stress or extreme events."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8348bd7e-f8b1-4ce8-a518-66bcb6cce403",
   "metadata": {},
   "outputs": [],
   "source": [
    "idx_anomalies = idx.groupby(\"time.weekofyear\") - msc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2d44388-2148-4cd8-945a-c851dc76e565",
   "metadata": {},
   "source": [
    "Add the name and units of each index anomaly to the attributes according to the CF conventions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9662402b-5002-46ea-a3df-c7d19709d59e",
   "metadata": {},
   "outputs": [],
   "source": [
    "idx_anomalies.NDVI.attrs[\"long_name\"] = spyndex.indices.NDVI.long_name + \" Anomaly\"\n",
    "idx_anomalies.NDVI.attrs[\"units\"] = \"1\"\n",
    "\n",
    "idx_anomalies.kNDVI.attrs[\"long_name\"] = spyndex.indices.kNDVI.long_name + \" Anomaly\"\n",
    "idx_anomalies.kNDVI.attrs[\"units\"] = \"1\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80a33155-82e0-4a56-99e5-4140be40f3fa",
   "metadata": {},
   "source": [
    "### Visualize Time Series\n",
    "\n",
    "Aggregate the indices in space using the median to produce a time series."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99d6274d-8ce7-4435-9f34-0f34e582bfee",
   "metadata": {},
   "outputs": [],
   "source": [
    "idx_agg = idx.median([\"x\",\"y\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e575743f-21dc-4a5a-9def-fcee571d0646",
   "metadata": {},
   "source": [
    "Plot the NDVI and kNDVI time series together with the GPP measurements for the DE-Hai site.\n",
    "\n",
    "A secondary axis is used to display GPP, allowing direct visual comparison between vegetation dynamics and ecosystem productivity.\n",
    "\n",
    "Extreme low-GPP events are highlighted as shaded red intervals.  \n",
    "\n",
    "These events are defined as **periods of at least two consecutive days** in which GPP anomalies fall **below the 10th percentile** of the lower tail of the distribution.  \n",
    "\n",
    "This visualization helps link vegetation index anomalies to observed reductions in carbon uptake, revealing how forest canopy responses relate to ecosystem-scale stress signals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3e20a0b-0fc8-4dad-97ed-1d7f20984034",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(15, 3))\n",
    "\n",
    "ax.plot(idx_agg.time, idx_agg[\"NDVI\"],  color=NDVI_COLOR,  label=\"NDVI\")\n",
    "ax.plot(idx_agg.time, idx_agg[\"kNDVI\"], color=kNDVI_COLOR, label=\"kNDVI\")\n",
    "ax.set_ylim([-0.15,1.2])\n",
    "ax.set_ylabel(\"VI\")\n",
    "ax.legend(loc=\"upper left\")\n",
    "\n",
    "ax2 = ax.twinx()\n",
    "ax2.scatter(df.index, df[\"GPP_NT_VUT_REF\"], \n",
    "            s=20, color=\"grey\", alpha=0.6, label=\"GPP\")\n",
    "ax2.set_ylim([-3.5,17.5])\n",
    "ax2.set_ylabel(\"GPP\")\n",
    "ax2.legend(loc=\"upper right\")\n",
    "\n",
    "extreme_mask = df[\"extreme\"] == 1\n",
    "groups = (extreme_mask != extreme_mask.shift()).cumsum()\n",
    "\n",
    "for _, group in df[extreme_mask].groupby(groups):\n",
    "    start = group.index.min()\n",
    "    end   = group.index.max()\n",
    "    ax.axvspan(start, end, color=\"red\", alpha=0.15)\n",
    "\n",
    "plt.title(\"NDVI, kNDVI, GPP and Extreme Events\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9050d4ac-199c-4054-97f4-6729a076bef3",
   "metadata": {},
   "source": [
    "Aggregate the anomalies of the indices in space using the median to produce a time series."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5a6729a-c0e0-4685-a17b-bb723170333b",
   "metadata": {},
   "outputs": [],
   "source": [
    "idx_anomalies_agg = idx_anomalies.median([\"x\",\"y\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0c52b84-0d14-401d-9b55-5e3c1eb7e20b",
   "metadata": {},
   "source": [
    "Visualize the **anomaly time series** of NDVI, kNDVI, and GPP for the DE-Hai site.  \n",
    "\n",
    "Here, both vegetation indices and GPP have been transformed into **weekly anomalies**, representing deviations from their typical seasonal cycles. A horizontal line at zero indicates the expected baseline.\n",
    "\n",
    "A secondary axis displays **GPP anomalies**, allowing direct comparison between canopy-level spectral responses and ecosystem-level carbon uptake changes.\n",
    "\n",
    "Extreme low-GPP events are shown as shaded red intervals.  \n",
    "\n",
    "This plot highlights how vegetation index anomalies co-occur with (e.g. 2021) or diverge (e.g. 2018) from carbon uptake anomalies, offering insight into forest responses to stress events."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "085be88b-40b1-4451-a00e-232dbf32a5ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(15, 3))\n",
    "\n",
    "ax.plot(idx_anomalies_agg.time, idx_anomalies_agg[\"NDVI\"],  color=NDVI_COLOR,  label=\"NDVI\")\n",
    "ax.plot(idx_anomalies_agg.time, idx_anomalies_agg[\"kNDVI\"], color=kNDVI_COLOR, label=\"kNDVI\")\n",
    "ax.axhline(0, color=ZERO_COLOR, linewidth=1)\n",
    "ax.set_ylim([-0.45,0.45])\n",
    "ax.set_ylabel(\"VI Anomaly\")\n",
    "ax.legend(loc=\"upper left\")\n",
    "\n",
    "ax2 = ax.twinx()\n",
    "ax2.scatter(df.index, df[\"anomaly\"], \n",
    "            s=20, color=\"grey\", alpha=0.6, label=\"GPP\")\n",
    "ax2.set_ylim([-6.5,6.5])\n",
    "ax2.set_ylabel(\"GPP Anomaly\")\n",
    "ax2.legend(loc=\"upper right\")\n",
    "\n",
    "extreme_mask = df[\"extreme\"] == 1\n",
    "groups = (extreme_mask != extreme_mask.shift()).cumsum()\n",
    "\n",
    "for _, group in df[extreme_mask].groupby(groups):\n",
    "    start = group.index.min()\n",
    "    end   = group.index.max()\n",
    "    ax.axvspan(start, end, color=\"red\", alpha=0.15)\n",
    "\n",
    "plt.title(\"NDVI, kNDVI, GPP Anomalies and Extreme Events\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27",
   "metadata": {},
   "source": [
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28",
   "metadata": {},
   "source": [
    "## ðŸ’ª Now it is your turn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5742263-264b-4c9f-81f5-3c38946f3e4d",
   "metadata": {},
   "source": [
    "The following exercises will help you reproduce the previous workflow for another dataset.\n",
    "\n",
    "### Task 1: Create a data cube for DE-Tha\n",
    "* Use the coordinates of DE-Tha (provided in the cell below) to create a data cube for this site.\n",
    "* Retrieve the red edge bands in addition to the NIR and red bands.\n",
    "* Modify the code as you need."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c6f7cc0-ae17-46e7-82d8-cdcaf42fe0c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DE-Tha Coordinates\n",
    "THA_LAT = 50.9625\n",
    "THA_LON = 13.56515\n",
    "\n",
    "# Get all items as a list of dicts\n",
    "# THA_items = get_items(THA_LAT,THA_LON,return_as_dicts=True)\n",
    "\n",
    "# Create the delayed Dask objects\n",
    "# THA_ds = [open_and_curate_data(..., bands=[\"b04\", \"b05\", \"b06\", \"b07\", \"b8a\"]) for ...]\n",
    "\n",
    "# ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f70efa0-00bf-408a-adbb-ecccb005d046",
   "metadata": {},
   "source": [
    "### Task 2: Compute Vegetation Indices\n",
    "* Select a vegetation index from [Awesome Spectral Indices](https://github.com/awesome-spectral-indices/awesome-spectral-indices) that uses the Red Edge bands.\n",
    "* Modify the code as you need."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ea1945e-5a8b-4c4f-b59f-67e2408a2378",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Indices that include any of the red edge bands\n",
    "for idx, attrs in spyndex.indices.items():\n",
    "    if any(item in [\"RE1\",\"RE2\",\"RE3\"] for item in attrs.bands):\n",
    "        print(idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29",
   "metadata": {},
   "source": [
    "### Task 3: Calculate Vegetation Anomalies\n",
    "* Calculate anomalies for the selected index and compare them against the GPP anomalies of DE-Tha."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61e02f51-00e9-461e-b4d7-4a006df4d9dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "THA_df = curate_gpp(\"DE-Tha\")\n",
    "\n",
    "# THA_df[\"GPP_NT_VUT_REF\"] -> GPP values\n",
    "# THA_df[\"time\"] -> time simension\n",
    "# THA_df[\"anomaly\"] -> Anomalies\n",
    "# THA_df[\"extreme\"] -> Extreme = 1, Normal condition = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30",
   "metadata": {},
   "source": [
    "## Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31",
   "metadata": {},
   "source": [
    "In this notebook, we explored how **Sentinel-2 L2A zarr data cubes** can be used to monitor forest vegetation dynamics and detect anomalous behavior linked to ecosystem stress. By leveraging zarr, STAC-based discovery, and `xarray`/`Dask` for scalable computation, we built an end-to-end workflow that included:\n",
    "\n",
    "- Accessing Sentinel-2 data from the EOPF zarr STAC  \n",
    "- Creating spatiotemporal data cubes centered on forest monitoring sites  \n",
    "- Computing spectral indices (NDVI, kNDVI) using the Awesome Spectral Indices catalogue  \n",
    "- Constructing weekly time series and climatological baselines  \n",
    "- Deriving vegetation anomalies and comparing them with GPP anomalies from ICOS  \n",
    "- Identifying and visualizing extreme low-GPP events\n",
    "\n",
    "Through the joint analysis of **spectral indices** and **ecosystem productivity**, we demonstrated how remote sensing can reveal (or not) signals of forest stress and complement flux tower observations. This workflow illustrates the value of **zarr-based EO data**, **open standards (STAC)**, and **modern geospatial Python tools** for reproducible and scalable environmental monitoring."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60edc3e8-2e39-414b-94d1-7266f0b48505",
   "metadata": {},
   "source": [
    "### Acknowledgements\n",
    "\n",
    "We would like to thank ICOS for providing the data on the Ecosystem stations DE-Hai [1] and DE-Tha [2]."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19645d83-94d8-46ba-bf2d-8a5a6c311b5e",
   "metadata": {},
   "source": [
    "### References\n",
    "\n",
    "[1] Knohl, A., Schulze, E.-D., Kolle, O., & Buchmann, N. (2003). Large carbon uptake by an unmanaged 250-year-old deciduous forest in Central Germany. Agricultural and Forest Meteorology, 118(3â€“4), 151â€“167. https://doi.org/10.1016/s0168-1923(03)00115-1\n",
    "\n",
    "[2] GrÃ¼nwald, T., & Bernhofer, C. (2007). A decade of carbon, water and energy flux measurements of an old spruce forest at the Anchor Station Tharandt. Tellus B: Chemical and Physical Meteorology, 59(3), 387. https://doi.org/10.1111/j.1600-0889.2007.00259.x\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
