{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "---\n",
    "title: \"Zarr Overviews - Visualizing Multiscale Pyramids\"\n",
    "execute:\n",
    "  enabled: true\n",
    "  keep-ipynb: false\n",
    "  freeze: auto\n",
    "format: html\n",
    "---"
   ],
   "id": "1cb7d228abd53cf"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://jupyterhub.user.eopf.eodc.eu/hub/user-redirect/git-pull?repo=https://github.com/eopf-toolkit/eopf-101&branch=main&urlpath=lab/tree/eopf-101/66_use_overviews.ipynb\" target=\"_blank\">\n",
    "  <button style=\"background-color:#0072ce; color:white; padding:0.6em 1.2em; font-size:1rem; border:none; border-radius:6px; margin-top:1em;\">\n",
    "    üöÄ Launch this notebook in JupyterLab\n",
    "  </button>\n",
    "</a>"
   ],
   "id": "f3b1ee0dfbf95447"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction"
   ],
   "id": "b42aadbc6eb45035"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we will explore and visualize the **multiscale overview pyramid** created in the previous tutorial ([65_create_overviews.ipynb](65_create_overviews.ipynb)). Building upon what we learned about creating overviews, we will now focus on how to use them effectively for interactive visualization and exploration of large Earth Observation datasets.\n",
    "\n",
    "Overviews enable efficient visualization by providing progressively coarser representations of the data, allowing us to quickly navigate and explore large satellite images without loading the full-resolution dataset every time."
   ],
   "id": "7a48206eec074822"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What we will learn"
   ],
   "id": "e23df9afd326daea"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- üìä How to parse and inspect the multiscales metadata structure?\n",
    "- üîç How to load different overview levels dynamically from the Zarr hierarchy?\n",
    "- üé® How to create RGB composites at different resolutions for visualization?\n",
    "- üñ±Ô∏è How to build interactive widgets for exploring zoom levels?\n",
    "- üó∫Ô∏è How to create an interactive web map with automatic level selection?"
   ],
   "id": "111c69424bcd1790"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prerequisites"
   ],
   "id": "7375faba27350410"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook assumes you have:\n",
    "- Completed **[65_create_overviews.ipynb](65_create_overviews.ipynb)** (created the overview hierarchy)\n",
    "- Successfully generated multiscales metadata for Sentinel-2 L2A reflectance data\n",
    "- A local Zarr dataset with overview levels stored in the `overviews/` subfolder\n",
    "\n",
    "Required packages:\n",
    "- **Core**: `xarray`, `zarr`, `numpy`, `pandas`\n",
    "- **Visualization**: `matplotlib`, `ipywidgets`\n",
    "- **Web mapping**: `ipyleaflet`, `pyproj`, `pillow`\n"
   ],
   "id": "8510b7d326c26516"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>"
   ],
   "id": "f660e6134912cb38"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import libraries"
   ],
   "id": "79b0e11c55b94500"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "import xarray as xr, os, json, pandas as pd, warnings, numpy as np, matplotlib.pyplot as plt\n",
    "from ipywidgets import interact, IntSlider\n",
    "warnings.filterwarnings(\"ignore\")"
   ],
   "id": "a98da66501ad5330",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>"
   ],
   "id": "db25f0699dcd0d98"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding the Overview Hierarchy"
   ],
   "id": "37ca479fb0e40f22"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first step in working with overviews is to understand their structure. In the previous tutorial, we created a multiscale pyramid with several levels (L0 through L7), where each level represents the data at a different resolution.\n",
    "\n",
    "Let's load the dataset and examine the **multiscales metadata** that describes this hierarchy. This metadata follows the GeoZarr Overviews specification and contains information about:\n",
    "- Which levels exist and where they are stored in the Zarr hierarchy\n",
    "- How each level relates to others (parent-child relationships through `derived_from`)\n",
    "- The spatial resolution (`cell_size`) of each level in meters\n",
    "- What resampling method was used to create each level"
   ],
   "id": "15716576451e251b"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Opening the dataset"
   ],
   "id": "e2b6d0db308835b6"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start by opening the dataset at the group level where our overviews are stored. This is the same path we used in the previous tutorial when creating the overviews (`measurements/reflectance/r10m`)."
   ],
   "id": "6912723dab68ef0e"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "local_zarr_path = \"output/S2A_MSIL2A_20250831T135741_N0511_R010_T26WPD_20250831T185012.zarr\"\n",
    "variable_group_path = \"measurements/reflectance/r10m\"\n",
    "dataset_path = os.path.join(local_zarr_path, variable_group_path)\n",
    "dataset = xr.open_dataset(dataset_path, engine=\"zarr\")  # Load dataset\n",
    "print(f\"Dataset: {dict(dataset.dims)} | Variables: {list(dataset.data_vars)} | Multiscales: {'multiscales' in dataset.attrs}\\n\")"
   ],
   "id": "5743bdcb19029797",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output confirms that our dataset has dimensions of 10,980 √ó 10,980 pixels and contains four spectral bands (b02, b03, b04, b08), along with the multiscales metadata we created in the previous tutorial."
   ],
   "id": "131199b77aa9d97d"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parsing the multiscales metadata"
   ],
   "id": "be60917e043c3c72"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `multiscales` attribute contains all the information about our overview pyramid. Let's parse it into a table format using pandas to make it easier to understand.\n",
    "\n",
    "Each row in the table represents one resolution level:\n",
    "- **id**: Level identifier (L0, L1, L2, etc.)\n",
    "- **path**: Where this level is stored in the Zarr hierarchy (`.` means current group, `overviews/L*` means subfolder)\n",
    "- **cell_size**: Spatial resolution in meters as `[x_res, y_res]`\n",
    "- **derived_from**: Which parent level this was created from\n",
    "- **factors**: Downsampling factors applied as `[y_factor, x_factor]`\n",
    "- **resampling_method**: How pixels were aggregated (average, nearest, etc.)"
   ],
   "id": "493aea49dfc60e63"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Parse multiscales metadata\n",
    "multiscales= dataset.attrs[\"multiscales\"]\n",
    "layout_df = pd.DataFrame(multiscales[\"layout\"])\n",
    "print(\"Overview Level Hierarchy:\")\n",
    "display(layout_df)"
   ],
   "id": "5621dd6fbc68fd05",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice the hierarchical structure:\n",
    "- **L0** has `path=\".\"`, meaning it references the native data at the current group level (no data duplication)\n",
    "- **L1-L7** have `path=\"overviews/L*\"`, stored in the `overviews/` subfolder we created\n",
    "- Each level is progressively coarser: L1 is 2√ó downsampled, L2 is 4√ó, L3 is 8√ó, and so on\n",
    "- The `cell_size` doubles at each level: 10m ‚Üí 20m ‚Üí 40m ‚Üí 80m ‚Üí 160m ‚Üí 320m ‚Üí 640m ‚Üí 1280m\n",
    "- All levels use `average` resampling, which is appropriate for continuous reflectance data"
   ],
   "id": "8420fc1f5dc3a94a"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>"
   ],
   "id": "5f8c2d96775b74a3"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Overview Levels"
   ],
   "id": "71f2a6e1302a8b8a"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we understand the structure from the metadata, let's load each overview level into memory. We'll iterate through the layout array from the multiscales metadata and open each level as a separate xarray Dataset.\n",
    "\n",
    "The code below:\n",
    "1. Creates an empty dictionary to store all overview datasets\n",
    "2. Loops through each entry in the layout metadata\n",
    "3. Constructs the full path by joining the base path with the level's relative path\n",
    "4. Opens each level using `xr.open_dataset()` with the Zarr engine\n",
    "5. Displays the dimensions and downsampling factor for each level"
   ],
   "id": "18e4f285d96677ee"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "overview_datasets = {}  # Dictionary to store all levels\n",
    "for entry in multiscales[\"layout\"]:\n",
    "    level_id, level_path = entry[\"id\"], entry[\"path\"]\n",
    "    full_path = os.path.normpath(os.path.join(dataset_path, level_path))\n",
    "    overview_datasets[level_id] = xr.open_dataset(full_path, engine=\"zarr\")\n",
    "    print(f\"Loaded  {level_id}: {overview_datasets[level_id]['b02'].shape[0]:5d} √ó {overview_datasets[level_id]['b02'].shape[1]:5d} pixels\")"
   ],
   "id": "27a6fda693d473e4",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observe how the dimensions decrease at each level:\n",
    "- **L0**: 10,980 √ó 10,980 pixels (full resolution, ~121 megapixels)\n",
    "- **L1**: 5,490 √ó 5,490 pixels (half resolution, ~30 megapixels)\n",
    "- **L2**: 2,745 √ó 2,745 pixels (quarter resolution, ~7.5 megapixels)\n",
    "- **L7**: 85 √ó 85 pixels (128√ó downsampled, ~7 kilopixels)\n",
    "\n",
    "Lower resolution levels are much faster to load and visualize, making them ideal for quick previews, thumbnails, or zoomed-out views. This is the key benefit of the overview pyramid structure."
   ],
   "id": "62e851aba0f2577d"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>"
   ],
   "id": "4989b38f5144bd2e"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Visualization of Overviews",
   "id": "c772d135e20b78fc"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating RGB Composites\n",
    "\n",
    "To visualize our Sentinel-2 data, we'll create **RGB true-color composites** by combining three spectral bands:\n",
    "- **Red channel**: Band 4 (b04) at 664 nm - sensitive to red wavelengths\n",
    "- **Green channel**: Band 3 (b03) at 560 nm - sensitive to green wavelengths  \n",
    "- **Blue channel**: Band 2 (b02) at 490 nm - sensitive to blue wavelengths\n",
    "\n",
    "However, raw reflectance values (typically ranging from 0 to ~0.4) don't display well directly. We need to apply **contrast enhancement** through percentile stretching to make features visible and create visually appealing images."
   ],
   "id": "2531d05266896851"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Percentile stretching for contrast enhancement"
   ],
   "id": "bbb45a9ece01c974"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The normalize() helper function below performs **percentile-based contrast stretching**, a standard technique in remote sensing visualization. It works by:\n",
    "\n",
    "1. Filtering out NaN (fill) values, which are common in Earth Observation data due to clouds, partial coverage, or sensor issues\n",
    "2. Computing the 2nd and 98th percentile values of the valid data\n",
    "3. Linearly mapping these percentile values to 0 and 1 respectively\n",
    "4. Clipping any values outside this range\n",
    "\n",
    "This approach is robust to outliers (very bright or dark pixels) and produces visually balanced images. **Critical note**: Proper NaN handling is essential - if NaN values aren't filtered out, `np.percentile()` returns NaN, causing the entire image to display as white/blank."
   ],
   "id": "9fbe6e61f9dd20fe"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "    # Normalize each channel using percentile stretch (2% - 98%)\n",
    "def normalize(band):\n",
    "    valid_pixels = band[~np.isnan(band)]  # ‚Üê ADD THIS LINE\n",
    "    if len(valid_pixels) == 0:\n",
    "        return np.zeros_like(band)\n",
    "    p2, p98 = np.percentile(valid_pixels, [2, 98])  # ‚Üê USE valid_pixels\n",
    "    if p98 == p2:\n",
    "        return np.zeros_like(band)\n",
    "    normalized = (band - p2) / (p98 - p2)\n",
    "    return np.clip(normalized, 0, 1)"
   ],
   "id": "e494be76bc2e1dd",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interactive visualization with zoom levels"
   ],
   "id": "b2d69892cd6b0b6"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's create an **interactive visualization** that demonstrates the power of overviews. The widget below allows you to:\n",
    "\n",
    "- **Move the slider** to switch between zoom levels (0 = highest resolution L0, 7 = lowest resolution L7)\n",
    "- **Observe the performance difference**: Lower resolution levels (L5-L7) display almost instantly, while higher resolution levels take longer\n",
    "- **Notice the quality tradeoff**: Higher levels show more detail but require more time to load and render\n",
    "\n",
    "The function crops each image to the top-left quarter to make the display more responsive. Even with this cropping, L0 at full resolution would be 2,745 √ó 2,745 pixels, which is still quite large. This demonstrates why overviews are so valuable for interactive applications."
   ],
   "id": "a232ab4afd961f0d"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "\n",
    "def visualize_level(i):\n",
    "    \"\"\"Show RGB composite for overview level (cropped top-left).\"\"\"\n",
    "    ds = overview_datasets[f\"L{i}\"]                         # Select overview level\n",
    "    r, g, b = [ds[b].values for b in ('b04', 'b03', 'b02')] # Extract RGB bands\n",
    "    h, w = r.shape\n",
    "    rgb = np.dstack([r, g, b])[:h//4, :w//4]                # Crop top-left quarter\n",
    "    img = np.dstack([normalize(rgb[..., j]) for j in range(3)]) # Normalise bands\n",
    "    fig, ax = plt.subplots(figsize=(4.5, 4.5))               # Set figure size\n",
    "    ax.imshow(img)                                           # Display RGB image\n",
    "    plt.show()\n",
    "interact(visualize_level, i=IntSlider(min=0, max=7, value=4, description='Zoom Level')); # Interactive slider"
   ],
   "id": "d63e28291984bd14",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import xarray as xr\n",
    "print(xr.__version__)"
   ],
   "id": "6883355bf724beff",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>"
   ],
   "id": "c169432047d2ae6a"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparing Multiple Resolution Levels"
   ],
   "id": "89b1e2454cf8a045"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To better understand the effect of downsampling, let's compare **multiple overview levels side-by-side**. This visual comparison helps us see:\n",
    "\n",
    "- **How much spatial detail is lost** at each downsampling level\n",
    "- **The tradeoff between image quality and file size/performance** - L2 is 16√ó smaller than L0 but still retains good detail\n",
    "- **Which level is appropriate for different use cases**: L2 for regional analysis, L5 for quick previews, L7 for thumbnails\n",
    "\n",
    "The function below displays three different levels (L2, L5, and L7) to show this progression from medium to very coarse resolution."
   ],
   "id": "3e1ddeb10a3ce25f"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "def show_levels(levels=(2, 5, 7)):\n",
    "    \"\"\"Display RGB composites for selected overview levels.\"\"\"\n",
    "    fig, axes = plt.subplots(1, len(levels), figsize=(12, 4))\n",
    "    for ax, i in zip(axes, levels):\n",
    "        ds = overview_datasets[f\"L{i}\"]                              # Select overview level\n",
    "        rgb = np.dstack([ds[b].values for b in ('b04', 'b03', 'b02')]) # Stack RGB bands\n",
    "        img = np.dstack([normalize(rgb[..., j]) for j in range(3)])     # Normalise\n",
    "        ax.imshow(img); ax.axis(\"off\")\n",
    "        ax.set_title(f\"L{i} | {rgb.shape[0]}√ó{rgb.shape[1]} | √ó{2**i}\", fontsize=10)\n",
    "    plt.tight_layout(); plt.show()\n",
    "\n",
    "show_levels()  # Show levels 0, 3, and 7"
   ],
   "id": "21be16a1b69a88fa",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice the progressive loss of detail:\n",
    "- **L2 (4√ó downsampled, 2745√ó2745 px)**: Retains good spatial detail, suitable for regional-scale visualization and analysis\n",
    "- **L5 (32√ó downsampled, 343√ó343 px)**: Shows general patterns and major features but loses fine details like small fields or roads\n",
    "- **L7 (128√ó downsampled, 85√ó85 px)**: Provides only a coarse overview showing large-scale patterns, useful for thumbnails or global context\n",
    "\n",
    "This demonstrates why overviews are essential for **progressive rendering** in web mapping applications: the application can display L7 instantly for context, then progressively load L5, L3, L1 as the user waits, and finally load L0 when zoomed in for detailed analysis."
   ],
   "id": "13ceb18cbb0da712"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>"
   ],
   "id": "674a712254895fe1"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interactive Web Map with Automatic Level Selection"
   ],
   "id": "838a3f7c3b97fdac"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's put everything together by creating a **professional-like interactive web map** using ipyleaflet. This demonstrates a real-world application of overviews for geospatial visualization.\n",
    "\n",
    "**Key features:**\n",
    "- **Automatic level selection**: As you zoom in or out, the map automatically switches to the most appropriate overview level based on the current zoom level and ground resolution\n",
    "- **Manual override**: Use the slider to manually select a specific level if you want to compare quality at different resolutions\n",
    "- **Standard map controls**: Pan by clicking and dragging, zoom with mouse wheel or +/- buttons, switch basemap layers\n",
    "- **Real-time information**: The label shows which level is displayed, its dimensions, cell size, and current ground resolution\n",
    "\n",
    "The automatic selection algorithm works by:\n",
    "1. Calculating the ground resolution (meters per pixel) at the current Web Mercator zoom level\n",
    "2. Comparing it to the `cell_size` metadata of each overview level\n",
    "3. Selecting the level whose `cell_size` is closest to the ground resolution\n",
    "\n",
    "This ensures you always see the optimal balance between image quality and loading performance for your current view."
   ],
   "id": "8b6a3cc99b24a86"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Create Interactive Map with Smart Overview Level Selection\n",
    "from ipyleaflet_multiscales import create_interactive_map\n",
    "\n",
    "# Get metadata from the base level (L0)\n",
    "metadata = overview_datasets[\"L0\"].b02.attrs\n",
    "\n",
    "# Define RGB band names (Sentinel-2: R=b04, G=b03, B=b02)\n",
    "band_names = {\"r\": \"b04\", \"g\": \"b03\", \"b\": \"b02\"}\n",
    "\n",
    "# Create the interactive map\n",
    "# - initial_level=4: Start with L4 (good balance between quality and performance)\n",
    "# - initial_zoom=10: Start zoomed to show the full area\n",
    "# - band_names: Specify which bands to use for RGB composite\n",
    "map_widget = create_interactive_map(\n",
    "    overview_datasets=overview_datasets,\n",
    "    multiscales=multiscales,\n",
    "    metadata=metadata,\n",
    "    initial_level=5,\n",
    "    initial_zoom=7,\n",
    "    band_names=band_names\n",
    ")\n",
    "\n",
    "# Display the map\n",
    "# The label now shows: Level | Pixel dimensions | Cell size | Zoom | Ground resolution\n",
    "display(map_widget)"
   ],
   "id": "dce9949abb0b7c5e",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**How to use the interactive map:**\n",
    "\n",
    "1. **Zoom in/out** using the +/- buttons, mouse wheel, or double-click - watch as the overview level automatically adjusts to match your zoom level\n",
    "2. **Pan the map** by clicking and dragging to explore different areas\n",
    "3. **Use the slider** to manually select a specific overview level if you want to compare quality\n",
    "4. **Monitor the label** which shows:\n",
    "   - **Level ID and dimensions**: e.g., \"L5 (343√ó343 px)\"\n",
    "   - **Downsampling factor**: e.g., \"32√ó downsampled\"\n",
    "   - **Cell size**: The spatial resolution in meters (e.g., \"320.0m\" means each pixel represents 320m on the ground)\n",
    "   - **Current zoom**: The Web Mercator zoom level (typically 1-18)\n",
    "   - **Ground resolution**: The actual pixel size at the current zoom level (e.g., \"84.5m/px\")\n",
    "\n",
    "Try zooming in and out to see how the system automatically switches between levels to maintain both visual quality and responsiveness!"
   ],
   "id": "16bc5c5a51acb40c"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>"
   ],
   "id": "3559e510127a4b77"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üí™ Now it is your turn"
   ],
   "id": "6b244d24e48ecfe1"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that you've learned how to visualize and interact with multiscale overviews, try these exercises to deepen your understanding:\n",
    "\n",
    "### Task 1: Experiment with different band combinations\n",
    "Instead of true color RGB (b04, b03, b02), try creating **false color composites** that highlight different features:\n",
    "- **Color infrared (CIR)**: `{\"r\": \"b08\", \"g\": \"b04\", \"b\": \"b03\"}` - highlights vegetation in red tones\n",
    "- **Agriculture composite**: `{\"r\": \"b08\", \"g\": \"b03\", \"b\": \"b02\"}` - emphasizes crop health and vigor\n",
    "- **Urban analysis**: `{\"r\": \"b08\", \"g\": \"b04\", \"b\": \"b02\"}` - distinguishes urban areas from vegetation\n",
    "\n",
    "Modify the `band_names` dictionary in the map creation code above and re-run the cell to see how different band combinations reveal different information about the landscape.\n",
    "\n",
    "### Task 2: Apply this workflow to a different area\n",
    "Go back to the previous tutorial ([65_create_overviews.ipynb](65_create_overviews.ipynb)) and create overviews for a different Sentinel-2 scene from your region of interest. Then return to this notebook and update the `local_zarr_path` variable to visualize your new dataset. Compare how the overview structure works for different landscapes (urban vs rural, mountainous vs flat, etc.).\n",
    "\n",
    "### Task 3: Analyze the performance tradeoff\n",
    "Measure how long it takes to load and display different overview levels. Add timing code like:\n",
    "```python\n",
    "import time\n",
    "start = time.time()\n",
    "visualize_level(0)  # L0 - full resolution\n",
    "print(f\"L0 took {time.time()-start:.2f} seconds\")\n",
    "\n",
    "start = time.time()\n",
    "visualize_level(5)  # L5 - much coarser\n",
    "print(f\"L5 took {time.time()-start:.2f} seconds\")\n",
    "```\n",
    "How much faster is L5 compared to L0? What about L7? At what level do you think the quality becomes too low for useful analysis?"
   ],
   "id": "693de018dbf8d4c5"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion"
   ],
   "id": "ab6e991cf6d2e36d"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this tutorial, we've learned how to **visualize and interact with multiscale overview pyramids** for GeoZarr datasets. We covered:\n",
    "\n",
    "1. **Understanding the hierarchy**: How to parse and inspect multiscales metadata following the GeoZarr specification\n",
    "2. **Loading levels dynamically**: How to iterate through the layout metadata and load different resolution levels\n",
    "3. **Creating RGB composites**: How to combine spectral bands with proper contrast enhancement and NaN handling\n",
    "4. **Interactive exploration**: How to build responsive widgets for exploring different zoom levels\n",
    "5. **Professional web mapping**: How to create maps with automatic level selection based on zoom and ground resolution\n",
    "\n",
    "**Key takeaways:**\n",
    "- Overviews enable **efficient multi-scale visualization** by providing progressively coarser representations of large datasets\n",
    "- **Automatic level selection** ensures optimal performance while maintaining visual quality appropriate for the current view\n",
    "- **Proper NaN handling** is critical - failing to filter NaN values before percentile calculations causes white/blank displays\n",
    "- The `cell_size` metadata enables **intelligent zoom-aware rendering** by matching overview resolution to ground resolution\n",
    "\n",
    "These techniques are essential for building **interactive Earth Observation applications** that remain responsive even when working with very large satellite imagery datasets."
   ],
   "id": "b057af427a75d7ab"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What's next?"
   ],
   "id": "e3a150490e968d18"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that you've mastered creating and visualizing GeoZarr overviews through tutorials 65 and 66, you can:\n",
    "\n",
    "- **Apply these techniques to your own datasets**: Use the complete workflow on your Earth Observation data from any source (Sentinel-2, Landsat, commercial satellites, etc.)\n",
    "- **Build custom web applications**: The `ipyleaflet_multiscales` module provides a foundation for developing interactive mapping tools tailored to your specific needs\n",
    "- **Optimize for your use case**: Test different chunk sizes, compression algorithms, and scale factors to find the best balance between file size and access performance\n",
    "- **Integrate with cloud platforms**: Host your overview-enhanced Zarr datasets on cloud object storage (S3, Azure Blob, Google Cloud Storage) for web-scale access\n",
    "\n",
    "**Further resources:**\n",
    "- [GeoZarr Specification](https://github.com/zarr-developers/geozarr-spec) - Full technical specification for GeoZarr extensions\n",
    "- [xarray documentation](https://docs.xarray.dev/) - Comprehensive guide to working with labeled multi-dimensional arrays\n"
   ],
   "id": "23a3235c49f586c0"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "eopf-101",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
