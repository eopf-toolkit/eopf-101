---
title: "Zarr Chunking Mastery for EOPF Datasets"
format: html
---

Chunking is the secret sauce that makes Zarr format incredibly efficient for Earth Observation data processing. **Understanding chunking transforms how you work with massive satellite datasets** — turning memory-crushing, slow operations into fast, scalable analysis workflows. This tutorial takes you from chunking basics to advanced optimization strategies specifically tailored for EOPF (Earth Observation Processing Framework) datasets.

## Understanding the fundamentals of Zarr chunking

Zarr revolutionizes how we store and access large multidimensional arrays by breaking them into smaller, manageable pieces called **chunks**. Think of chunks as rectangular tiles that together compose your complete dataset, but with a crucial difference: you can access and process these tiles independently.

### What chunks actually are

<div style="float: left; margin-right: 25px; margin-bottom: 25px;">
<img src="img/zarr_chunk_data.png" style="width: 200px; align: center;" alt="Zarr chunked data">
<figcaption>Zarr Chunked Data. Source [earthmover.io](https://earthmover.io/blog/what-is-zarr)</figcaption>
</div>

A chunk is Zarr's fundamental storage unit — an equally-sized block of array data that gets compressed and stored as a separate object. When you have a massive 10,000 × 10,000 pixel satellite image, Zarr might divide it into 100 chunks of 1,000 × 1,000 pixels each. Each chunk becomes a separate compressed file or object in your storage system.

<br/>
<br/>
<br/>
<br/>

```python
import zarr
import numpy as np

# Create a large array with chunking
z = zarr.zeros((10000, 10000), chunks=(1000, 1000), dtype='float32')
print(f"Array shape: {z.shape}")
print(f"Chunk shape: {z.chunks}")
print(f"Number of chunks: {z.nchunks}")  # 100 chunks total
```

The magic happens in how chunks are stored and accessed. Each chunk gets its own storage key (like `c/0/1` for the chunk at row 0, column 1 of the chunk grid) and is compressed individually using algorithms like Blosc or Zstandard. This design enables several powerful capabilities that traditional array storage can't match.

### Why chunks transform performance

Chunking addresses three critical performance bottlenecks in large-scale data processing:

**Memory efficiency**: Instead of loading entire datasets into RAM, you only load the chunks containing data you actually need. This lets you work with datasets much larger than your available memory — a 100 GB satellite time series can be processed on a machine with just 8 GB of RAM.

**I/O optimization**: Chunking minimizes data transfer by reading only relevant sections. When you need data from one geographic region, Zarr loads only those chunks covering that area. For cloud storage, this translates to fewer, more efficient HTTP range requests instead of downloading massive files.

**Parallel processing**: Different chunks can be processed simultaneously by multiple CPU cores or distributed workers. This transforms compute-intensive operations from sequential bottlenecks into scalable parallel workflows.

Research demonstrates that properly chunked Zarr stores can achieve **5-20 GB/s sustained read speeds** from cloud storage with appropriate parallelization — performance impossible with traditional monolithic file formats.

## How EOPF structures Earth Observation datasets

::: {.callout-tip}
If you have not already, please review the [Overview of EOPF Zarr Products](13_overview_eopf_datasets.qmd) to better understand the EOPF Dataset structure before diving deeper into chunking strategies.
:::

### What makes EO data unique for chunking

Earth Observation datasets exhibit characteristics that significantly influence optimal chunking strategies:

**Multi-dimensional complexity**: Satellite data combines spatial dimensions (often tens of thousands of pixels per side), spectral dimensions (multiple wavelength bands), and temporal dimensions (time series spanning years or decades). Each dimension has different access patterns and computational requirements.

**Scale characteristics**: Modern satellites generate massive data volumes. Sentinel-2 alone produces approximately 1.6 TB per orbit, with the full archive exceeding 25 petabytes and growing rapidly. Processing workflows must handle this scale efficiently.

**Access patterns**: Unlike general-purpose arrays, EO data has predictable access patterns. Spatial analysis typically accesses rectangular geographic regions. Spectral analysis needs multiple bands for the same locations. Time series analysis follows individual pixels or regions through time.

**Heterogeneous resolutions**: Many instruments capture data at multiple spatial resolutions simultaneously. Sentinel-2's 10m, 20m, and 60m bands require coordinated chunking strategies that balance storage efficiency with processing convenience.

## Mastering Sentinel-2's multi-resolution complexity

Sentinel-2's Multi-Spectral Instrument exemplifies the chunking challenges in modern Earth Observation missions. The instrument captures **13 spectral bands across three spatial resolutions**, each optimized for different applications but requiring coordinated access for comprehensive analysis.

| Sentinel-2 bands | Central wavelength (nm) | Spatial resolution (m) |
|-------------------|-------------------------|------------------------|
| B01 (Aerosol)     | 443                     | 60                     |
| B02 (Blue)        | 490                     | 10                     |
| B03 (Green)      | 560                     | 10                     |
| B04 (Red)        | 665                     | 10                     |
| B05 (Red Edge 1) | 705                     | 20                     |
| B06 (Red Edge 2) | 740                     | 20                     |
| B07 (Red Edge 3) | 783                     | 20                     |
| B08 (NIR)        | 842                     | 10                     |
| B8A (Narrow NIR) | 865                     | 20                     |
| B09 (Water Vapour)| 945                     | 60                     |
| B10 (Cirrus)     | 1375                    | 60                     |
| B11 (SWIR 1)     | 1610                    | 20                     |
| B12 (SWIR 2)     | 2190                    | 20                     |

### The multi-resolution band architecture

Sentinel-2's resolution design reflects careful engineering tradeoffs between information content, data volume, and detector technology constraints:

**10-meter resolution bands** (B02-Blue, B03-Green, B04-Red, B08-NIR) provide the core visible and near-infrared channels essential for land cover classification and RGB visualization. These broad spectral bands capture high light throughput, enabling fine spatial detail.

**20-meter resolution bands** include the red-edge bands (B05, B06, B07) critical for vegetation analysis, narrow NIR (B8A) for precise spectral measurements, and SWIR bands (B11, B12) for moisture and mineral content. These specialized channels sacrifice spatial resolution for spectral precision.

**60-meter resolution bands** serve atmospheric correction purposes. Coastal aerosol (B01) enables aerosol detection, water vapour (B09) supports atmospheric correction algorithms, and cirrus (B10) identifies high-altitude clouds that would otherwise contaminate surface observations.

### Optimal chunking strategies for multi-resolution data

The multi-resolution structure creates three distinct chunking approaches, each with specific advantages:

**Resolution-aligned chunking** maintains each band at its native resolution with coordinated chunk boundaries. This approach uses 1024×1024 pixel chunks for 10m bands, 512×512 pixel chunks for 20m bands, and 171×171 pixel chunks for 60m bands — all covering approximately the same 10.24 km ground area. This preserves maximum information content and avoids resampling artifacts but complicates multi-resolution analysis workflows.

**Unified resolution chunking** resamples all bands to a common resolution (typically 10m or 20m) and uses consistent spatial chunking throughout. While this simplifies data access patterns and analysis code, it increases storage requirements by 4× for upsampled bands and may introduce interpolation artifacts in the finest resolution data.

**Hierarchical pyramid chunking** stores data at multiple resolutions simultaneously, with different chunk sizes optimized for each scale. This approach excels for visualization applications that need efficient access across zoom levels but requires additional storage overhead and synchronization complexity.

Based on EOPF implementation experience and performance research, **1024×1024 pixel spatial chunks** emerge as the optimal balance for most Sentinel-2 applications. This chunk size provides good I/O efficiency for cloud storage (typically 10-100 MB per chunk after compression), enables effective parallelization, and aligns well with common analysis regions.

## Optimizing chunk sizes for maximum performance

Chunk size selection fundamentally determines your application's performance characteristics. The choice affects memory usage, I/O efficiency, parallel scaling, and network transfer costs — making it one of the most critical optimization decisions in Earth Observation data processing.

### The chunk size performance matrix

**Small chunks (1-10 MB)** excel at fine-grained access patterns where you frequently need small spatial regions or sparse data selections. They minimize memory usage per operation and enable high granularity for parallel processing. However, small chunks suffer from high metadata overhead, require numerous network requests for cloud storage, and create complex task graphs that can overwhelm schedulers.

**Medium chunks (10-100 MB)** provide the optimal balance for most Earth Observation applications. This size range works well with cloud storage byte-range request patterns, enables efficient compression ratios, and supports good parallelization without excessive overhead. Research consistently identifies this range as the performance sweet spot for geospatial analysis workflows.

**Large chunks (>100 MB)** maximize compression efficiency and minimize network request count, making them excellent for sequential access patterns or batch processing workflows. They can dramatically reduce metadata overhead for massive datasets. However, large chunks increase memory requirements, may transfer unnecessary data for sparse access patterns, and can limit parallel efficiency when processing power exceeds chunk count.

### Use case-specific optimization

Different Earth Observation applications have dramatically different optimal chunking strategies:

**Visualization and display applications** benefit from tile-aligned chunking that matches web mapping standards. Use 256×256 or 512×512 pixel chunks aligned with standard tile pyramid levels. This enables efficient zoom and pan operations by loading only visible tiles. Progressive loading with smaller chunks (1-10 MB) creates responsive user interfaces.

**Scientific analysis workflows** should align chunks with computational patterns. For time series analysis, use large spatial chunks but small temporal chunks to optimize pixel-by-pixel processing. For spatial analysis algorithms, reverse this pattern with large temporal chunks and smaller spatial chunks. Consider algorithm-specific requirements — FFT operations prefer power-of-2 dimensions, while machine learning training benefits from chunks containing complete training samples.

**Tiling and map service workflows** require careful alignment with tile boundaries and zoom levels. Web Mercator tiling works best with chunks that are multiples of 256×256 pixels. Use different chunk sizes for different zoom levels to optimize both overview generation and detail rendering. Consider request patterns — frequently accessed zoom levels may benefit from smaller chunks for cache efficiency.

### Memory and computational considerations

Effective chunk size selection requires understanding memory behavior beyond simple size calculations. Dask typically keeps **2-3× the number of active worker threads worth of chunks in memory simultaneously**. With 8 worker threads and 64 GB RAM, target chunk sizes around 1-2 GB to maintain adequate memory headroom.

Consider **computational overhead** relative to chunk processing time. Each chunk access involves ~1ms of scheduling overhead, so chunks should require significantly more computation time to maintain efficiency. For most Earth Observation algorithms, this translates to minimum 10-100ms processing per chunk, supporting the 10-100 MB chunk size recommendation.

**Compression interactions** significantly affect optimal chunk sizes. Larger chunks generally achieve better compression ratios — particularly important for spectral data with high spatial correlation. However, compressed chunks must be fully decompressed when accessed, potentially increasing memory usage beyond the nominal chunk size. Balance compression benefits with memory requirements for your specific workflows.

## Leveraging cloud-native access patterns

Cloud computing fundamentally changes how we think about data access and storage optimization. Traditional file-based approaches that worked well on local storage systems often perform poorly in cloud environments, where network latency and bandwidth characteristics dominate performance.

### How Zarr enables cloud-native processing

Zarr's architecture aligns perfectly with cloud storage systems through several key design principles:

**HTTP range request optimization**: Each chunk becomes a separate object or uses byte-range requests within larger objects. This enables accessing specific data portions without downloading entire files — crucial for cloud storage where data transfer costs can be substantial.

**Parallel access patterns**: Multiple chunks can be read simultaneously through concurrent HTTP requests, saturating available bandwidth and dramatically improving throughput. Properly configured Zarr stores routinely achieve 5-20 GB/s sustained read speeds from cloud object storage.

**Serverless compatibility**: Individual chunks can be processed by serverless functions (AWS Lambda, Google Cloud Functions), enabling cost-effective processing that scales automatically with workload demands. Each function processes one or more chunks independently, making workflows naturally fault-tolerant.

### Consolidated metadata performance benefits

Consolidated metadata represents one of Zarr's most important cloud optimization features. Without consolidation, opening a complex Zarr store requires separate HTTP requests to read metadata from every array and group — potentially hundreds of requests for typical EOPF datasets.

The **`.zmetadata` file** combines all structural metadata into a single JSON document at the store root. This reduces store opening time from multiple seconds to milliseconds, dramatically improving user experience and reducing cloud API request costs. For Earth Observation applications with complex hierarchical structures, consolidated metadata is essential rather than optional.

Modern tools like Xarray automatically detect missing consolidated metadata and display performance warnings. Always regenerate consolidated metadata after structural changes to your Zarr stores to maintain optimal performance.

### Object storage optimization strategies

Different cloud providers have distinct characteristics that influence optimal chunking strategies:

**AWS S3** performs best with chunk sizes in the 8-16 MB range for byte-range requests. Use S3 Transfer Acceleration for global access patterns and consider S3 Intelligent Tiering for datasets with varying access frequency. Minimize small object proliferation — thousands of tiny chunks can impact both performance and billing.

**Google Cloud Storage** and **Azure Blob Storage** follow similar principles but benefit from regional bucket placement to minimize latency. Leverage provider-specific features like Google's composite objects or Azure's CDN integration when appropriate for your access patterns.

**Multi-cloud strategies** benefit from Zarr's storage-agnostic design through fsspec backends. The same Zarr store can be accessed identically regardless of whether it's stored on S3, GCS, or Azure, simplifying workflow development and enabling cloud portability.

## Integrating with rio-tiler for efficient tiling

Rio-tiler provides the bridge between Zarr's chunked storage and web mapping applications, enabling dynamic tile generation from multidimensional Earth Observation datasets. The integration between Zarr chunking and rio-tiler's tiling engine creates opportunities for significant performance optimization.

### Tiling-optimized chunking strategies

Web mapping applications have specific access patterns that influence optimal chunking. **Tile requests** typically cover small geographic areas at specific zoom levels, creating predictable spatial access patterns. Align your chunks with common tile boundaries to minimize the number of chunks that must be read for each tile request.

For **multi-scale applications**, consider different chunk sizes for different zoom levels. High zoom levels (detailed views) benefit from smaller chunks aligned with tile boundaries. Low zoom levels (continental or global views) may perform better with larger chunks that reduce the mosaicking overhead when combining many chunks into overview tiles.

**Temporal tiling** adds another optimization dimension. For time-series datasets, organize chunks to optimize both spatial tiling and temporal access. Consider use cases — animated visualizations may benefit from temporal chunking, while static maps at specific dates benefit from spatial chunking.

### Performance optimization techniques

Rio-tiler performance depends heavily on chunk organization and access patterns:

**Chunk alignment** with tile grids minimizes partial chunk reads. When a tile request requires reading portions of many chunks, performance degrades significantly. Design your chunking strategy so that typical tile requests align with chunk boundaries.

**Caching strategies** become crucial for interactive applications. Cache generated tiles at the application level, but also consider caching frequently accessed chunks at the storage level. Cloud storage providers offer various caching options that can dramatically improve repeated access performance.

**Pyramid generation** creates pre-computed overviews at different resolutions, enabling efficient low-zoom-level tile generation. While this increases storage requirements, the performance benefits often justify the cost for frequently accessed datasets.

### Advanced tiling workflows

Modern Earth Observation applications often require sophisticated tiling capabilities beyond simple RGB visualization:

**Multi-band tiling** enables false-color composites, vegetation indices, and other derived products through dynamic band combinations. Design your chunking to co-locate bands frequently used together — RGB bands should be in the same chunks, vegetation index components should be co-located.

**Temporal compositing** creates cloud-free mosaics by combining multiple observations over time. This requires efficient access to the same spatial regions across multiple time steps, favoring spatial chunking strategies over temporal chunking for these applications.

## Understanding compression and network tradeoffs

Compression creates complex tradeoffs between storage efficiency, computational overhead, and network performance. In cloud environments, where both storage costs and data transfer costs matter, these tradeoffs become critical optimization factors.

### Compression algorithm selection

Different compression algorithms excel in different scenarios:

**Blosc with LZ4** provides excellent speed with moderate compression ratios, making it ideal for interactive applications where decompression speed matters more than maximum storage efficiency. LZ4 typically achieves 2-5× compression on satellite data with very fast decompression.

**Zstandard (Zstd)** offers exceptional balance between compression ratio and speed, making it the preferred choice for many Earth Observation applications. Zstd often achieves 3-8× compression on spectral data while maintaining reasonable decompression performance.

**Specialized algorithms** like JPEG 2000 provide excellent compression for certain data types but may not integrate well with general-purpose array processing workflows. Consider format compatibility when selecting compression approaches.

### Network transfer optimization

Compression effectiveness depends heavily on network characteristics:

**Bandwidth-limited environments** benefit tremendously from aggressive compression since decompression is typically faster than network transfer. In these cases, higher compression ratios directly translate to reduced analysis time.

**High-bandwidth, low-latency networks** may make compression counterproductive if decompression becomes the bottleneck. Profile your specific network environment to determine optimal compression levels.

**Cloud storage considerations** include both transfer costs and access speed. Compressed data reduces both storage costs and transfer times, but increases CPU usage. For most Earth Observation applications, compression provides net benefits.

### Chunk size and compression interactions

Larger chunks generally achieve better compression ratios because compression algorithms can exploit more redundancy across larger data blocks. This creates tension with other performance factors:

**Storage efficiency** favors larger chunks for better compression, but **access efficiency** favors smaller chunks for reduced data transfer. Profile your specific datasets to find the optimal balance.

**Compression memory overhead** can be substantial — compressed chunks must be fully decompressed when accessed, potentially requiring much more memory than the compressed size suggests. Account for this in memory planning.

## Best practices and optimization guidelines

Successful Zarr chunking for Earth Observation applications requires balancing multiple competing factors while understanding the specific characteristics of your data, access patterns, and computational environment.

### Fundamental optimization principles

Start with **proven defaults** and optimize based on measured performance. Use 100MB target chunk sizes for initial implementations, employ consolidated metadata, and enable compression with balanced algorithms like Zstd. These defaults work well for most Earth Observation applications and provide a solid foundation for further optimization.

**Measure actual performance** rather than relying on theoretical expectations. Use monitoring tools to track memory usage, I/O throughput, task duration, and parallel efficiency. The Dask dashboard provides excellent visualization of performance characteristics including task streams, memory usage patterns, and worker utilization.

**Align with access patterns** by designing chunks around how your applications actually use the data. Spatial analysis applications should use large spatial chunks. Time series analysis should favor temporal chunking. Visualization applications should align with tile boundaries and zoom levels.

### Implementation workflow

Follow a systematic approach to chunking optimization:

1. **Assess your data characteristics** — total size, dimensionality, access patterns, and storage environment
2. **Identify computational requirements** — available memory, processing power, and network bandwidth
3. **Start with conservative defaults** — 100MB chunks, consolidated metadata, moderate compression
4. **Implement monitoring** — track key performance metrics throughout your workflow
5. **Optimize iteratively** — adjust chunk sizes and strategies based on measured performance
6. **Validate improvements** — ensure optimizations actually improve real-world performance

### Common optimization mistakes

**Over-chunking** with too many small chunks creates scheduler overhead and poor parallel efficiency. Symptoms include excessive white space in task streams and slow computation startup. Solution: increase chunk sizes to reduce task graph complexity.

**Under-chunking** with too few large chunks causes memory exhaustion and poor parallelization. Watch for memory spilling indicators and idle workers. Solution: decrease chunk sizes to better utilize available parallelism.

**Ignoring storage alignment** creates poor I/O performance when Zarr chunks don't align with underlying storage chunk boundaries. Always ensure your chunk dimensions are multiples of storage format chunks.

**Frequent rechunking** operations are expensive and should be avoided through careful initial chunk selection. Plan your chunking strategy around your complete workflow rather than optimizing individual operations in isolation.

### Production deployment considerations

**Monitor resource utilization** continuously in production environments. Set up alerts for memory pressure, poor parallel efficiency, and excessive task duration. Earth Observation workloads can vary dramatically in resource requirements.

**Plan for data growth** — chunking strategies that work for pilot projects may not scale to production data volumes. Consider how your approach will perform with 10× or 100× more data.

**Consider operational requirements** including backup strategies, disaster recovery, and data migration capabilities. Chunking decisions affect these operational aspects significantly.

**Document your rationale** for specific chunking choices to enable future optimization efforts. Include information about data characteristics, access patterns, and performance measurements that influenced your decisions.

**Test failure scenarios** — how does your system behave when individual chunks are unavailable, when memory is exhausted, or when network connectivity is poor? Robust chunking strategies should degrade gracefully under failure conditions.

This comprehensive understanding of Zarr chunking principles, combined with specific knowledge of EOPF datasets and Earth Observation applications, enables you to design efficient, scalable data processing workflows that can handle the massive scale of modern satellite missions while maintaining excellent performance across diverse analysis requirements.

Here's a comprehensive list of all the sources referenced in creating this tutorial, organized by category for easier navigation:

## Further Reading and Sources

### Official EOPF Documentation
- [Overview of the EOPF Zarr format](https://eopf-toolkit.github.io/eopf-101/21_what_is_zarr.html) - EOPF Toolkit's comprehensive guide to Zarr implementation
- [EOPF Storage Format Specification](https://cpm.pages.eopf.copernicus.eu/eopf-cpm/main/PSFD/4-storage-formats.html) - Technical specification for EOPF data storage
- [EOPF Overview - CSC Data Processors Re-engineering](https://eopf.copernicus.eu/eopf/) - Official Copernicus EOPF program overview

### Zarr Format Documentation
- [Zarr Tutorial](https://zarr.readthedocs.io/en/v1.1.0/tutorial.html) - Official Zarr documentation and tutorials
- [Optimizing Performance - Zarr Documentation](https://zarr.readthedocs.io/en/latest/user-guide/performance.html) - Performance optimization guide
- [Zarr - Official Project Site](https://zarr.dev/) - Main Zarr project website
- [Zarr (data format) - Wikipedia](https://en.wikipedia.org/wiki/Zarr_(data_format)) - General overview and history

### Cloud-Native Geospatial Resources
- [Cloud-Optimized Geospatial Formats Guide - Zarr](https://guide.cloudnativegeo.org/zarr/intro.html) - Cloud Native Geo's comprehensive Zarr guide
- [Cloud Native Geospatial Formats Explained](https://forrest.nyc/cloud-native-geospatial-formats-geoparquet-zarr-cog-and-pmtiles-explained/) - Matt Forrest's overview of modern formats
- [Zarr Takes Cloud-Native Geospatial by Storm](https://earthmover.io/blog/zarr-takes-cloud-native-geospatial-by-storm) - Earthmover's analysis of Zarr adoption

### Chunking Strategy Guides
- [Chunking Strategies for LLM Applications](https://www.pinecone.io/learn/chunking-strategies/) - Pinecone's comprehensive chunking guide
- [Choosing Good Chunk Sizes in Dask](https://blog.dask.org/2021/11/02/choosing-dask-chunk-sizes) - Dask team's chunking recommendations
- [Optimization Practices - Chunking](https://esipfed.github.io/cloud-computing-cluster/optimization-practices.html) - ESIP Fed's chunking best practices
- [Dask Array Best Practices](https://docs.dask.org/en/latest/array-best-practices.html) - Official Dask chunking guidelines

### Sentinel-2 Mission Documentation
- [Copernicus: Sentinel-2 - eoPortal](https://www.eoportal.org/satellite-missions/copernicus-sentinel-2) - Comprehensive mission overview
- [Sentinel-2 Products](https://sentiwiki.copernicus.eu/web/s2-products) - Official product specifications
- [Sentinel-2 - Wikipedia](https://en.wikipedia.org/wiki/Sentinel-2) - Mission overview and technical details
- [ESA - Introducing Sentinel-2](https://www.esa.int/Applications/Observing_the_Earth/Copernicus/Sentinel-2/Introducing_Sentinel-2) - ESA's official introduction

### Research Papers and Technical Articles
- [Cloud-Performant NetCDF4/HDF5 Reading with Zarr](https://medium.com/pangeo/cloud-performant-reading-of-netcdf4-hdf5-data-using-the-zarr-library-1a95c5c92314) - Richard Signell's performance analysis
- [Federated and Reusable Processing of Earth Observation Data](https://www.nature.com/articles/s41597-025-04513-y) - Nature Scientific Data paper
- [Optimal Chunking Strategies for Cloud-based Storage](https://www.researchgate.net/publication/359832694_Optimal_Chunking_Strategies_for_Cloud-based_Storage_of_Geospatial_Data_Using_Zarr) - Research on geospatial chunking
- [Rechunker: The Missing Link for Chunked Array Analytics](https://medium.com/pangeo/rechunker-the-missing-link-for-chunked-array-analytics-5b2359e9dc11) - Ryan Abernathey on rechunking strategies

### Compression and Performance
- [To Compress or Not to Compress — A Zarr Question](https://medium.com/@lubonjaariel/to-compress-or-not-to-compress-a-zarr-question-812160b3777d) - Ariel Lubonja's compression analysis
- [HDF5 I/O Performance Guide](https://biorack.github.io/BASTet/HDF5_format_performance.html) - Berkeley's performance documentation
- [Compress Zarr Meteo Data for Cheap Blob Storage](https://the-fonz.gitlab.io/posts/compress-zarr-meteo/) - Practical compression strategies

### Additional Technical Resources
- [Zarr Encoding Specification - Xarray](https://docs.xarray.dev/en/stable/internals/zarr-encoding-spec.html) - Xarray's Zarr encoding details
- [Understanding Optimal Zarr Chunking for Climatology](https://discourse.pangeo.io/t/understanding-optimal-zarr-chunking-scheme-for-a-climatology/2335) - Pangeo community discussion
- [Best Practices for Using Tile Layers](https://www.esri.com/arcgis-blog/products/sharing-collaboration/sharing-collaboration/best-practices-for-using-tile-layers-as-operational-layers) - ESRI's tiling recommendations

These resources provide both theoretical foundations and practical implementation guidance for working with Zarr chunking in Earth Observation contexts. The official EOPF documentation offers the most authoritative information for EOPF-specific implementations, while the broader Zarr and cloud-native geospatial resources provide valuable context for optimization strategies and best practices.