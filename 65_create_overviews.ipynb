{
 "cells": [
  {
   "metadata": {},
   "cell_type": "raw",
   "source": [
    "---\n",
    "title: \"Zarr Overviews - Creation\"\n",
    "execute:\n",
    "  enabled: true\n",
    "format: html\n",
    "---"
   ],
   "id": "46388d096a513686"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "<a href=\"https://jupyterhub.user.eopf.eodc.eu/hub/user-redirect/git-pull?repo=https://github.com/eopf-toolkit/eopf-101&branch=main&urlpath=lab/tree/eopf-101/65_create_overviews.ipynb\" target=\"_blank\">\n",
    "  <button style=\"background-color:#0072ce; color:white; padding:0.6em 1.2em; font-size:1rem; border:none; border-radius:6px; margin-top:1em;\">\n",
    "    üöÄ Launch this notebook in JupyterLab\n",
    "  </button>\n",
    "</a>"
   ],
   "id": "f750d82b4f861cd9"
  },
  {
   "cell_type": "markdown",
   "id": "y4i6lu71mz",
   "source": [
    "### Introduction\n",
    "\n",
    "Large satellite products, such as Sentinel-2 Level-2A scenes, contain tens of millions of pixels per band. Accessing or visualising them at full resolution is often unnecessary, especially for exploratory analysis, map rendering, or quality checks. Multiscale pyramids address this by providing progressively coarser versions of the original data, allowing client applications to request only the level of detail required. This approach reduces computational load, improves user experience, and aligns with modern cloud-native geospatial workflows.\n",
    "\n",
    "In this notebook, we will demonstrate how to create overviews (also called multiscale pyramids) for large Earth Observation datasets stored in Zarr format. Overviews are downscaled representations of gridded data that support efficient visualisation and scalable access to high-resolution datasets. They enable fast inspection at multiple zoom levels, reduce data transfer volumes, and improve performance when working with cloud-optimised storage.\n",
    "\n",
    "### What we will learn\n",
    "\n",
    "- üóÇÔ∏è How to compute multiscale overview levels from high-resolution satellite data\n",
    "- üìä How to attach GeoZarr-compliant metadata to datasets\n",
    "- üíæ How to write overview pyramids to Zarr storage\n",
    "\n",
    "### Prerequisites\n",
    "\n",
    "This notebook uses:\n",
    "- **Dataset**: Sentinel-2 L2A reflectance data from EODC object storage (hosted by the STAC catalogue)\n",
    "- **Resolution**: 10m spatial resolution (10980 √ó 10980 pixels)\n",
    "- **Bands**: Blue (b02), Green (b03), Red (b04), NIR (b08)\n",
    "\n",
    "The workflow follows a **compute-then-write pattern** that separates in-memory computation from disk persistence, allowing validation before committing changes.\n",
    "\n",
    "---"
   ],
   "metadata": {}
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Import libraries",
   "id": "bb36f1a166d0570e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import os, warnings, json, s3fs, zarr, dask\n",
    "import xarray as xr\n",
    "from pathlib import Path\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "bucket = os.environ[\"BUCKET_NAME\"]\n",
    "access = os.environ[\"ACCESS_KEY\"]\n",
    "secret = os.environ[\"SECRET_KEY\"]\n",
    "bucket_endpoint = os.environ[\"BUCKET_ENDPOINT\"]\n",
    "# S3 filesystem\n",
    "fs = s3fs.S3FileSystem(\n",
    "    key=access,\n",
    "    secret=secret,\n",
    "    client_kwargs={\"endpoint_url\": bucket_endpoint},\n",
    ")\n"
   ],
   "id": "7ff019143e903276",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Copy Remote Dataset to Local Storage\n",
    "\n",
    "The starting point for our overviews starts with the ‚Äúlocal‚Äù access to the STAC zarr item we are interested in. We do this based on two main reasons:\n",
    "\n",
    "*\tThe remote dataset is read-only (object storage) and we need a writable local copy to add new groups (L1-L5)\n",
    "*\tThis preserves the complete original structure\n"
   ],
   "id": "ebe59a345eae9b36"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "To make sure that we use a convenient scene, we select a source URL from the STAC catalogue.",
   "id": "4c2a5639f1051a38"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "remote_url = (    \"https://objects.eodc.eu/e05ab01a9d56408d82ac32d69a5aae2a:202508-s02msil2a/31/products/cpm_v256/S2A_MSIL2A_20250831T135741_N0511_R010_T26WPD_20250831T185012.zarr\")\n",
    "# Target location inside the bucket\n",
    "base_store = fs.get_mapper(\n",
    "    f\"{os.environ['BUCKET_NAME']}/\"\n",
    "    \"S2A_MSIL2A_20250831T135741_N0511_R010_T26WPD_20250831T185012.zarr/\"\n",
    ")"
   ],
   "id": "ccefb8e7b5c23fd4",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "As a first step, we download the remote Zarr dataset and saves it as a Zarr copy ready on a S3 bucket for exploration.\n",
    "Chunking helps large datasets load faster especially when used in visualisation tools that read data in small spatial tiles."
   ],
   "id": "1e506db3f6fafa2c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "print(f\"Copying remote Zarr to {local_zarr_path}... (may take several minutes)\")\n",
    "#os.makedirs(\"output\", exist_ok=True)\n",
    "s2l2a_remote = xr.open_datatree(remote_url, engine=\"zarr\")\n",
    "# Rechunk all datasets in the datatree\n",
    "#for node in s2l2a_remote.subtree:\n",
    "#    if node.has_data:\n",
    "#        node.ds = node.ds.chunk({dim: 512 for dim in node.ds.dims})\n",
    "print(f\"Copying to s3://{target_store} ...\")\n",
    "s2l2a_remote.to_zarr(\n",
    "    store=base_store,\n",
    "    mode=\"w\",\n",
    "    consolidated=False,\n",
    "    zarr_version=2,\n",
    "    compute=True,\n",
    ")"
   ],
   "id": "dffb1e02b64a6fda",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Then, we can access the dataset from the S3 bucket and look inside the group that contains the 10-metre reflectance data to understand which variables, dimensions, and coordinates it contains.",
   "id": "35f140264dc5acba"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# --- Step 2: Load Local Dataset and Inspect Structure ---\n",
    "variable_group_path = \"measurements/reflectance/r10m\"\n",
    "r10m_store = fs.get_mapper(\n",
    "    f\"{os.environ['BUCKET_NAME']}/\"\n",
    "    \"S2A_MSIL2A_20250831T135741_N0511_R010_T26WPD_20250831T185012.zarr/\"\n",
    "    f\"{variable_group_path}\"\n",
    ")\n",
    "dataset = xr.open_dataset(r10m_store, engine=\"zarr\")\n",
    "dataset"
   ],
   "id": "f3f8d590451e70e7",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Compute Overviews (In-Memory)\n",
    "\n",
    "Now we compute the overview levels **in memory only** - no data is written to disk at this stage.\n",
    "\n",
    "We extract the reflectance group at 10m resolution and automatically discover variables and dimensions.\n",
    "\n",
    "**Key operations:**\n",
    "- Open the local Zarr dataset as a datatree\n",
    "- Extract the reflectance group and convert to xarray Dataset\n",
    "- Automatically discover all variables and dimensions\n",
    "- Identify spatial coordinate names (x, y)\n"
   ],
   "id": "4094ed73571bbe88"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "In this step, we identify the spatial dimensions and variables in the dataset and define the scale levels that will be used to generate lower-resolution overviews.",
   "id": "85347b2db02f31d3"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "scales = [2, 4, 8, 16, 32, 64, 128]  # Scale factors for each level\n",
    "variables = [var for var in dataset.data_vars]  # Discover variables\n",
    "spatial_dims = [dim for dim in dataset.dims]  # Discover dimensions\n",
    "x_dim = next((d for d in spatial_dims if d in ['x', 'X', 'lon', 'longitude']), 'x')  # Identify x dimension\n",
    "y_dim = next((d for d in spatial_dims if d in ['y', 'Y', 'lat', 'latitude']), 'y')  # Identify y dimension\n",
    "print(f\"Variables: {variables} | Dims: {spatial_dims} | Shape: {dataset[variables[0]].shape} | Using: x_dim='{x_dim}', y_dim='{y_dim}'\\n\")"
   ],
   "id": "937342193664e438",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Accessing such information allows us to generate a series of lower-resolution overview datasets directly in memory.\n",
    "\n",
    "For each scale factor, we use xarray‚Äôs `coarsen()` function to average groups of pixels along the spatial dimensions (x, y). Each coarsened version is stored under a level name like L1, L2, etc., representing progressively coarser spatial resolutions."
   ],
   "id": "13afa4288409e355"
  },
  {
   "metadata": {
    "collapsed": true
   },
   "cell_type": "code",
   "source": [
    "overviews = {}  # Generate in-memory overview datasets\n",
    "for i, factor in enumerate(scales):\n",
    "    level_id = f\"L{i+1}\"\n",
    "    coarsened = dataset.coarsen({x_dim: factor, y_dim: factor}, boundary=\"trim\").mean()\n",
    "    overviews[level_id] = coarsened[variables]\n",
    "\n",
    "print(f\"Created {len(overviews)} overview levels:\")\n",
    "for level_id, level_ds in overviews.items():\n",
    "    print(f\"  {level_id}: shape {level_ds[variables[0]].shape}, dims {dict(level_ds.dims)}\")\n",
    "print(\"\\nOverview datasets created successfully (in memory only, not written to disk)\")"
   ],
   "id": "f70fac2c26455380",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "hyrf8kgah4",
   "source": [
    "## Attach Multiscales Metadata\n",
    "\n",
    "Once the data has been processed, relating it with the GeoZarr-compliant metadata will enhance the application and self description:\n",
    "\n",
    "- **Version**: Schema version (\"1.0\")\n",
    "- **Resampling method**: How data was aggregated (\"average\")\n",
    "- **Variables**: Which bands have overviews\n",
    "- **Layout**: The complete hierarchy including L0 (base) and all derived levels\n",
    "\n",
    "The metadata is stored in `dataset.attrs[\"multiscales\"]` following the GeoZarr Overviews specification. This ensures interoperability with GeoZarr-aware tools and libraries."
   ],
   "metadata": {}
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Here we prepare the information needed to describe the overview hierarchy in the GeoZarr metadata. We set overview_path to indicate where the overview groups will be stored, record the `resampling_method(\"average\")` used to create them, and compute the base spatial resolutions (`x_res` and `y_res`) from the coordinate spacing.",
   "id": "3f008e44e7d0e0ec"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "overview_path = \"overviews\"  # Where overviews are written (\".\" for direct children)\n",
    "resampling_method = \"average\"\n",
    "x_res = abs(float(dataset['x'].values[1] - dataset['x'].values[0]))\n",
    "y_res = abs(float(dataset['y'].values[1] - dataset['y'].values[0]))"
   ],
   "id": "106d3b893c38728",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Now, we build the multiscales layout metadata that describes how all overview levels relate to the base dataset.\n",
    "\n",
    "The first entry (L0) represents the original data, including its spatial resolution (cell_size). Each subsequent level (L1, L2, ‚Ä¶) is added to the layout with information about its path, the level it was derived from, the scale factors applied, the resampling method, and its corresponding cell size.\n",
    "\n",
    "Finally, this complete structure is stored in `dataset.attrs[\"multiscales\"]` following the GeoZarr Overviews specification (draft). The printed JSON summary shows the final metadata layout that GeoZarr-aware tools can use to identify and navigate between resolution levels."
   ],
   "id": "75baa6a2bfa850e7"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "layout = [{\"id\": \"L0\", \"path\": \".\", \"cell_size\": [x_res, y_res]}]  # Base level (native data at current group)\n",
    "for i, factor in enumerate(scales):\n",
    "    level_id = f\"L{i+1}\"\n",
    "    level_path = level_id if overview_path == \".\" else f\"{overview_path}/{level_id}\"\n",
    "    # CHANGE: Calculate level cell_size and add to layout entry\n",
    "    level_cell_size = [x_res * factor, y_res * factor]\n",
    "    layout.append({\"id\": level_id, \"path\": level_path, \"derived_from\": \"L0\" if i == 0 else f\"L{i}\", \"factors\": [factor, factor], \"resampling_method\": resampling_method, \"cell_size\": level_cell_size})\n",
    "dataset.attrs[\"multiscales\"] = {\"version\": \"1.0\", \"resampling_method\": resampling_method, \"variables\": variables, \"layout\": layout}\n",
    "print(\"Metadata structure:\")\n",
    "print(json.dumps(dataset.attrs[\"multiscales\"], indent=2))\n",
    "\n"
   ],
   "id": "3e902e5cb2e4f9a7",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "\n",
    "new_attrs = dataset.attrs.copy()   # includes your multiscales metadata\n",
    "json_bytes = json.dumps(new_attrs).encode(\"utf-8\")\n",
    "\n",
    "# FSMap stores values as bytes\n",
    "r10m_store['.zattrs'] = json_bytes"
   ],
   "id": "2ae5957e4c699476",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "un2rac3flq",
   "source": [
    "### Write Overviews to Local Zarr Store\n",
    "\n",
    "The final step consists of adding the computed overviews to the Zarr store copy.\n",
    "\n",
    "**Overview path options:**\n",
    "- `overview_path=\".\"` - Write overviews as direct children (L1, L2, L3, ...)\n",
    "- `overview_path=\"overviews\"` - Write overviews in a subfolder (overviews/L1, overviews/L2, ...)\n",
    "\n",
    "**Write operations:**\n",
    "1. **Write L1-L5** - Add overview levels as subgroups\n",
    "2. **Add metadata** - Update group attributes with multiscales metadata\n",
    "\n",
    "**Key point:** Native data stays at the group level. The multiscales metadata uses `path: \".\"` for L0 to reference the existing native data without duplication.\n",
    "\n",
    "**Result with `overview_path=\".\"`:**\n",
    "```\n",
    "measurements/reflectance/r10m/\n",
    "‚îú‚îÄ‚îÄ b02, b03, b04, b08  # Native data (L0 via path=\".\")\n",
    "‚îú‚îÄ‚îÄ x, y                # Coordinates\n",
    "‚îú‚îÄ‚îÄ L1/                 # Overview levels (direct children)\n",
    "‚îú‚îÄ‚îÄ L2/\n",
    "‚îú‚îÄ‚îÄ L3/\n",
    "‚îú‚îÄ‚îÄ L4/\n",
    "‚îú‚îÄ‚îÄ L5/\n",
    "‚îî‚îÄ‚îÄ .zattrs             # multiscales metadata\n",
    "```\n",
    "\n",
    "**Alternative with `overview_path=\"overviews\"`:**\n",
    "```\n",
    "measurements/reflectance/r10m/\n",
    "‚îú‚îÄ‚îÄ b02, b03, b04, b08  # Native data (L0 via path=\".\")\n",
    "‚îú‚îÄ‚îÄ x, y                # Coordinates\n",
    "‚îú‚îÄ‚îÄ overviews/          # Overview levels in subfolder\n",
    "‚îÇ   ‚îú‚îÄ‚îÄ L1/\n",
    "‚îÇ   ‚îú‚îÄ‚îÄ L2/\n",
    "‚îÇ   ‚îú‚îÄ‚îÄ L3/\n",
    "‚îÇ   ‚îú‚îÄ‚îÄ L4/\n",
    "‚îÇ   ‚îî‚îÄ‚îÄ L5/\n",
    "‚îî‚îÄ‚îÄ .zattrs             # multiscales metadata\n",
    "```"
   ],
   "metadata": {}
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "print(f\"Adding overviews to {variable_group_path} | Variables: {variables} | Path: '{overview_path}'\\n\")\n",
    "\n",
    "# Create the overview group folder on S3\n",
    "zarr.open_group(store=base_overview_store, mode=\"a\", zarr_version=2)\n",
    "\n",
    "print(f\"Writing {len(overviews)} overview levels...\")\n",
    "for level_id, level_dataset in overviews.items():\n",
    "\n",
    "    level_store = fs.get_mapper(\n",
    "        f\"{os.environ['BUCKET_NAME']}/\"\n",
    "        \"S2A_MSIL2A_20250831T135741_N0511_R010_T26WPD_20250831T185012.zarr/\"\n",
    "        f\"{variable_group_path}/{overview_path}/{level_id}\"\n",
    "    )\n",
    "\n",
    "    level_dataset.to_zarr(\n",
    "        store=level_store,\n",
    "        mode=\"a\",\n",
    "        zarr_version=2,\n",
    "    )\n",
    "\n",
    "# Write coordinates + attrs into r10m group\n",
    "coords_only = xr.Dataset(coords=dataset.coords, attrs=dataset.attrs)\n",
    "coords_only.to_zarr(\n",
    "    store=r10m_store,\n",
    "    mode=\"a\",\n",
    "    zarr_version=2,\n",
    ")\n",
    "\n",
    "new_attrs = dataset.attrs.copy()\n",
    "new_attrs[\"multiscales\"] = multiscales\n",
    "\n",
    "with r10m_store.open('.zattrs', 'w') as f:\n",
    "    f.write(json.dumps(new_attrs))\n",
    "\n",
    "print(f\"Generating consolidated metadata for {overview_path}/\")\n",
    "zarr.consolidate_metadata(store=base_overview_store)\n",
    "\n",
    "print(f\"\\nSuccessfully added overviews to {variable_group_path}\\n\")\n",
    "print(f\"Final structure:\\n  {variable_group_path}/\")\n",
    "print(f\"    ‚îú‚îÄ‚îÄ {', '.join(variables)}\")\n",
    "print(f\"    ‚îú‚îÄ‚îÄ {x_dim}, {y_dim}\")\n",
    "print(f\"    ‚îú‚îÄ‚îÄ {overview_path}/\")\n",
    "for level_id in overviews.keys():\n",
    "    print(f\"    ‚îÇ   ‚îú‚îÄ‚îÄ {level_id}/\")\n",
    "print(f\"    ‚îî‚îÄ‚îÄ .zattrs\")\n"
   ],
   "id": "1c08c20946520675",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "\n",
    "## üí™ Now it is your turn\n",
    "\n",
    "With everything we have learnt so far, you are now able to create multiscale overviews for your own datasets.\n",
    "\n",
    "### Task 1: Experiment with Different Scale Factors\n",
    "\n",
    "Try modifying the `scales` list to create different pyramid structures. For example:\n",
    "- **Fewer levels**: `scales = [2, 4, 8]` for a smaller pyramid\n",
    "- **More aggressive downsampling**: `scales = [4, 16, 64]` for rapid zoom levels\n",
    "- **Fine-grained levels**: `scales = [2, 3, 4, 6, 8]` for smoother transitions\n",
    "\n",
    "### Task 2: Apply to Your Own Dataset\n",
    "\n",
    "Use this notebook as a template for your own Earth Observation data:\n",
    "1. Replace the URL with your own Zarr dataset\n",
    "2. Let the code discover variables and dimensions automatically\n",
    "3. Adjust scale factors based on your data resolution\n",
    "4. Validate and write the results\n",
    "\n",
    "---\n",
    "\n",
    "## Conclusion\n",
    "\n",
    "This tutorial demonstrated the complete workflow for creating GeoZarr-compliant multiscale overviews:\n",
    "\n",
    "1. ‚úÖ Load and discover dataset structure automatically\n",
    "2. ‚úÖ Compute overview levels in memory (no disk I/O)\n",
    "3. ‚úÖ Attach specification-compliant metadata\n",
    "4. ‚úÖ Write to Zarr storage\n",
    "\n",
    "**Key takeaways:**\n",
    "- The **compute-then-write pattern** separates computation from I/O\n",
    "- **Dynamic discovery** makes code adaptable to different datasets\n",
    "\n",
    "## What's next?\n",
    "\n",
    "In the next step, we will use ([66_use_overviews](./66_use_overviews.ipynb) to visualise the generated overviews with map libraries for progressive rendering."
   ],
   "id": "5a8e8d222f9856bb"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
