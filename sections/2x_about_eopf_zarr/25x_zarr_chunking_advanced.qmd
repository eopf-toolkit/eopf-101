---
title: "Mastering Sentinel-2's multi-resolution complexity"
format: html
---

In the [previous section](251_zarr_chunking_intro.qmd), we explored the fundamentals of Zarr chunking and its significance for Earth Observation data. Now, let's delve into the specific challenges posed by multi-resolution datasets, using Sentinel-2 as a case study.

## Mastering Sentinel-2's multi-resolution complexity

Sentinel-2's Multi-Spectral Instrument exemplifies the chunking challenges in modern Earth Observation missions. The instrument captures **13 spectral bands across three spatial resolutions**, each optimized for different applications but requiring coordinated access for comprehensive analysis.

| Sentinel-2 bands   | Central wavelength (nm) | Spatial resolution (m) |
| ------------------ | ----------------------- | ---------------------- |
| B01 (Aerosol)      | 443                     | 60                     |
| B02 (Blue)         | 490                     | 10                     |
| B03 (Green)        | 560                     | 10                     |
| B04 (Red)          | 665                     | 10                     |
| B05 (Red Edge 1)   | 705                     | 20                     |
| B06 (Red Edge 2)   | 740                     | 20                     |
| B07 (Red Edge 3)   | 783                     | 20                     |
| B08 (NIR)          | 842                     | 10                     |
| B8A (Narrow NIR)   | 865                     | 20                     |
| B09 (Water Vapour) | 945                     | 60                     |
| B10 (Cirrus)       | 1375                    | 60                     |
| B11 (SWIR 1)       | 1610                    | 20                     |
| B12 (SWIR 2)       | 2190                    | 20                     |

### The multi-resolution band architecture

Sentinel-2's resolution design reflects careful engineering tradeoffs between information content, data volume, and detector technology constraints:

**10-meter resolution bands** (B02-Blue, B03-Green, B04-Red, B08-NIR) provide the core visible and near-infrared channels essential for land cover classification and RGB visualization. These broad spectral bands capture high light throughput, enabling fine spatial detail.

**20-meter resolution bands** include the red-edge bands (B05, B06, B07) critical for vegetation analysis, narrow NIR (B8A) for precise spectral measurements, and SWIR bands (B11, B12) for moisture and mineral content. These specialized channels sacrifice spatial resolution for spectral precision.

**60-meter resolution bands** serve atmospheric correction purposes. Coastal aerosol (B01) enables aerosol detection, water vapour (B09) supports atmospheric correction algorithms, and cirrus (B10) identifies high-altitude clouds that would otherwise contaminate surface observations.

### Optimal chunking strategies for multi-resolution data

The multi-resolution structure creates three distinct chunking approaches, each with specific advantages:

**Resolution-aligned chunking** maintains each band at its native resolution with coordinated chunk boundaries. This approach uses 1024×1024 pixel chunks for 10m bands, 512×512 pixel chunks for 20m bands, and 171×171 pixel chunks for 60m bands — all covering approximately the same 10.24 km ground area. This preserves maximum information content and avoids resampling artifacts but complicates multi-resolution analysis workflows.

**Unified resolution chunking** resamples all bands to a common resolution (typically 10m or 20m) and uses consistent spatial chunking throughout. While this simplifies data access patterns and analysis code, it increases storage requirements by 4× for upsampled bands and may introduce interpolation artifacts in the finest resolution data.

**Hierarchical pyramid chunking** stores data at multiple resolutions simultaneously, with different chunk sizes optimized for each scale. This approach excels for visualization applications that need efficient access across zoom levels but requires additional storage overhead and synchronization complexity.

Based on EOPF implementation experience and performance research, **1024×1024 pixel spatial chunks** emerge as the optimal balance for most Sentinel-2 applications. This chunk size provides good I/O efficiency for cloud storage (typically 10-100 MB per chunk after compression), enables effective parallelization, and aligns well with common analysis regions.

## Optimizing chunk sizes for maximum performance

Chunk size selection fundamentally determines your application's performance characteristics. The choice affects memory usage, I/O efficiency, parallel scaling, and network transfer costs — making it one of the most critical optimization decisions in Earth Observation data processing.

### The chunk size performance matrix

**Small chunks (1-10 MB)** excel at fine-grained access patterns where you frequently need small spatial regions or sparse data selections. They minimize memory usage per operation and enable high granularity for parallel processing. However, small chunks suffer from high metadata overhead, require numerous network requests for cloud storage, and create complex task graphs that can overwhelm schedulers.

**Medium chunks (10-100 MB)** provide the optimal balance for most Earth Observation applications. This size range works well with cloud storage byte-range request patterns, enables efficient compression ratios, and supports good parallelization without excessive overhead. Research consistently identifies this range as the performance sweet spot for geospatial analysis workflows.

**Large chunks (>100 MB)** maximize compression efficiency and minimize network request count, making them excellent for sequential access patterns or batch processing workflows. They can dramatically reduce metadata overhead for massive datasets. However, large chunks increase memory requirements, may transfer unnecessary data for sparse access patterns, and can limit parallel efficiency when processing power exceeds chunk count.

### Use case-specific optimization

Different Earth Observation applications have dramatically different optimal chunking strategies:

**Visualization and display applications** benefit from tile-aligned chunking that matches web mapping standards. Use 256×256 or 512×512 pixel chunks aligned with standard tile pyramid levels. This enables efficient zoom and pan operations by loading only visible tiles. Progressive loading with smaller chunks (1-10 MB) creates responsive user interfaces.

**Scientific analysis workflows** should align chunks with computational patterns. For time series analysis, use large spatial chunks but small temporal chunks to optimize pixel-by-pixel processing. For spatial analysis algorithms, reverse this pattern with large temporal chunks and smaller spatial chunks. Consider algorithm-specific requirements — FFT operations prefer power-of-2 dimensions, while machine learning training benefits from chunks containing complete training samples.

**Tiling and map service workflows** require careful alignment with tile boundaries and zoom levels. Web Mercator tiling works best with chunks that are multiples of 256×256 pixels. Use different chunk sizes for different zoom levels to optimize both overview generation and detail rendering. Consider request patterns — frequently accessed zoom levels may benefit from smaller chunks for cache efficiency.

### Memory and computational considerations

Effective chunk size selection requires understanding memory behavior beyond simple size calculations. Dask typically keeps **2-3× the number of active worker threads worth of chunks in memory simultaneously**. With 8 worker threads and 64 GB RAM, target chunk sizes around 1-2 GB to maintain adequate memory headroom.

Consider **computational overhead** relative to chunk processing time. Each chunk access involves ~1ms of scheduling overhead, so chunks should require significantly more computation time to maintain efficiency. For most Earth Observation algorithms, this translates to minimum 10-100ms processing per chunk, supporting the 10-100 MB chunk size recommendation.

**Compression interactions** significantly affect optimal chunk sizes. Larger chunks generally achieve better compression ratios — particularly important for spectral data with high spatial correlation. However, compressed chunks must be fully decompressed when accessed, potentially increasing memory usage beyond the nominal chunk size. Balance compression benefits with memory requirements for your specific workflows.



## Understanding compression and network tradeoffs

Compression creates complex tradeoffs between storage efficiency, computational overhead, and network performance. In cloud environments, where both storage costs and data transfer costs matter, these tradeoffs become critical optimization factors.

### Compression algorithm selection

Different compression algorithms excel in different scenarios:

**Blosc with LZ4** provides excellent speed with moderate compression ratios, making it ideal for interactive applications where decompression speed matters more than maximum storage efficiency. LZ4 typically achieves 2-5× compression on satellite data with very fast decompression.

**Zstandard (Zstd)** offers exceptional balance between compression ratio and speed, making it the preferred choice for many Earth Observation applications. Zstd often achieves 3-8× compression on spectral data while maintaining reasonable decompression performance.

**Specialized algorithms** like JPEG 2000 provide excellent compression for certain data types but may not integrate well with general-purpose array processing workflows. Consider format compatibility when selecting compression approaches.

### Network transfer optimization

Compression effectiveness depends heavily on network characteristics:

**Bandwidth-limited environments** benefit tremendously from aggressive compression since decompression is typically faster than network transfer. In these cases, higher compression ratios directly translate to reduced analysis time.

**High-bandwidth, low-latency networks** may make compression counterproductive if decompression becomes the bottleneck. Profile your specific network environment to determine optimal compression levels.

**Cloud storage considerations** include both transfer costs and access speed. Compressed data reduces both storage costs and transfer times, but increases CPU usage. For most Earth Observation applications, compression provides net benefits.

### Chunk size and compression interactions

Larger chunks generally achieve better compression ratios because compression algorithms can exploit more redundancy across larger data blocks. This creates tension with other performance factors:

**Storage efficiency** favors larger chunks for better compression, but **access efficiency** favors smaller chunks for reduced data transfer. Profile your specific datasets to find the optimal balance.

**Compression memory overhead** can be substantial — compressed chunks must be fully decompressed when accessed, potentially requiring much more memory than the compressed size suggests. Account for this in memory planning.

## Best practices and optimization guidelines

Successful Zarr chunking for Earth Observation applications requires balancing multiple competing factors while understanding the specific characteristics of your data, access patterns, and computational environment.

### Fundamental optimization principles

Start with **proven defaults** and optimize based on measured performance. Use 100MB target chunk sizes for initial implementations, employ consolidated metadata, and enable compression with balanced algorithms like Zstd. These defaults work well for most Earth Observation applications and provide a solid foundation for further optimization.

**Measure actual performance** rather than relying on theoretical expectations. Use monitoring tools to track memory usage, I/O throughput, task duration, and parallel efficiency. The Dask dashboard provides excellent visualization of performance characteristics including task streams, memory usage patterns, and worker utilization.

**Align with access patterns** by designing chunks around how your applications actually use the data. Spatial analysis applications should use large spatial chunks. Time series analysis should favor temporal chunking. Visualization applications should align with tile boundaries and zoom levels.

### Implementation workflow

Follow a systematic approach to chunking optimization:

1. **Assess your data characteristics** — total size, dimensionality, access patterns, and storage environment
2. **Identify computational requirements** — available memory, processing power, and network bandwidth
3. **Start with conservative defaults** — 100MB chunks, consolidated metadata, moderate compression
4. **Implement monitoring** — track key performance metrics throughout your workflow
5. **Optimize iteratively** — adjust chunk sizes and strategies based on measured performance
6. **Validate improvements** — ensure optimizations actually improve real-world performance

### Common optimization mistakes

**Over-chunking** with too many small chunks creates scheduler overhead and poor parallel efficiency. Symptoms include excessive white space in task streams and slow computation startup. Solution: increase chunk sizes to reduce task graph complexity.

**Under-chunking** with too few large chunks causes memory exhaustion and poor parallelization. Watch for memory spilling indicators and idle workers. Solution: decrease chunk sizes to better utilize available parallelism.

**Ignoring storage alignment** creates poor I/O performance when Zarr chunks don't align with underlying storage chunk boundaries. Always ensure your chunk dimensions are multiples of storage format chunks.

**Frequent rechunking** operations are expensive and should be avoided through careful initial chunk selection. Plan your chunking strategy around your complete workflow rather than optimizing individual operations in isolation.



## Further Reading and Sources

### Official EOPF Documentation
- [Overview of the EOPF Zarr format](https://eopf-toolkit.github.io/eopf-101/21_what_is_zarr.html) - EOPF Toolkit's comprehensive guide to Zarr implementation
- [EOPF Storage Format Specification](https://cpm.pages.eopf.copernicus.eu/eopf-cpm/main/PSFD/4-storage-formats.html) - Technical specification for EOPF data storage
- [EOPF Overview - CSC Data Processors Re-engineering](https://eopf.copernicus.eu/eopf/) - Official Copernicus EOPF program overview

### Zarr Format Documentation
- [Zarr Tutorial](https://zarr.readthedocs.io/en/v1.1.0/tutorial.html) - Official Zarr documentation and tutorials
- [Optimizing Performance - Zarr Documentation](https://zarr.readthedocs.io/en/latest/user-guide/performance.html) - Performance optimization guide
- [Zarr - Official Project Site](https://zarr.dev/) - Main Zarr project website
- [Zarr (data format) - Wikipedia](https://en.wikipedia.org/wiki/Zarr_(data_format)) - General overview and history

### Cloud-Native Geospatial Resources
- [Cloud-Optimized Geospatial Formats Guide - Zarr](https://guide.cloudnativegeo.org/zarr/intro.html) - Cloud Native Geo's comprehensive Zarr guide
- [Cloud Native Geospatial Formats Explained](https://forrest.nyc/cloud-native-geospatial-formats-geoparquet-zarr-cog-and-pmtiles-explained/) - Matt Forrest's overview of modern formats
- [Zarr Takes Cloud-Native Geospatial by Storm](https://earthmover.io/blog/zarr-takes-cloud-native-geospatial-by-storm) - Earthmover's analysis of Zarr adoption

### Chunking Strategy Guides
- [Chunking Strategies for LLM Applications](https://www.pinecone.io/learn/chunking-strategies/) - Pinecone's comprehensive chunking guide
- [Choosing Good Chunk Sizes in Dask](https://blog.dask.org/2021/11/02/choosing-dask-chunk-sizes) - Dask team's chunking recommendations
- [Optimization Practices - Chunking](https://esipfed.github.io/cloud-computing-cluster/optimization-practices.html) - ESIP Fed's chunking best practices
- [Dask Array Best Practices](https://docs.dask.org/en/latest/array-best-practices.html) - Official Dask chunking guidelines

### Sentinel-2 Mission Documentation
- [Copernicus: Sentinel-2 - eoPortal](https://www.eoportal.org/satellite-missions/copernicus-sentinel-2) - Comprehensive mission overview
- [Sentinel-2 Products](https://sentiwiki.copernicus.eu/web/s2-products) - Official product specifications
- [Sentinel-2 - Wikipedia](https://en.wikipedia.org/wiki/Sentinel-2) - Mission overview and technical details
- [ESA - Introducing Sentinel-2](https://www.esa.int/Applications/Observing_the_Earth/Copernicus/Sentinel-2/Introducing_Sentinel-2) - ESA's official introduction

### Research Papers and Technical Articles
- [Cloud-Performant NetCDF4/HDF5 Reading with Zarr](https://medium.com/pangeo/cloud-performant-reading-of-netcdf4-hdf5-data-using-the-zarr-library-1a95c5c92314) - Richard Signell's performance analysis
- [Federated and Reusable Processing of Earth Observation Data](https://www.nature.com/articles/s41597-025-04513-y) - Nature Scientific Data paper
- [Optimal Chunking Strategies for Cloud-based Storage](https://www.researchgate.net/publication/359832694_Optimal_Chunking_Strategies_for_Cloud-based_Storage_of_Geospatial_Data_Using_Zarr) - Research on geospatial chunking
- [Rechunker: The Missing Link for Chunked Array Analytics](https://medium.com/pangeo/rechunker-the-missing-link-for-chunked-array-analytics-5b2359e9dc11) - Ryan Abernathey on rechunking strategies

### Compression and Performance
- [To Compress or Not to Compress — A Zarr Question](https://medium.com/@lubonjaariel/to-compress-or-not-to-compress-a-zarr-question-812160b3777d) - Ariel Lubonja's compression analysis
- [HDF5 I/O Performance Guide](https://biorack.github.io/BASTet/HDF5_format_performance.html) - Berkeley's performance documentation
- [Compress Zarr Meteo Data for Cheap Blob Storage](https://the-fonz.gitlab.io/posts/compress-zarr-meteo/) - Practical compression strategies

### Additional Technical Resources
- [Zarr Encoding Specification - Xarray](https://docs.xarray.dev/en/stable/internals/zarr-encoding-spec.html) - Xarray's Zarr encoding details
- [Understanding Optimal Zarr Chunking for Climatology](https://discourse.pangeo.io/t/understanding-optimal-zarr-chunking-scheme-for-a-climatology/2335) - Pangeo community discussion
- [Best Practices for Using Tile Layers](https://www.esri.com/arcgis-blog/products/sharing-collaboration/sharing-collaboration/best-practices-for-using-tile-layers-as-operational-layers) - ESRI's tiling recommendations

These resources provide both theoretical foundations and practical implementation guidance for working with Zarr chunking in Earth Observation contexts. The official EOPF documentation offers the most authoritative information for EOPF-specific implementations, while the broader Zarr and cloud-native geospatial resources provide valuable context for optimization strategies and best practices.
