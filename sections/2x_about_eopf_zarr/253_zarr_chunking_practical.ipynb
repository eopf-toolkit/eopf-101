{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "title: \"Creating and Optimizing Multi-temporal EOPF Zarr Datasets\"\n",
    "format:\n",
    "  html:\n",
    "    code-fold: false\n",
    "jupyter: python3\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "In this notebook, we will create a multi-temporal EOPF Zarr dataset from multiple Sentinel-2 acquisitions, focusing on the 10-meter resolution bands. We will explore different chunking strategies and demonstrate their impact on storage efficiency and access performance. This hands-on approach will help you understand how to optimize Zarr chunking for your specific Earth Observation workflows."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What we will learn\n",
    "\n",
    "- üõ∞Ô∏è How to create a reduced EOPF Zarr dataset from multiple Sentinel-2 acquisitions\n",
    "- üìä How to implement different chunking strategies for multi-temporal data\n",
    "- ‚ö° How to measure and compare performance metrics for different chunk sizes\n",
    "- üîß How to optimize chunking for specific access patterns (spatial vs temporal)\n",
    "- üíæ How to evaluate storage efficiency with different compression settings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prerequisites\n",
    "\n",
    "This notebook builds upon the concepts introduced in the [Zarr Chunking Introduction](251_zarr_chunking_intro.qmd). You should be familiar with:\n",
    "- Basic Zarr concepts and structure\n",
    "- STAC catalog navigation\n",
    "- Xarray operations\n",
    "\n",
    "::: {.callout-note}\n",
    "**Note:** This notebook uses utility functions from `zarr_chunking_utils.py` to keep the code focused on the key concepts. You can explore the utility functions to understand the implementation details.\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Libraries imported successfully!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import xarray as xr\n",
    "import zarr\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Import our utility functions\n",
    "from zarr_chunking_utils import (\n",
    "    create_dask_client,\n",
    "    get_sentinel2_data,\n",
    "    create_sample_data,\n",
    "    create_multitemporal_zarr,\n",
    "    create_multitemporal_zarr_from_eopf,\n",
    "    analyze_zarr_performance,\n",
    "    compare_chunking_strategies,\n",
    "    visualize_chunk_layout\n",
    ")\n",
    "\n",
    "# Set up plotting style\n",
    "sns.set_theme(style=\"whitegrid\")\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "\n",
    "print(\"Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setting up the environment\n",
    "\n",
    "First, we will initialize our Dask client for parallel processing and define our area of interest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dask dashboard available at: http://127.0.0.1:42591/status\n",
      "\n",
      "‚úì Dask client initialized with 4 workers\n"
     ]
    }
   ],
   "source": [
    "# Initialize Dask client for parallel processing\n",
    "client = create_dask_client(n_workers=4, threads_per_worker=2, memory_limit='4GB')\n",
    "print(f\"\\n‚úì Dask client initialized with {len(client.nthreads())} workers\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Area of Interest: [5.0, 52.0, 5.5, 52.5]\n",
      "Time period: 2024-06-01 to 2024-09-30\n"
     ]
    }
   ],
   "source": [
    "# Define area of interest and time period\n",
    "# Example: Agricultural area in Netherlands\n",
    "bbox = [5.0, 52.0, 5.5, 52.5]  # [min_lon, min_lat, max_lon, max_lat]\n",
    "start_date = \"2024-06-01\"\n",
    "end_date = \"2024-09-30\"\n",
    "\n",
    "print(f\"Area of Interest: {bbox}\")\n",
    "print(f\"Time period: {start_date} to {end_date}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Retrieving Sentinel-2 data from EOPF STAC\n",
    "\n",
    "We will first attempt to retrieve real Sentinel-2 data from the EOPF STAC catalog. If the connection is not available, we will fall back to sample data for demonstration purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Searching for Sentinel-2 data from EOPF STAC Catalog...\n",
      "  Area: [5.0, 52.0, 5.5, 52.5]\n",
      "  Period: 2024-06-01 to 2024-09-30\n",
      "  Max cloud cover: 20%\n",
      "Found 0 Sentinel-2 acquisitions with cloud storage URLs\n",
      "\n",
      "‚úì Found EOPF data:\n"
     ]
    }
   ],
   "source": [
    "# Try to retrieve real data from EOPF STAC\n",
    "items = get_sentinel2_data(\n",
    "    bbox=bbox,\n",
    "    start_date=start_date,\n",
    "    end_date=end_date,\n",
    "    max_items=20,  # Get 5 acquisitions\n",
    "    cloud_cover=20\n",
    ")\n",
    "\n",
    "# Display acquisition information\n",
    "print(\"\\n‚úì Found EOPF data:\")\n",
    "for item in items:\n",
    "    date = pd.to_datetime(item['datetime']).strftime('%Y-%m-%d')\n",
    "    print(f\"  - {item['id']}: {date}\")\n",
    "use_real_data = True\n",
    "\n",
    "# Define the 10m bands we want to work with\n",
    "bands_10m = ['B02', 'B03', 'B04', 'B08']  # Blue, Green, Red, NIR\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Defining chunking strategies\n",
    "\n",
    "We will test three different chunking strategies, each optimized for different access patterns:\n",
    "\n",
    "1. **Spatial-optimized**: Large spatial chunks (1024√ó1024), single time steps\n",
    "2. **Temporal-optimized**: Small spatial chunks (256√ó256), all time steps together\n",
    "3. **Balanced**: Medium chunks (512√ó512), 2 time steps per chunk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define chunking strategies\n",
    "chunking_strategies = {\n",
    "    'spatial_optimized': {'time': 1, 'y': 1024, 'x': 1024},\n",
    "    'temporal_optimized': {'time': 5, 'y': 256, 'x': 256},\n",
    "    'balanced': {'time': 2, 'y': 512, 'x': 512}\n",
    "}\n",
    "\n",
    "# Display chunking strategies\n",
    "print(\"Chunking Strategies:\")\n",
    "print(\"=\" * 50)\n",
    "for name, config in chunking_strategies.items():\n",
    "    chunk_size_mb = (config['time'] * config['y'] * config['x'] * 2 * 4) / (1024**2)  # 2 bytes per pixel, 4 bands\n",
    "    print(f\"\\n{name}:\")\n",
    "    print(f\"  Time chunks: {config['time']} steps\")\n",
    "    print(f\"  Spatial chunks: {config['y']} √ó {config['x']} pixels\")\n",
    "    print(f\"  Estimated chunk size: ~{chunk_size_mb:.1f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing chunk layouts\n",
    "\n",
    "Let us visualize how each strategy divides the data array:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize spatial-optimized chunking\n",
    "print(\"Spatial-Optimized Chunking:\")\n",
    "visualize_chunk_layout(\n",
    "    chunking_strategies['spatial_optimized'],\n",
    "    {'time': 5, 'y': 2048, 'x': 2048}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize temporal-optimized chunking\n",
    "print(\"Temporal-Optimized Chunking:\")\n",
    "visualize_chunk_layout(\n",
    "    chunking_strategies['temporal_optimized'],\n",
    "    {'time': 5, 'y': 2048, 'x': 2048}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Creating Zarr datasets with different strategies\n",
    "\n",
    "Now we will save our sample dataset using each chunking strategy and compare their characteristics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create output directory\n",
    "output_dir = Path('zarr_chunking_examples')\n",
    "output_dir.mkdir(exist_ok=True)\n",
    "\n",
    "# Compare all strategies\n",
    "print(\"Creating Zarr datasets with different chunking strategies...\\n\")\n",
    "\n",
    "# Process each strategy with real EOPF data\n",
    "results = {}\n",
    "for strategy_name, chunk_config in chunking_strategies.items():\n",
    "    print(f\"\\nProcessing {strategy_name} strategy...\")\n",
    "    output_path = output_dir / f'sentinel2_{strategy_name}.zarr'\n",
    "    \n",
    "    # Remove existing if present\n",
    "    if output_path.exists():\n",
    "        import shutil\n",
    "        shutil.rmtree(output_path)\n",
    "    \n",
    "    # Create Zarr from EOPF data\n",
    "    create_multitemporal_zarr_from_eopf(\n",
    "        items=items,\n",
    "        output_path=str(output_path),\n",
    "        chunk_strategy=chunk_config,\n",
    "        bands_10m=['b02', 'b03', 'b04', 'b08'],\n",
    "        use_sample_data=not use_real_data\n",
    "    )\n",
    "    \n",
    "    # Analyze performance\n",
    "    metrics = analyze_zarr_performance(str(output_path))\n",
    "    results[strategy_name] = metrics\n",
    "\n",
    "# Create comparison dataframe\n",
    "comparison_data = []\n",
    "for strategy, metrics in results.items():\n",
    "    comparison_data.append({\n",
    "        'Strategy': strategy,\n",
    "        'Total Size (MB)': round(metrics['total_size_mb'], 2),\n",
    "        'Avg Chunk Size (MB)': round(metrics['avg_chunk_size_mb'], 2),\n",
    "        'Compression Ratio': round(metrics['avg_compression_ratio'], 2),\n",
    "        'Number of Chunks': metrics['num_chunks'],\n",
    "        'Spatial Read (s)': round(metrics.get('spatial_read_time_s', 0), 3),\n",
    "        'Temporal Read (s)': round(metrics.get('temporal_read_time_s', 0), 3)\n",
    "    })\n",
    "comparison_df = pd.DataFrame(comparison_data)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"Performance Comparison Results:\")\n",
    "print(\"=\" * 80)\n",
    "print(comparison_df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Analyzing performance results\n",
    "\n",
    "Let us visualize the performance characteristics of each chunking strategy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create performance visualization\n",
    "fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
    "\n",
    "# Plot 1: Storage size comparison\n",
    "ax = axes[0, 0]\n",
    "comparison_df.plot(x='Strategy', y='Total Size (MB)', kind='bar', ax=ax, color='steelblue', legend=False)\n",
    "ax.set_title('Total Storage Size')\n",
    "ax.set_ylabel('Size (MB)')\n",
    "ax.set_xlabel('')\n",
    "ax.tick_params(axis='x', rotation=45)\n",
    "\n",
    "# Plot 2: Average chunk size\n",
    "ax = axes[0, 1]\n",
    "comparison_df.plot(x='Strategy', y='Avg Chunk Size (MB)', kind='bar', ax=ax, color='coral', legend=False)\n",
    "ax.set_title('Average Chunk Size')\n",
    "ax.set_ylabel('Size (MB)')\n",
    "ax.set_xlabel('')\n",
    "ax.tick_params(axis='x', rotation=45)\n",
    "\n",
    "# Plot 3: Compression ratio\n",
    "ax = axes[0, 2]\n",
    "comparison_df.plot(x='Strategy', y='Compression Ratio', kind='bar', ax=ax, color='green', legend=False)\n",
    "ax.set_title('Compression Ratio')\n",
    "ax.set_ylabel('Ratio')\n",
    "ax.set_xlabel('')\n",
    "ax.tick_params(axis='x', rotation=45)\n",
    "\n",
    "# Plot 4: Number of chunks\n",
    "ax = axes[1, 0]\n",
    "comparison_df.plot(x='Strategy', y='Number of Chunks', kind='bar', ax=ax, color='purple', legend=False)\n",
    "ax.set_title('Total Number of Chunks')\n",
    "ax.set_ylabel('Count')\n",
    "ax.set_xlabel('Strategy')\n",
    "ax.tick_params(axis='x', rotation=45)\n",
    "\n",
    "# Plot 5: Read performance comparison\n",
    "ax = axes[1, 1]\n",
    "x = np.arange(len(comparison_df))\n",
    "width = 0.35\n",
    "ax.bar(x - width/2, comparison_df['Spatial Read (s)'], width, label='Spatial Read', color='#2E86AB')\n",
    "ax.bar(x + width/2, comparison_df['Temporal Read (s)'], width, label='Temporal Read', color='#A23B72')\n",
    "ax.set_xlabel('Strategy')\n",
    "ax.set_ylabel('Time (seconds)')\n",
    "ax.set_title('Read Performance Comparison')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(comparison_df['Strategy'], rotation=45)\n",
    "ax.legend()\n",
    "\n",
    "# Plot 6: Efficiency score (custom metric)\n",
    "ax = axes[1, 2]\n",
    "# Calculate efficiency score: lower is better\n",
    "# Combines storage size, read times, and chunk overhead\n",
    "efficiency = (\n",
    "    comparison_df['Total Size (MB)'] / comparison_df['Total Size (MB)'].min() +\n",
    "    comparison_df['Spatial Read (s)'] / comparison_df['Spatial Read (s)'].min() +\n",
    "    comparison_df['Temporal Read (s)'] / comparison_df['Temporal Read (s)'].min()\n",
    ") / 3\n",
    "\n",
    "ax.bar(comparison_df['Strategy'], efficiency, color='gold')\n",
    "ax.set_title('Overall Efficiency Score\\n(Lower is Better)')\n",
    "ax.set_ylabel('Score')\n",
    "ax.set_xlabel('Strategy')\n",
    "ax.tick_params(axis='x', rotation=45)\n",
    "\n",
    "plt.suptitle('Zarr Chunking Strategy Performance Analysis', fontsize=16, y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Testing different compression levels\n",
    "\n",
    "Let us explore how compression levels affect storage size and performance for the balanced chunking strategy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test different compression levels\n",
    "compression_levels = [1, 3, 5, 7, 9]\n",
    "compression_results = []\n",
    "\n",
    "print(\"Testing different compression levels...\\n\")\n",
    "\n",
    "for level in compression_levels:\n",
    "    output_path = output_dir / f'sentinel2_compression_level_{level}.zarr'\n",
    "    \n",
    "    # Remove existing if present\n",
    "    if output_path.exists():\n",
    "        import shutil\n",
    "        shutil.rmtree(output_path)\n",
    "    \n",
    "    # Create Zarr with specific compression level\n",
    "    create_multitemporal_zarr_from_eopf(\n",
    "        items=items,\n",
    "        output_path=str(output_path),\n",
    "        chunk_strategy=chunking_strategies['balanced'],\n",
    "        compression='zstd',\n",
    "        compression_level=level,\n",
    "        use_sample_data=not use_real_data\n",
    "    )\n",
    "    \n",
    "    # Analyze performance\n",
    "    metrics = analyze_zarr_performance(str(output_path), test_reads=False)\n",
    "    \n",
    "    compression_results.append({\n",
    "        'Compression Level': level,\n",
    "        'Total Size (MB)': round(metrics['total_size_mb'], 2),\n",
    "        'Compression Ratio': round(metrics['avg_compression_ratio'], 2)\n",
    "    })\n",
    "\n",
    "compression_df = pd.DataFrame(compression_results)\n",
    "print(\"\\nCompression Level Comparison:\")\n",
    "print(\"=\" * 50)\n",
    "print(compression_df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize compression impact\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "# Plot storage size vs compression level\n",
    "ax = axes[0]\n",
    "ax.plot(compression_df['Compression Level'], compression_df['Total Size (MB)'], \n",
    "        marker='o', linewidth=2, markersize=8, color='steelblue')\n",
    "ax.set_xlabel('Compression Level')\n",
    "ax.set_ylabel('Total Size (MB)')\n",
    "ax.set_title('Storage Size vs Compression Level')\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot compression ratio vs compression level\n",
    "ax = axes[1]\n",
    "ax.plot(compression_df['Compression Level'], compression_df['Compression Ratio'], \n",
    "        marker='s', linewidth=2, markersize=8, color='green')\n",
    "ax.set_xlabel('Compression Level')\n",
    "ax.set_ylabel('Compression Ratio')\n",
    "ax.set_title('Compression Ratio vs Compression Level')\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.suptitle('Impact of Compression Level on Storage Efficiency', fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üí™ Now it is your turn\n",
    "\n",
    "The following exercises will help you master Zarr chunking strategies for your own Earth Observation workflows.\n",
    "\n",
    "### Task 1: Explore Your Own Chunking Strategy\n",
    "\n",
    "* Define a new chunking strategy that might work better for your specific use case\n",
    "* Add it to the `chunking_strategies` dictionary\n",
    "* Run the comparison again to see how it performs\n",
    "\n",
    "### Task 2: Test with Different Data Dimensions\n",
    "\n",
    "* Modify the `data_shape` to simulate a longer time series (e.g., 20 time steps)\n",
    "* How does this affect the optimal chunking strategy?\n",
    "* Which strategy performs best for time series analysis?\n",
    "\n",
    "### Task 3: Optimize for Your Access Pattern\n",
    "\n",
    "* If you primarily need to extract time series for individual pixels, which strategy would you choose?\n",
    "* If you need to process entire scenes at specific dates, which strategy is optimal?\n",
    "* Create a custom chunking strategy that balances both requirements\n",
    "\n",
    "### Task 4: Experiment with Different Compression Algorithms\n",
    "\n",
    "* Modify the `create_multitemporal_zarr` function call to test different compression algorithms (e.g., 'lz4', 'zlib', 'blosclz')\n",
    "* Compare the trade-offs between compression ratio and read performance\n",
    "* Which algorithm works best for your data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Space for your experiments\n",
    "# Example: Add your custom chunking strategy\n",
    "\n",
    "# my_custom_strategy = {\n",
    "#     'time': 3,\n",
    "#     'y': 768,\n",
    "#     'x': 768\n",
    "# }\n",
    "\n",
    "# Add your code here..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "In this notebook, we have demonstrated how to create multi-temporal EOPF Zarr datasets with different chunking strategies. We explored three main approaches‚Äîspatial-optimized, temporal-optimized, and balanced chunking‚Äîand analyzed their performance characteristics.\n",
    "\n",
    "Key takeaways:\n",
    "- **Spatial-optimized chunking** (large spatial chunks, small temporal chunks) excels for spatial analysis workflows\n",
    "- **Temporal-optimized chunking** (small spatial chunks, large temporal chunks) is ideal for time series extraction\n",
    "- **Balanced chunking** provides reasonable performance for mixed access patterns\n",
    "- Compression level significantly affects storage size but has diminishing returns beyond level 5-7\n",
    "- The optimal strategy depends on your specific access patterns and computational constraints\n",
    "\n",
    "Remember that chunk size selection is one of the most critical optimization decisions in Earth Observation data processing. Always profile your specific workflows to determine the optimal configuration."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What's next?\n",
    "\n",
    "In the next notebook, we will explore advanced Zarr features including hierarchical storage, multi-resolution pyramids, and cloud-optimized access patterns for large-scale Earth Observation analysis."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
